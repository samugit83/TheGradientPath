{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6e23a1",
   "metadata": {},
   "source": [
    "# Vision RAG: Multimodal Retrieval-Augmented Generation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92d7e0",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Welcome to the comprehensive guide for the Vision RAG system! This notebook provides an in-depth explanation of a cutting-edge multimodal Retrieval-Augmented Generation (RAG) system that can process, understand, and query both textual and visual content.\n",
    "\n",
    "### What is Vision RAG?\n",
    "\n",
    "Vision RAG extends the traditional text-only RAG approach by incorporating visual understanding capabilities. While conventional RAG systems only work with text documents, this Vision RAG system can:\n",
    "\n",
    "1. **Process Multimodal Documents**: Extract and understand both text and images from PDF files\n",
    "2. **Generate Multimodal Embeddings**: Create separate vector representations for text (using OpenAI's text-embedding-3-large) and images (using Cohere's embed-v4.0)\n",
    "3. **Perform Semantic Search**: Find relevant content across both modalities using cosine similarity\n",
    "4. **Generate Contextual Answers**: Provide comprehensive responses using both text and image context\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)**: A paradigm that combines the power of large language models with external knowledge retrieval. Instead of relying solely on pre-trained knowledge, RAG systems retrieve relevant information from a knowledge base and use it to generate more accurate, contextual responses.\n",
    "\n",
    "**Vector Embeddings**: Dense numerical representations of text or images that capture semantic meaning. Similar content will have similar embeddings, enabling semantic search through vector similarity.\n",
    "\n",
    "**Semantic Chunking**: An intelligent text segmentation approach that breaks documents into semantically coherent chunks rather than arbitrary fixed-size segments, preserving contextual meaning.\n",
    "\n",
    "**Multimodal Understanding**: The ability to process and understand multiple types of data (text, images) simultaneously, enabling more comprehensive document analysis.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "The Vision RAG system consists of three main components:\n",
    "\n",
    "1. **Ingestion Pipeline**: Processes PDFs and images, extracts content, generates embeddings, and stores them in PostgreSQL with pgvector\n",
    "2. **Query Engine**: Searches for relevant content across both text and images using vector similarity\n",
    "3. **Answer Generation**: Uses either OpenAI GPT or Google Gemini to generate comprehensive answers using retrieved context\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Dual Embedding Strategy**: Text embeddings (3072-dimensional) and image embeddings (1536-dimensional) using different specialized models\n",
    "- **Flexible PDF Processing**: Can extract text and images separately or convert entire PDF pages to images\n",
    "- **Advanced Text Chunking**: Uses LangChain's SemanticChunker for intelligent text segmentation\n",
    "- **Vector Database**: Leverages PostgreSQL with pgvector extension for efficient similarity search\n",
    "- **Provider Flexibility**: Supports both OpenAI and Google Gemini for answer generation\n",
    "- **Comprehensive Metadata**: Stores rich metadata for better content organization and debugging\n",
    "\n",
    "This implementation provides a robust foundation for applications requiring multimodal document understanding, such as technical documentation analysis, research paper exploration, and visual content management systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e6fb2",
   "metadata": {},
   "source": [
    "## ðŸ“º Watch the Tutorial\n",
    "\n",
    "Prefer a video walkthrough? Check out the accompanying tutorial on YouTube:\n",
    "\n",
    "[Vision RAG](https://youtu.be/LNydD9ZemZ8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-prerequisites",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Docker and Docker Compose (for PostgreSQL only)\n",
    "- OpenAI API key\n",
    "- Cohere API key\n",
    "- Google Gemini API key (optional, for Gemini provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-start",
   "metadata": {},
   "source": [
    "## 3. Quick Start\n",
    "\n",
    "### 3.1 Environment Setup\n",
    "\n",
    "Create a `.env` file with your API keys:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "COHERE_API_KEY=your_cohere_api_key_here\n",
    "GEMINI_API_KEY=your_gemini_api_key_here\n",
    "POSTGRES_CONNECTION_STRING=postgresql://username:password@localhost:5432/vision_rag_db\n",
    "```\n",
    "\n",
    "### 3.2 Start PostgreSQL Database\n",
    "\n",
    "```bash\n",
    "# Start PostgreSQL with automatic database initialization\n",
    "docker-compose up -d\n",
    "\n",
    "# Check that database is running\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "The database will be automatically initialized with the required tables and indexes.\n",
    "\n",
    "### 3.3 Setup Python Environment\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate virtual environment\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 3.4 Add Your Documents\n",
    "\n",
    "Place your PDF files and images in the `docs/` folder:\n",
    "\n",
    "```\n",
    "docs/\n",
    "â”œâ”€â”€ document1.pdf\n",
    "â”œâ”€â”€ document2.pdf\n",
    "â”œâ”€â”€ image1.png\n",
    "â””â”€â”€ subfolder/\n",
    "    â””â”€â”€ image2.jpg\n",
    "```\n",
    "\n",
    "### 3.5 Run the Application\n",
    "\n",
    "```bash\n",
    "# Make sure virtual environment is activated\n",
    "source venv/bin/activate\n",
    "\n",
    "# Run the application\n",
    "python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-section",
   "metadata": {},
   "source": [
    "## 4. Usage\n",
    "\n",
    "### 4.1 Basic Commands\n",
    "\n",
    "```bash\n",
    "# Start PostgreSQL database\n",
    "docker-compose up -d\n",
    "\n",
    "# Stop PostgreSQL database\n",
    "docker-compose down\n",
    "\n",
    "# View database logs\n",
    "docker-compose logs -f postgres\n",
    "\n",
    "# Check database status\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "### 4.2 Python Application\n",
    "\n",
    "```bash\n",
    "# Activate virtual environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# Run main application (processes docs/ folder and runs example queries)\n",
    "python main.py\n",
    "```\n",
    "\n",
    "### 4.3 Database Management\n",
    "\n",
    "```bash\n",
    "# Connect to PostgreSQL directly\n",
    "docker-compose exec postgres psql -U username -d vision_rag_db\n",
    "\n",
    "# Check table contents\n",
    "docker-compose exec postgres psql -U username -d vision_rag_db -c \"SELECT COUNT(*) FROM text_embeddings;\"\n",
    "docker-compose exec postgres psql -U username -d vision_rag_db -c \"SELECT COUNT(*) FROM image_embeddings;\"\n",
    "```\n",
    "\n",
    "### 4.4 Complete Cleanup Commands\n",
    "\n",
    "```bash\n",
    "# Stop and remove containers with volumes (removes all data)\n",
    "docker-compose down -v\n",
    "\n",
    "# Remove project-specific Docker images\n",
    "docker-compose down --rmi all\n",
    "\n",
    "# Complete project cleanup (containers, volumes, images, and networks)\n",
    "docker-compose down -v --rmi all --remove-orphans\n",
    "\n",
    "# If you want to remove everything Docker Compose created for this project:\n",
    "docker-compose down -v --rmi all --remove-orphans && docker volume prune -f\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1a3c",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "The complete workflow of our Vision RAG:\n",
    "\n",
    "![Vision RAG](vision_rag_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-architecture",
   "metadata": {},
   "source": [
    "## 5. System Architecture & Main Classes\n",
    "\n",
    "The Vision RAG system is built around three core modules, each containing specialized classes designed for specific functionality:\n",
    "\n",
    "### 5.1 Core Module Overview\n",
    "\n",
    "1. **main.py**: Application entry point and orchestration\n",
    "2. **ingestion.py**: Document processing and embedding generation\n",
    "3. **query.py**: Search and answer generation\n",
    "\n",
    "### 5.2 Data Flow Architecture\n",
    "\n",
    "```\n",
    "Documents (PDFs/Images) â†’ Ingestion Pipeline â†’ Vector Database â†’ Query Engine â†’ Answer Generation\n",
    "                                    â†“                    â†“               â†“              â†“\n",
    "                              Text/Image           PostgreSQL      Similarity      OpenAI/Gemini\n",
    "                              Processing            +pgvector        Search         Generation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-py-analysis",
   "metadata": {},
   "source": [
    "## 6. Main Application (main.py)\n",
    "\n",
    "### 6.1 Purpose\n",
    "The main module serves as the application entry point, orchestrating the entire Vision RAG workflow from document ingestion to query processing.\n",
    "\n",
    "### 6.2 Core Functionality\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    \"\"\"Main function to initialize and run the Vision-RAG system\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    unified_ingestion = UnifiedIngestionPipe()\n",
    "    rag_query = RagQuery()\n",
    "```\n",
    "\n",
    "### 6.3 Key Features\n",
    "\n",
    "1. **Component Initialization**: Creates instances of the unified ingestion pipeline and query engine\n",
    "2. **Configurable Processing**: Uses `DEFAULT_CONFIG` to control whether ingestion and querying are active\n",
    "3. **Batch Document Processing**: Processes all files in the `docs/` folder when ingestion is enabled\n",
    "4. **Example Query Execution**: Demonstrates the system capabilities with predefined queries\n",
    "5. **Result Reporting**: Provides detailed statistics about processed documents and query results\n",
    "\n",
    "### 6.4 Workflow Stages\n",
    "\n",
    "1. **Initialization Phase**: Sets up logging and creates component instances\n",
    "2. **Ingestion Phase**: Processes documents if `activate_ingestion` is enabled\n",
    "3. **Query Phase**: Executes example queries if `activate_query` is enabled\n",
    "4. **Reporting Phase**: Displays processing statistics and query results\n",
    "\n",
    "### 6.5 Configuration Control\n",
    "The application behavior is controlled through configuration flags:\n",
    "- `DEFAULT_CONFIG.activate_ingestion`: Controls document processing\n",
    "- `DEFAULT_CONFIG.activate_query`: Controls query execution\n",
    "\n",
    "This design allows for flexible operation modes, such as ingestion-only for initial setup or query-only for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingestion-analysis",
   "metadata": {},
   "source": [
    "## 7. Ingestion Pipeline (ingestion.py)\n",
    "\n",
    "The ingestion module contains four specialized classes that work together to process multimodal documents and generate vector embeddings.\n",
    "\n",
    "### 7.1 ImageProcessor Class\n",
    "\n",
    "**Purpose**: Utility class for image preprocessing and format conversion.\n",
    "\n",
    "**Key Methods**:\n",
    "- `resize_image()`: Ensures images don't exceed maximum pixel limits (1568Ã—1568)\n",
    "- `base64_from_image()`: Converts images to base64 encoding for storage and API calls\n",
    "\n",
    "**Technical Details**:\n",
    "```python\n",
    "MAX_PIXELS = 1568 * 1568  # Optimized for vision model input\n",
    "```\n",
    "\n",
    "This class ensures images are properly formatted for both storage and processing by vision models.\n",
    "\n",
    "### 7.2 TextIngestionPipe Class\n",
    "\n",
    "**Purpose**: Handles text document processing with advanced semantic chunking and embedding generation.\n",
    "\n",
    "**Core Components**:\n",
    "- **Semantic Chunker**: Uses LangChain's SemanticChunker for intelligent text segmentation\n",
    "- **OpenAI Embeddings**: Generates 3072-dimensional vectors using text-embedding-3-large\n",
    "- **PostgreSQL Storage**: Stores embeddings with pgvector extension\n",
    "\n",
    "**Key Methods**:\n",
    "```python\n",
    "def chunk_text(self, text: str) -> List[str]:\n",
    "    \"\"\"Chunk text using semantic chunker\"\"\"\n",
    "    # Uses semantic boundaries rather than fixed sizes\n",
    "    \n",
    "def compute_text_embedding(self, text: str) -> np.ndarray:\n",
    "    \"\"\"Compute embedding using OpenAI text-embedding-3-large\"\"\"\n",
    "    # Returns 3072-dimensional vector\n",
    "    \n",
    "def store_text_embedding(self, text_content: str, ...):\n",
    "    \"\"\"Store embeddings in PostgreSQL with pgvector\"\"\"\n",
    "    # Includes metadata for better organization\n",
    "```\n",
    "\n",
    "**Advanced Features**:\n",
    "- Database connection verification with helpful error messages\n",
    "- Chunk filtering based on minimum size requirements\n",
    "- Comprehensive metadata storage for debugging and analysis\n",
    "\n",
    "### 7.3 ImageIngestionPipe Class\n",
    "\n",
    "**Purpose**: Processes images and generates embeddings using Cohere's multimodal model.\n",
    "\n",
    "**Core Technology**:\n",
    "- **Cohere Embed v4.0**: Generates 1536-dimensional image embeddings\n",
    "- **Base64 Storage**: Stores original images for retrieval and display\n",
    "- **MIME Type Detection**: Proper format handling for different image types\n",
    "\n",
    "**Key Methods**:\n",
    "```python\n",
    "def compute_image_embedding(self, img_path: str) -> np.ndarray:\n",
    "    \"\"\"Generate embedding using Cohere embed-v4.0\"\"\"\n",
    "    # Converts image to base64 and processes with Cohere API\n",
    "    \n",
    "def store_image_embedding(self, ...):\n",
    "    \"\"\"Store image embeddings with base64 data\"\"\"\n",
    "    # Includes original image data for display purposes\n",
    "```\n",
    "\n",
    "**Unique Features**:\n",
    "- Stores both embeddings and original image data\n",
    "- Supports multiple image formats (PNG, JPG, GIF, BMP, TIFF, WebP)\n",
    "- Automatic MIME type detection and validation\n",
    "\n",
    "### 7.4 UnifiedIngestionPipe Class\n",
    "\n",
    "**Purpose**: Orchestrates the complete ingestion workflow, combining text and image processing.\n",
    "\n",
    "**Core Functionality**:\n",
    "```python\n",
    "def process_all_files(self) -> Dict[str, List[str]]:\n",
    "    \"\"\"Process all files from the docs folder\"\"\"\n",
    "    # Handles both PDFs and standalone images\n",
    "    # Returns document IDs for tracking\n",
    "```\n",
    "\n",
    "**Processing Modes**:\n",
    "1. **Standard Mode**: Extracts text and images separately from PDFs\n",
    "2. **Page-as-Image Mode**: Converts entire PDF pages to images (useful for layout-heavy documents)\n",
    "\n",
    "**Advanced Features**:\n",
    "- Recursive file discovery in subdirectories\n",
    "- Duplicate prevention (excludes extracted_images directory)\n",
    "- Flexible PDF processing modes based on configuration\n",
    "- Comprehensive error handling and logging\n",
    "\n",
    "**File Organization**:\n",
    "```\n",
    "docs/\n",
    "â”œâ”€â”€ document.pdf          # Processed for text + images\n",
    "â”œâ”€â”€ image.png            # Processed as standalone image\n",
    "â””â”€â”€ extracted_images/    # PDF-extracted images (auto-created)\n",
    "    â””â”€â”€ document_img1.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-analysis",
   "metadata": {},
   "source": [
    "## 8. Query Engine (query.py)\n",
    "\n",
    "The query module contains the RagQuery class, which handles the complete query workflow from search to answer generation.\n",
    "\n",
    "### 8.1 RagQuery Class\n",
    "\n",
    "**Purpose**: Implements multimodal search and answer generation using retrieved context.\n",
    "\n",
    "**Core Architecture**:\n",
    "- **Dual Search System**: Separate search mechanisms for text and images\n",
    "- **Provider Flexibility**: Supports both OpenAI and Google Gemini for answer generation\n",
    "- **Vector Database Integration**: Uses PostgreSQL with pgvector for efficient similarity search\n",
    "\n",
    "### 8.2 Text Search Implementation\n",
    "\n",
    "```python\n",
    "def search_similar_texts(self, question: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search for similar text chunks using pgvector cosine similarity\"\"\"\n",
    "    # 1. Convert query to embedding using OpenAI text-embedding-3-large\n",
    "    # 2. Perform vector similarity search in PostgreSQL\n",
    "    # 3. Return top-k results with similarity scores\n",
    "```\n",
    "\n",
    "**Technical Details**:\n",
    "- Uses 3072-dimensional embeddings from OpenAI\n",
    "- Cosine distance calculation: `similarity = 1 - distance`\n",
    "- Returns comprehensive metadata including source files and chunk information\n",
    "\n",
    "### 8.3 Image Search Implementation\n",
    "\n",
    "```python\n",
    "def search_similar_images(self, question: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"Search for similar images using Cohere embeddings\"\"\"\n",
    "    # 1. Convert text query to embedding using Cohere embed-v4.0\n",
    "    # 2. Search against image embeddings in PostgreSQL\n",
    "    # 3. Return images with base64 data for display\n",
    "```\n",
    "\n",
    "**Key Features**:\n",
    "- Cross-modal search: text queries can find relevant images\n",
    "- 1536-dimensional embeddings from Cohere\n",
    "- Includes original image data for answer generation\n",
    "\n",
    "### 8.4 Answer Generation Systems\n",
    "\n",
    "#### 8.4.1 OpenAI Integration\n",
    "```python\n",
    "def generate_answer_with_openai(self, question: str, text_context: List[Dict], \n",
    "                                image_context: List[Dict]) -> str:\n",
    "    \"\"\"Generate answer using OpenAI with text context\"\"\"\n",
    "    # Uses LangChain ChatOpenAI with system/human message structure\n",
    "    # Combines text and image metadata for comprehensive context\n",
    "```\n",
    "\n",
    "#### 8.4.2 Google Gemini Integration\n",
    "```python\n",
    "def generate_answer_with_gemini(self, question: str, text_context: List[Dict], \n",
    "                                image_context: List[Dict]) -> str:\n",
    "    \"\"\"Generate answer using Google Gemini with both text and image context\"\"\"\n",
    "    # Supports true multimodal input with both text and images\n",
    "    # Converts base64 images to PIL format for Gemini API\n",
    "```\n",
    "\n",
    "**Gemini Advantages**:\n",
    "- Native multimodal support (text + images simultaneously)\n",
    "- Direct image processing without conversion\n",
    "- Better visual understanding for complex queries\n",
    "\n",
    "### 8.5 Complete Query Workflow\n",
    "\n",
    "```python\n",
    "def query(self, question: str) -> Dict:\n",
    "    \"\"\"Complete query flow: search both text and images, then generate answer\"\"\"\n",
    "    # 1. Search similar texts (top_k from config)\n",
    "    # 2. Search similar images (top_k from config)\n",
    "    # 3. Generate answer using retrieved context\n",
    "    # 4. Return comprehensive result with context information\n",
    "```\n",
    "\n",
    "**Result Structure**:\n",
    "```python\n",
    "{\n",
    "    \"question\": \"User's original question\",\n",
    "    \"answer\": \"Generated response\",\n",
    "    \"text_context\": [{\n",
    "        \"text_content\": \"Retrieved text chunk\",\n",
    "        \"source_file\": \"document.pdf\",\n",
    "        \"similarity\": 0.85,\n",
    "        \"metadata\": {...}\n",
    "    }],\n",
    "    \"image_context\": [{\n",
    "        \"image_name\": \"diagram.png\",\n",
    "        \"source_file\": \"document.pdf\",\n",
    "        \"similarity\": 0.78,\n",
    "        \"base64_data\": \"...\",\n",
    "        \"metadata\": {...}\n",
    "    }]\n",
    "}\n",
    "```\n",
    "\n",
    "This comprehensive result structure allows for transparency in the retrieval process and enables applications to display source materials alongside answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-details",
   "metadata": {},
   "source": [
    "## 9. Technical Implementation Details\n",
    "\n",
    "### 9.1 Database Schema\n",
    "\n",
    "#### Text Embeddings Table\n",
    "```sql\n",
    "CREATE TABLE text_embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    text_content TEXT NOT NULL,\n",
    "    chunk_index INTEGER,\n",
    "    source_file TEXT,\n",
    "    embedding VECTOR(3072),  -- OpenAI text-embedding-3-large\n",
    "    metadata JSONB,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "```\n",
    "\n",
    "#### Image Embeddings Table\n",
    "```sql\n",
    "CREATE TABLE image_embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    image_name TEXT NOT NULL,\n",
    "    image_path TEXT,\n",
    "    source_file TEXT,\n",
    "    base64_data TEXT,  -- Original image data\n",
    "    mime_type TEXT,\n",
    "    embedding VECTOR(1536),  -- Cohere embed-v4.0\n",
    "    metadata JSONB,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "```\n",
    "\n",
    "### 9.2 Embedding Strategy\n",
    "\n",
    "**Text Embeddings**: \n",
    "- Model: OpenAI text-embedding-3-large\n",
    "- Dimensions: 3072\n",
    "- Optimized for semantic text search\n",
    "\n",
    "**Image Embeddings**:\n",
    "- Model: Cohere embed-v4.0\n",
    "- Dimensions: 1536  \n",
    "- Supports cross-modal text-to-image search\n",
    "\n",
    "### 9.3 Performance Optimizations\n",
    "\n",
    "1. **Vector Indexing**: pgvector indexes for fast similarity search\n",
    "2. **Image Resizing**: Automatic resizing to prevent API limits\n",
    "3. **Semantic Chunking**: Preserves context while optimizing chunk size\n",
    "4. **Connection Pooling**: Efficient database connection management\n",
    "5. **Batch Processing**: Optimized for processing multiple documents\n",
    "\n",
    "### 9.4 Error Handling & Logging\n",
    "\n",
    "The system implements comprehensive error handling:\n",
    "- Database connection verification\n",
    "- API rate limit handling  \n",
    "- File format validation\n",
    "- Graceful degradation when components fail\n",
    "- Detailed logging for debugging\n",
    "\n",
    "### 9.5 Configuration Management\n",
    "\n",
    "All system behavior is controlled through the configuration system:\n",
    "- Processing modes (text-only, image-only, multimodal)\n",
    "- Chunking parameters\n",
    "- Retrieval settings (top-k values)\n",
    "- Provider selection (OpenAI vs Gemini)\n",
    "- Database connection settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "## 10. Best Practices & Usage Recommendations\n",
    "\n",
    "### 10.1 Document Preparation\n",
    "\n",
    "**Optimal PDF Structure**:\n",
    "- Clear text formatting for better extraction\n",
    "- High-quality images (minimum 300 DPI)\n",
    "- Consistent document structure\n",
    "\n",
    "**Image Guidelines**:\n",
    "- Supported formats: PNG, JPG, GIF, BMP, TIFF, WebP\n",
    "- Recommended resolution: 1024x1024 or higher\n",
    "- Clear, well-lit images for better embeddings\n",
    "\n",
    "### 10.2 Query Optimization\n",
    "\n",
    "**Effective Queries**:\n",
    "- Be specific about what you're looking for\n",
    "- Include context about document types or content\n",
    "- Use descriptive language for image searches\n",
    "\n",
    "**Example Good Queries**:\n",
    "- \"Find a floor plan that includes a tea store and gaming space\"\n",
    "- \"What are the technical specifications mentioned in the manual?\"\n",
    "- \"Show me diagrams related to network architecture\"\n",
    "\n",
    "### 10.3 Performance Tuning\n",
    "\n",
    "**Database Optimization**:\n",
    "```sql\n",
    "-- Create indexes for better performance\n",
    "CREATE INDEX idx_text_embeddings_vector ON text_embeddings USING ivfflat (embedding vector_cosine_ops);\n",
    "CREATE INDEX idx_image_embeddings_vector ON image_embeddings USING ivfflat (embedding vector_cosine_ops);\n",
    "```\n",
    "\n",
    "**Configuration Tuning**:\n",
    "- Adjust `top_k` values based on your use case\n",
    "- Optimize chunk size for your document types\n",
    "- Choose the appropriate answer provider for your needs\n",
    "\n",
    "### 10.4 Monitoring & Maintenance\n",
    "\n",
    "**Regular Maintenance**:\n",
    "- Monitor database size and performance\n",
    "- Update embeddings when documents change\n",
    "- Review query performance and adjust indexes\n",
    "- Monitor API usage and costs\n",
    "\n",
    "**Debugging Tools**:\n",
    "- Check processing logs for errors\n",
    "- Verify embedding dimensions\n",
    "- Test similarity scores for relevance\n",
    "- Review metadata for completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "The Vision RAG system represents a significant advancement in document understanding technology, combining the power of large language models with sophisticated multimodal retrieval capabilities. By processing both text and visual content, it enables more comprehensive and contextual responses to user queries.\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "1. **Multimodal Processing**: Successfully handles both text and image content from complex documents\n",
    "2. **Advanced Chunking**: Uses semantic segmentation for better context preservation\n",
    "3. **Flexible Architecture**: Supports multiple embedding providers and answer generation models\n",
    "4. **Production Ready**: Includes comprehensive error handling, logging, and monitoring capabilities\n",
    "5. **Scalable Design**: Built on PostgreSQL with pgvector for efficient similarity search at scale\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "- **Additional File Formats**: Support for more document types (Word, PowerPoint, etc.)\n",
    "- **Real-time Processing**: Live document updates and incremental ingestion\n",
    "- **Advanced Chunking**: Domain-specific chunking strategies\n",
    "- **Multi-language Support**: Embedding models for different languages\n",
    "- **Enhanced Metadata**: Richer document structure understanding\n",
    "\n",
    "This system provides a solid foundation for applications requiring sophisticated document understanding, from enterprise knowledge management to research assistance and automated content analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
