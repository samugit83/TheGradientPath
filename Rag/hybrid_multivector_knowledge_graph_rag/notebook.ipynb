{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Hybrid Multivector Knowledge Graph RAG System: Mastering Advanced Graph Traversal Algorithms\n",
        "\n",
        "## *The Ultimate Guide to Building Revolutionary AI Systems that Think Beyond Traditional Boundaries*\n",
        "\n",
        "### Featuring 11+ Advanced Graph Traversal Algorithms with LLM-Powered Intelligence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåü Introduction: Welcome to the Future of Intelligent Information Retrieval\n",
        "\n",
        "**Prepare to embark on a transformative journey into the cutting-edge realm of hybrid RAG systems** ‚Äì where traditional vector similarity search meets the sophisticated intelligence of graph databases, creating an AI system that doesn't just find information, but truly *understands* the complex relationships between concepts, entities, and ideas.\n",
        "\n",
        "### What Makes This System Revolutionary?\n",
        "\n",
        "This isn't just another RAG tutorial. This is your gateway to mastering the **most advanced hybrid retrieval system** ever conceived ‚Äì one that combines the semantic power of vector embeddings with the structural intelligence of knowledge graphs to create an AI that can:\n",
        "\n",
        "- **Think Beyond Surface-Level Similarity**: While traditional RAG systems can only find semantically similar content, our hybrid approach discovers hidden connections and contextual relationships that span multiple degrees of separation\n",
        "- **Navigate Complex Knowledge Networks**: Through sophisticated graph traversal algorithms, the system can follow chains of reasoning, trace causal relationships, and uncover narrative threads that connect disparate pieces of information\n",
        "- **Adapt Intelligence Dynamically**: Using LLM-powered query generation and predicate filtering, the system doesn't just follow pre-programmed patterns ‚Äì it *reasons* about what information to seek and how to find it\n",
        "\n",
        "### üéØ What You'll Master in This Tutorial\n",
        "\n",
        "This comprehensive guide will transform you from a RAG novice into a **hybrid AI architect** capable of building systems that rival the most sophisticated information retrieval platforms. You'll master:\n",
        "\n",
        "#### **üèóÔ∏è Revolutionary System Architecture**\n",
        "*   **Knowledge Graph Engineering**: Transform raw documents into intelligent, semantically-rich knowledge graphs using Neo4j's powerful graph database capabilities\n",
        "*   **Multi-Vector Embedding Strategy**: Create multiple semantic representations of the same content, enabling nuanced retrieval across different conceptual dimensions\n",
        "*   **Hybrid Processing Pipeline**: Seamlessly integrate vector similarity search with graph traversal for comprehensive context discovery\n",
        "*   **LLM-Powered Intelligence**: Leverage large language models for entity extraction, relationship identification, and dynamic query generation\n",
        "\n",
        "#### **üß† Advanced Graph Traversal Mastery**\n",
        "*   **Context-to-Cypher Revolution**: Learn the groundbreaking technique of using LLMs to dynamically generate custom Cypher queries based on context analysis\n",
        "*   **Intelligent BFS/DFS Exploration**: Master both breadth-first and depth-first search strategies with LLM-powered relationship filtering\n",
        "*   **Optimal Search Algorithms**: Implement Uniform Cost Search and A* heuristic algorithms for semantically-guided exploration\n",
        "*   **Beam Search Optimization**: Control computational complexity while maximizing information discovery through intelligent pruning\n",
        "*   **Multi-Hop Relationship Discovery**: Uncover complex chains of reasoning and causal relationships across multiple graph layers\n",
        "\n",
        "#### **‚ö° Production-Ready Implementation**\n",
        "*   **Scalable Architecture**: Build systems that can handle massive document collections with efficient parallel processing\n",
        "*   **Real-Time Query Processing**: Implement singleton patterns and optimized caching for production-level performance\n",
        "*   **Comprehensive Error Handling**: Create robust systems that gracefully handle edge cases and unexpected scenarios\n",
        "*   **Modular Design**: Develop extensible architectures that can easily incorporate new algorithms and capabilities\n",
        "\n",
        "### üîÑ Complete System Workflow\n",
        "\n",
        "Our hybrid RAG system operates through an elegant **two-phase architecture** that maximizes both semantic understanding and structural intelligence:\n",
        "\n",
        "#### **Phase 1: Knowledge Graph Ingestion** üèóÔ∏è\n",
        "Transform your raw documents into a sophisticated knowledge graph through:\n",
        "- **Semantic Document Processing**: Intelligent chunking based on conceptual boundaries\n",
        "- **LLM-Powered Entity Extraction**: Identify key concepts, relationships, and semantic structures\n",
        "- **Multi-Vector Embedding Generation**: Create diverse semantic representations for comprehensive search\n",
        "- **Graph Construction**: Build rich, interconnected knowledge networks in Neo4j\n",
        "\n",
        "#### **Phase 2: Intelligent Query Processing** üîç\n",
        "Answer complex questions through sophisticated hybrid retrieval:\n",
        "- **Multi-Vector Similarity Search**: Find semantically relevant content across multiple embedding dimensions\n",
        "- **Advanced Graph Traversal**: Discover additional context through intelligent relationship exploration\n",
        "- **Context Integration**: Seamlessly merge vector and graph-based results for comprehensive understanding\n",
        "- **LLM-Powered Answer Generation**: Synthesize final responses using the full contextual understanding\n",
        "\n",
        "### üöÄ Quick Start Guide\n",
        "\n",
        "Ready to experience the power of hybrid RAG? Follow these simple steps to get your system running:\n",
        "\n",
        "#### **1. Initialize Your Knowledge Graph**\n",
        "Transform your documents into an intelligent knowledge graph:\n",
        "```bash\n",
        "python ingestion.py\n",
        "```\n",
        "This command will:\n",
        "- Load and semantically chunk your documents\n",
        "- Extract entities and relationships using LLM intelligence\n",
        "- Build your knowledge graph in Neo4j\n",
        "- Create multi-vector embeddings for enhanced retrieval\n",
        "- Generate custom document properties for comprehensive search\n",
        "\n",
        "#### **2. Start Querying Your Hybrid System**\n",
        "Experience the power of advanced graph traversal:\n",
        "```bash\n",
        "python query.py\n",
        "```\n",
        "This will activate the complete hybrid RAG pipeline:\n",
        "- Multi-vector similarity search across your knowledge graph\n",
        "- Advanced graph traversal using your chosen algorithm\n",
        "- Intelligent context integration and answer generation\n",
        "- Comprehensive result analysis and visualization\n",
        "\n",
        "#### **3. Explore Your Knowledge Graph Visually**\n",
        "Witness the beauty of your knowledge graph through Neo4j's powerful browser interface:\n",
        "\n",
        "**üåê Open Neo4j Browser**: Navigate to `http://localhost:7474` in your web browser\n",
        "\n",
        "**üîê Login Credentials**:\n",
        "- **Username**: `neo4j`\n",
        "- **Password**: `your_password` (as configured in your .env file)\n",
        "\n",
        "**üé® Visualization Commands**:\n",
        "```cypher\n",
        "// View your complete knowledge graph\n",
        "MATCH (n) RETURN n LIMIT 100\n",
        "\n",
        "// Explore document nodes and their relationships\n",
        "MATCH (d:Document)-[r]->(n) RETURN d, r, n LIMIT 50\n",
        "\n",
        "// Discover entity relationships\n",
        "MATCH (e:Entity)-[r]->(related) RETURN e, r, related LIMIT 25\n",
        "\n",
        "// View vector-indexed properties\n",
        "CALL db.indexes() YIELD name, type WHERE type = 'VECTOR' RETURN name, type\n",
        "```\n",
        "\n",
        "### üí° What Makes This Tutorial Special\n",
        "\n",
        "Unlike traditional RAG implementations that rely solely on vector similarity, this system represents a **paradigm shift** in how AI systems understand and retrieve information. You're not just learning to build another chatbot ‚Äì you're mastering the architecture of **next-generation AI systems** that can:\n",
        "\n",
        "- **Reason About Complex Relationships**: Trace multi-step causal chains and discover hidden connections\n",
        "- **Adapt to Query Complexity**: Dynamically select the most appropriate search strategies based on question characteristics\n",
        "- **Scale to Enterprise Level**: Handle massive knowledge bases with sophisticated optimization techniques\n",
        "- **Integrate Seamlessly**: Combine multiple AI technologies into a unified, intelligent system\n",
        "\n",
        "### üé¨ Ready to Begin Your Journey?\n",
        "\n",
        "The future of AI lies not in simple similarity matching, but in **intelligent relationship discovery** and **contextual understanding**. This tutorial will equip you with the knowledge and skills to build systems that don't just retrieve information ‚Äì they **understand** it, **contextualize** it, and **reason** about it.\n",
        "\n",
        "**Let's dive into building the most advanced RAG system you've ever imagined!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*From basic vector search to revolutionary hybrid intelligence ‚Äì your transformation starts now.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì∫ Watch the Tutorial\n",
        "\n",
        "Prefer a video walkthrough? Check out the accompanying tutorial on YouTube:\n",
        "\n",
        "[Hybrid Knowledge Graph RAG Tutorial](https://youtu.be/q5zc1BIaEbI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Started\n",
        "\n",
        "### Hardware Requirements\n",
        "\n",
        "This tutorial requires:\n",
        "- A machine with sufficient RAM for Neo4j (minimum 4GB, recommended 8GB+)\n",
        "- Docker for containerized Neo4j deployment\n",
        "- OpenAI API access for embeddings and LLM operations\n",
        "\n",
        "### Environment Setup\n",
        "\n",
        "First, ensure you have Docker installed for Neo4j:\n",
        "\n",
        "```bash\n",
        "# Install Docker (Ubuntu/Debian)\n",
        "sudo apt update\n",
        "sudo apt install docker.io docker-compose -y\n",
        "sudo systemctl start docker\n",
        "sudo systemctl enable docker\n",
        "```\n",
        "\n",
        "Set up a Python virtual environment:\n",
        "\n",
        "```bash\n",
        "# Create a virtual environment\n",
        "python -m venv venv\n",
        "\n",
        "# Activate the virtual environment\n",
        "# On Linux/Mac:\n",
        "source venv/bin/activate\n",
        "# On Windows:\n",
        "# venv\\Scripts\\activate\n",
        "\n",
        "# Install requirements\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "Create a `.env` file with your credentials:\n",
        "\n",
        "```bash\n",
        "# OpenAI API\n",
        "OPENAI_API_KEY=your_openai_api_key\n",
        "\n",
        "# Neo4j Configuration\n",
        "NEO4J_URI=bolt://localhost:7687\n",
        "NEO4J_USERNAME=neo4j\n",
        "NEO4J_PASSWORD=your_password\n",
        "```\n",
        "\n",
        "### Starting Neo4j with Docker\n",
        "\n",
        "```bash\n",
        "# Start Neo4j using Docker Compose\n",
        "docker-compose up -d\n",
        "\n",
        "# Or run Neo4j directly\n",
        "docker run -d \\\n",
        "  --name neo4j \\\n",
        "  -p 7474:7474 -p 7687:7687 \\\n",
        "  -e NEO4J_AUTH=neo4j/your_password \\\n",
        "  neo4j:latest\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Graph Ingestion System: Building the Foundation\n",
        "\n",
        "### Introduction to Knowledge Graph Ingestion\n",
        "\n",
        "Before we can query and traverse a knowledge graph, we must first construct it from raw documents. The `KnowledgeGraphIngestion` class represents the cornerstone of our hybrid RAG system, responsible for transforming unstructured text documents into a rich, semantically-indexed knowledge graph stored in Neo4j. This sophisticated ingestion pipeline combines multiple AI technologies to create a comprehensive knowledge representation that serves as the foundation for our advanced retrieval and reasoning capabilities.\n",
        "\n",
        "The ingestion process operates through five distinct phases:\n",
        "\n",
        "1. **Document Processing**: Load and semantically chunk documents using intelligent text splitting\n",
        "2. **Graph Extraction**: Use LLMs to identify entities, relationships, and concepts within the text\n",
        "3. **Knowledge Graph Construction**: Build a structured graph representation in Neo4j\n",
        "4. **Vector Embedding Generation**: Create multiple semantic embeddings for enhanced retrieval\n",
        "5. **Multi-Vector Enhancement**: Add specialized properties to document nodes for comprehensive search\n",
        "\n",
        "What makes this system revolutionary is its ability to create multiple semantic representations of the same content, enabling both precise similarity search and comprehensive graph traversal. The system generates embeddings not just for document chunks, but for extracted entities, relationships, and dynamically generated properties, creating a multi-dimensional search space that captures different aspects of the knowledge.\n",
        "\n",
        "### Core Architecture and Design Principles\n",
        "\n",
        "The `KnowledgeGraphIngestion` system is built on several key architectural principles:\n",
        "\n",
        "- **Incremental Processing**: Track session-specific ingestion to enable efficient updates without reprocessing entire datasets\n",
        "- **Parallel Processing**: Leverage concurrent execution for embedding generation and property creation\n",
        "- **Configurable Transformation**: Support custom prompts, schema constraints, and property configurations\n",
        "- **Robust Error Handling**: Graceful degradation and comprehensive error reporting\n",
        "- **Resource Management**: Efficient service initialization and connection management\n",
        "\n",
        "### Detailed Code Analysis\n",
        "\n",
        "Let's examine each component of the ingestion system in detail:\n",
        "\n",
        "#### 1. Import Dependencies and Core Services\n",
        "\n",
        "```python\n",
        "import os\n",
        "import glob\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import params\n",
        "```\n",
        "\n",
        "This comprehensive import section reveals the multi-layered architecture of the ingestion system:\n",
        "\n",
        "**Parallel Processing Infrastructure**: The `concurrent.futures` module enables sophisticated parallel processing for embedding generation and property creation, dramatically improving ingestion speed for large document collections.\n",
        "\n",
        "**LangChain Integration**: The system leverages multiple LangChain components:\n",
        "- `Document`: Standardized document representation with metadata\n",
        "- `LLMGraphTransformer`: Converts documents into graph structures using LLM intelligence\n",
        "- `SemanticChunker`: Intelligent text splitting based on semantic boundaries\n",
        "- `ChatOpenAI` and `OpenAIEmbeddings`: AI services for text analysis and vector generation\n",
        "- `Neo4jGraph`: High-level interface for graph database operations\n",
        "\n",
        "**Configuration Management**: The `params` module centralizes all configurable aspects of the ingestion process, from chunk sizes to embedding dimensions, enabling easy experimentation and optimization.\n",
        "\n",
        "#### 2. Class Declaration and Service Management\n",
        "\n",
        "```python\n",
        "class KnowledgeGraphIngestion:\n",
        "    \"\"\"\n",
        "    A comprehensive class for ingesting documents into a Neo4j knowledge graph\n",
        "    with optional vector embeddings and indexes.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the ingestion system with all services.\"\"\"\n",
        "        # Core services\n",
        "        self.graph: Optional[Neo4jGraph] = None\n",
        "        self.llm: Optional[ChatOpenAI] = None\n",
        "        self.transformer: Optional[LLMGraphTransformer] = None\n",
        "        \n",
        "        # Embedding services\n",
        "        self.embeddings: Optional[OpenAIEmbeddings] = None\n",
        "        self.chunker_embeddings: Optional[OpenAIEmbeddings] = None\n",
        "        self.semantic_chunker: Optional[SemanticChunker] = None\n",
        "        \n",
        "        # Data storage\n",
        "        self.documents: List[Document] = []\n",
        "        \n",
        "        # Track newly ingested data for this session\n",
        "        self.current_session_sources: set = set()\n",
        "        self.current_session_nodes: List[str] = []\n",
        "        self.current_session_relationships: List[str] = []\n",
        "```\n",
        "\n",
        "The class initialization demonstrates sophisticated service management and session tracking:\n",
        "\n",
        "**Lazy Service Initialization**: Services are declared as `Optional` types and initialized only when needed, optimizing resource usage and startup time. This approach prevents unnecessary API calls and connection establishment during class instantiation.\n",
        "\n",
        "**Separate Embedding Services**: The system maintains separate embedding instances for different purposes (`embeddings` for indexing, `chunker_embeddings` for semantic chunking) to avoid conflicts and enable independent configuration.\n",
        "\n",
        "**Session Tracking**: The session tracking mechanism (`current_session_sources`, `current_session_nodes`, `current_session_relationships`) enables incremental processing, allowing the system to work efficiently with new documents without reprocessing existing data.\n",
        "\n",
        "#### 3. Service Initialization and Configuration\n",
        "\n",
        "```python\n",
        "def _initialize_embedding_services(self) -> None:\n",
        "    \"\"\"Initialize embedding services.\"\"\"\n",
        "    # Initialize embeddings for general use (vector indexing)\n",
        "    self.embeddings = OpenAIEmbeddings()\n",
        "    \n",
        "    # Initialize separate embeddings instance for chunking to avoid conflicts\n",
        "    self.chunker_embeddings = OpenAIEmbeddings()\n",
        "    \n",
        "    # Initialize semantic chunker with configurable parameters\n",
        "    self.semantic_chunker = SemanticChunker(\n",
        "        embeddings=self.chunker_embeddings,\n",
        "        breakpoint_threshold_type=params.semantic_chunker_breakpoint_type,\n",
        "        breakpoint_threshold_amount=params.semantic_chunker_breakpoint_threshold,\n",
        "        min_chunk_size=params.semantic_chunker_min_chunk_size\n",
        "    )\n",
        "    \n",
        "def _initialize_llm(self) -> None:\n",
        "    \"\"\"Initialize LLM service.\"\"\"\n",
        "    self.llm = ChatOpenAI(model_name=params.model)\n",
        "    \n",
        "def _ensure_services_initialized(self) -> None:\n",
        "    \"\"\"Ensure all services are properly initialized, reinitialize if needed.\"\"\"\n",
        "    if self.embeddings is None or self.chunker_embeddings is None or self.semantic_chunker is None:\n",
        "        print(\"üîß Initializing embedding services...\")\n",
        "        self._initialize_embedding_services()\n",
        "    \n",
        "    if self.llm is None:\n",
        "        print(\"üîß Initializing LLM...\")\n",
        "        self._initialize_llm()\n",
        "```\n",
        "\n",
        "The service initialization system demonstrates sophisticated resource management:\n",
        "\n",
        "**Semantic Chunking Configuration**: The `SemanticChunker` is configured with parameters that control how documents are split into semantically coherent chunks. This intelligent chunking ensures that related concepts remain together, improving both graph extraction and vector search quality.\n",
        "\n",
        "**Service Isolation**: Separate embedding services prevent conflicts between different operations, ensuring that chunking operations don't interfere with indexing processes.\n",
        "\n",
        "**Robust Reinitialization**: The `_ensure_services_initialized` method provides a safety net that reinitializes services if they become unavailable, ensuring system resilience.\n",
        "\n",
        "#### 4. Environment Setup and Credential Management\n",
        "\n",
        "```python\n",
        "def setup_environment(self) -> None:\n",
        "    \"\"\"Load environment variables and validate required credentials.\"\"\"\n",
        "    print(\"üîß Setting up environment...\")\n",
        "    load_dotenv()\n",
        "    \n",
        "    # OpenAI API Key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "    \n",
        "    # Neo4j Credentials validation\n",
        "    self._validate_neo4j_credentials()\n",
        "    print(\"‚úÖ Environment setup complete\")\n",
        "    \n",
        "def _validate_neo4j_credentials(self) -> None:\n",
        "    \"\"\"Validate Neo4j connection credentials.\"\"\"\n",
        "    required_vars = [\"NEO4J_URI\", \"NEO4J_USERNAME\", \"NEO4J_PASSWORD\"]\n",
        "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "    \n",
        "    if missing_vars:\n",
        "        raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
        "```\n",
        "\n",
        "The environment setup system implements comprehensive credential management:\n",
        "\n",
        "**Interactive Credential Collection**: The system can prompt for missing credentials interactively, improving usability in development environments while maintaining security.\n",
        "\n",
        "**Comprehensive Validation**: All required credentials are validated upfront, preventing runtime failures and providing clear error messages.\n",
        "\n",
        "**Security Best Practices**: Environment variables are used for sensitive information, following security best practices for credential management.\n",
        "\n",
        "#### 5. Neo4j Connection and Version Compatibility\n",
        "\n",
        "```python\n",
        "def initialize_neo4j_connection(self) -> None:\n",
        "    \"\"\"Initialize Neo4j graph connection.\"\"\"\n",
        "    print(\"üîó Initializing Neo4j connection...\")\n",
        "    \n",
        "    self.graph = Neo4jGraph(\n",
        "        url=os.getenv(\"NEO4J_URI\"),\n",
        "        username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "        password=os.getenv(\"NEO4J_PASSWORD\"),\n",
        "        refresh_schema=False,\n",
        "    )\n",
        "    \n",
        "    # Check Neo4j version for compatibility\n",
        "    self._check_neo4j_version()\n",
        "    \n",
        "    print(\"‚úÖ Neo4j connection established\")\n",
        "    \n",
        "def _check_neo4j_version(self) -> None:\n",
        "    \"\"\"Check Neo4j version and log compatibility information.\"\"\"\n",
        "    try:\n",
        "        version_result = self.graph.query(\"\"\"CALL dbms.components() \n",
        "                                             YIELD name, versions, edition \n",
        "                                             WHERE name = 'Neo4j Kernel' \n",
        "                                             RETURN versions[0] as version\"\"\")\n",
        "        if version_result:\n",
        "            version = version_result[0]['version']\n",
        "            print(f\"‚ÑπÔ∏è  Neo4j version: {version}\")\n",
        "            \n",
        "            # Parse version to determine vector index syntax\n",
        "            major_version = int(version.split('.')[0])\n",
        "            if major_version >= 5:\n",
        "                print(\"‚ÑπÔ∏è  Using Neo4j 5+ vector index syntax\")\n",
        "                # Test if relationship vector indexes are supported\n",
        "                self._test_relationship_vector_support()\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è  Using legacy Neo4j vector index syntax\")\n",
        "                print(\"‚ö†Ô∏è  Relationship vector indexes may not be supported in this version\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  Could not determine Neo4j version\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ÑπÔ∏è  Could not check Neo4j version: {e}\")\n",
        "```\n",
        "\n",
        "The Neo4j connection system demonstrates sophisticated version compatibility management:\n",
        "\n",
        "**Version-Aware Initialization**: The system automatically detects the Neo4j version and adapts its behavior accordingly, ensuring compatibility across different Neo4j deployments.\n",
        "\n",
        "**Feature Detection**: The system tests for advanced features like relationship vector indexes, providing clear feedback about available capabilities.\n",
        "\n",
        "**Graceful Degradation**: Version detection failures don't prevent system operation, ensuring robustness in various deployment scenarios.\n",
        "\n",
        "#### 6. LLM Graph Transformer Configuration\n",
        "\n",
        "```python\n",
        "def setup_llm_and_transformer(self) -> None:\n",
        "    \"\"\"Setup LLM and configure the graph transformer with parameters.\"\"\"\n",
        "    print(\"ü§ñ Setting up LLM and graph transformer...\")\n",
        "    \n",
        "    # Ensure all services are initialized\n",
        "    self._ensure_services_initialized()\n",
        "    \n",
        "    # Build transformer with pre-initialized LLM\n",
        "    transformer_params = self._build_transformer_parameters()\n",
        "    self.transformer = LLMGraphTransformer(**transformer_params)\n",
        "    \n",
        "    print(\"‚úÖ LLM and transformer setup complete\")\n",
        "    \n",
        "def _build_transformer_parameters(self) -> Dict[str, Any]:\n",
        "    \"\"\"Build transformer parameters based on configuration.\"\"\"\n",
        "    transformer_params = {\"llm\": self.llm}\n",
        "    \n",
        "    if params.use_custom_prompt and params.custom_prompt:\n",
        "        # Use custom prompt configuration\n",
        "        custom_chat_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", params.custom_prompt),\n",
        "            (\"human\", \"{input}\")\n",
        "        ])\n",
        "        transformer_params[\"prompt\"] = custom_chat_prompt\n",
        "    else:\n",
        "        # Use schema constraints\n",
        "        self._add_schema_constraints(transformer_params)\n",
        "    \n",
        "    # Add property configurations\n",
        "    self._add_property_configurations(transformer_params)\n",
        "    \n",
        "    # Add additional instructions\n",
        "    if params.use_additional_instructions and params.additional_instructions:\n",
        "        transformer_params[\"additional_instructions\"] = params.additional_instructions\n",
        "    \n",
        "    return transformer_params\n",
        "```\n",
        "\n",
        "The LLM graph transformer configuration system demonstrates sophisticated customization capabilities:\n",
        "\n",
        "**Flexible Prompt Configuration**: The system supports both custom prompts and schema-based constraints, enabling fine-tuned control over graph extraction behavior.\n",
        "\n",
        "**Schema Constraints**: The system can constrain the types of nodes and relationships extracted, ensuring consistency with domain-specific requirements.\n",
        "\n",
        "**Property Configuration**: The transformer can be configured to extract specific properties for nodes and relationships, enabling rich graph representations.\n",
        "\n",
        "#### 7. Document Loading and Semantic Chunking\n",
        "\n",
        "```python\n",
        "def load_documents(self) -> None:\n",
        "    \"\"\"Load documents from configured source.\"\"\"\n",
        "    print(\"üìÑ Loading documents...\")\n",
        "    self._load_from_docs_folder()\n",
        "    \n",
        "    if not self.documents:\n",
        "        raise ValueError(\"No documents found. Please check your data source.\")\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(self.documents)} document(s)\")\n",
        "    \n",
        "def _chunk_document(self, content: str, source_path: str, text_splitter: SemanticChunker) -> List[Document]:\n",
        "    \"\"\"Chunk a document's content into semantic chunks and return Document objects.\"\"\"\n",
        "    chunks = text_splitter.split_text(content)\n",
        "    chunked_documents = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_metadata = {\n",
        "            \"source\": source_path,\n",
        "            \"chunk_index\": i,\n",
        "            \"total_chunks\": len(chunks)\n",
        "        }\n",
        "        chunked_documents.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
        "    \n",
        "    return chunked_documents\n",
        "    \n",
        "def _load_from_docs_folder(self) -> None:\n",
        "    \"\"\"Load documents from docs/ folder and apply semantic chunking.\"\"\"\n",
        "    self.documents = []\n",
        "    \n",
        "    # Ensure semantic chunker is initialized\n",
        "    if self.semantic_chunker is None:\n",
        "        print(\"üîß Semantic chunker not initialized, reinitializing...\")\n",
        "        self._initialize_embedding_services()\n",
        "    \n",
        "    text_splitter = self.semantic_chunker\n",
        "    \n",
        "    # Load documents from configurable source path\n",
        "    for path in glob.glob(params.documents_source_path):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "            \n",
        "            # Chunk the document and add all chunks to documents list\n",
        "            chunked_docs = self._chunk_document(content, path, text_splitter)\n",
        "            self.documents.extend(chunked_docs)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not load {path}: {e}\")\n",
        "```\n",
        "\n",
        "The document loading system demonstrates intelligent text processing:\n",
        "\n",
        "**Semantic Chunking**: Unlike traditional fixed-size chunking, the system uses semantic similarity to determine optimal chunk boundaries, preserving conceptual coherence.\n",
        "\n",
        "**Rich Metadata**: Each chunk includes comprehensive metadata about its source and position, enabling sophisticated retrieval and reconstruction.\n",
        "\n",
        "**Error Resilience**: Individual document loading failures don't prevent processing of other documents, ensuring robust operation across diverse document collections.\n",
        "\n",
        "#### 8. Graph Extraction and Neo4j Ingestion\n",
        "\n",
        "```python\n",
        "def extract_graph_documents(self) -> List[Any]:\n",
        "    \"\"\"Extract graph structures from documents.\"\"\"\n",
        "    print(\"üîç Extracting graph structures...\")\n",
        "    \n",
        "    graph_docs = self.transformer.convert_to_graph_documents(self.documents)\n",
        "    \n",
        "    # Display first document's extracted elements\n",
        "    if graph_docs:\n",
        "        print(f\"Nodes: {graph_docs[0].nodes}\")\n",
        "        print(f\"Relationships: {graph_docs[0].relationships}\")\n",
        "    \n",
        "    print(f\"‚úÖ Extracted graph structures from {len(graph_docs)} document(s)\")\n",
        "    return graph_docs\n",
        "    \n",
        "def ingest_to_neo4j(self, graph_docs: List[Any]) -> None:\n",
        "    \"\"\"Ingest graph documents into Neo4j and track newly created nodes and relationships.\"\"\"\n",
        "    print(\"üíæ Ingesting data into Neo4j...\")\n",
        "    \n",
        "    # Track sources being processed in this session\n",
        "    for doc in self.documents:\n",
        "        source = doc.metadata.get('source', 'unknown')\n",
        "        self.current_session_sources.add(source)\n",
        "    \n",
        "    print(f\"üìÑ Processing sources: {sorted(self.current_session_sources)}\")\n",
        "    \n",
        "    # Ingest the graph documents\n",
        "    self.graph.add_graph_documents(\n",
        "        graph_docs,\n",
        "        baseEntityLabel=params.baseEntityLabel,\n",
        "        include_source=params.include_source,\n",
        "    )\n",
        "    \n",
        "    # Get newly created nodes and relationships for this session\n",
        "    self._collect_newly_created_elements()\n",
        "    \n",
        "    print(f\"‚úÖ Successfully ingested {len(graph_docs)} document(s) into Neo4j\")\n",
        "    print(f\"üìä New nodes: {len(self.current_session_nodes)}, New relationships: {len(self.current_session_relationships)}\")\n",
        "```\n",
        "\n",
        "The graph extraction and ingestion system demonstrates sophisticated data processing:\n",
        "\n",
        "**LLM-Powered Extraction**: The system uses large language models to intelligently identify entities, relationships, and concepts within documents, creating rich graph representations.\n",
        "\n",
        "**Session Tracking**: The system tracks which nodes and relationships are created in each session, enabling efficient incremental processing and targeted embedding generation.\n",
        "\n",
        "**Configurable Ingestion**: The ingestion process can be customized with different base entity labels and source inclusion options.\n",
        "\n",
        "#### 9. Parallel Vector Embedding Generation\n",
        "\n",
        "```python\n",
        "def create_vector_embeddings(self) -> None:\n",
        "    \"\"\"Create vector embeddings for graph properties if enabled.\"\"\"\n",
        "    if not params.add_vector_index:\n",
        "        print(\"‚ÑπÔ∏è  Vector embeddings disabled in configuration\")\n",
        "        return\n",
        "    \n",
        "    print(\"üîç Creating vector embeddings...\")\n",
        "    \n",
        "    try:\n",
        "        # Process nodes and relationships in parallel\n",
        "        self._parallel_process_embeddings()\n",
        "        \n",
        "        # Create vector indexes in parallel\n",
        "        self._parallel_create_vector_indexes()\n",
        "        \n",
        "        # Verify embeddings were created\n",
        "        self._verify_embeddings()\n",
        "        \n",
        "        print(\"‚úÖ Vector embeddings creation complete\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error creating vector embeddings: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "def _parallel_process_embeddings(self) -> None:\n",
        "    \"\"\"Process node and relationship embeddings in parallel.\"\"\"\n",
        "    \n",
        "    def process_node_embeddings():\n",
        "        \"\"\"Process embeddings for node properties.\"\"\"\n",
        "        print(\"üìã Processing node embeddings...\")\n",
        "        \n",
        "        nodes_data = self._get_nodes_data()\n",
        "        if not nodes_data:\n",
        "            print(\"‚ö†Ô∏è  No nodes found for embedding\")\n",
        "            return []\n",
        "        \n",
        "        properties_to_embed = self._collect_node_properties_to_embed(nodes_data)\n",
        "        \n",
        "        if properties_to_embed:\n",
        "            self._parallel_create_embeddings_batch(properties_to_embed, \"node\")\n",
        "            print(f\"‚úÖ Created embeddings for {len(properties_to_embed)} node properties\")\n",
        "            return properties_to_embed\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No suitable node properties found for embedding\")\n",
        "            return []\n",
        "    \n",
        "    def process_relationship_embeddings():\n",
        "        \"\"\"Process embeddings for relationship properties.\"\"\"\n",
        "        print(\"üìã Processing relationship embeddings...\")\n",
        "        \n",
        "        relationships_data = self._get_relationships_data()\n",
        "        if not relationships_data:\n",
        "            print(\"‚ö†Ô∏è  No relationships found for embedding\")\n",
        "            return []\n",
        "        \n",
        "        properties_to_embed = self._collect_relationship_properties_to_embed(relationships_data)\n",
        "        \n",
        "        if properties_to_embed:\n",
        "            self._parallel_create_embeddings_batch(properties_to_embed, \"relationship\")\n",
        "            print(f\"‚úÖ Created embeddings for {len(properties_to_embed)} relationship properties\")\n",
        "            return properties_to_embed\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No suitable relationship properties found for embedding\")\n",
        "            return []\n",
        "    \n",
        "    # Process nodes and relationships in parallel\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        node_future = executor.submit(process_node_embeddings)\n",
        "        rel_future = executor.submit(process_relationship_embeddings)\n",
        "        \n",
        "        # Wait for both to complete\n",
        "        node_properties = node_future.result()\n",
        "        rel_properties = rel_future.result()\n",
        "```\n",
        "\n",
        "The parallel embedding generation system demonstrates sophisticated optimization:\n",
        "\n",
        "**Parallel Processing Architecture**: The system processes node and relationship embeddings concurrently, significantly reducing processing time for large knowledge graphs.\n",
        "\n",
        "**Batch Processing**: Embeddings are generated in configurable batches, optimizing API usage and memory consumption.\n",
        "\n",
        "**Selective Embedding**: The system only creates embeddings for properties that meet specific criteria, avoiding unnecessary computational overhead.\n",
        "\n",
        "#### 10. Multi-Vector Property Enhancement\n",
        "\n",
        "```python\n",
        "def add_multivectors_to_document_nodes(self) -> None:\n",
        "    \"\"\"Add custom properties to Document nodes from current session only.\"\"\"\n",
        "    if not hasattr(params, 'document_multi_vector_properties') or not params.document_multi_vector_properties:\n",
        "        print(\"‚ÑπÔ∏è  No document node properties to add\")\n",
        "        return\n",
        "    \n",
        "    print(\"üîß Adding custom properties to Document nodes from current session...\")\n",
        "    \n",
        "    try:\n",
        "        # Get Document nodes from current session sources only\n",
        "        sources_list = list(self.current_session_sources)\n",
        "        document_nodes = self.graph.query(\"\"\"\n",
        "            MATCH (d:Document) \n",
        "            WHERE any(source IN $sources WHERE d.source CONTAINS source OR d.source = source)\n",
        "            RETURN elementId(d) as node_id, d.text as text, d.source as source\n",
        "        \"\"\", {\"sources\": sources_list})\n",
        "        \n",
        "        # Parallel processing of document properties\n",
        "        added_properties, properties_to_embed = self._parallel_process_document_properties(document_nodes)\n",
        "        \n",
        "        # Create vector embeddings for the new properties\n",
        "        if params.add_vector_index and properties_to_embed:\n",
        "            print(\"üîç Creating vector embeddings for new document properties...\")\n",
        "            \n",
        "            # Create embeddings in parallel batches\n",
        "            self._parallel_create_embeddings_batch(properties_to_embed, \"document_node\")\n",
        "            \n",
        "            # Create vector indexes for the new properties\n",
        "            self._parallel_create_document_property_indexes(added_properties)\n",
        "            \n",
        "            print(\"‚úÖ Vector embeddings for document properties created successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error adding properties to Document nodes: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "def _parallel_process_document_properties(self, document_nodes: List[Dict]) -> Tuple[set, List[Dict]]:\n",
        "    \"\"\"Process document properties in parallel using ThreadPoolExecutor.\"\"\"\n",
        "    \n",
        "    def process_single_property(doc_node: Dict, prop_config: Dict) -> Dict:\n",
        "        \"\"\"Process a single document-property combination.\"\"\"\n",
        "        node_id = doc_node['node_id']\n",
        "        document_text = doc_node['text']\n",
        "        source = doc_node.get('source', 'unknown')\n",
        "        property_name = prop_config.get('property_name')\n",
        "        prompt = prop_config.get('prompt')\n",
        "        \n",
        "        try:\n",
        "            # Create the full prompt with document text\n",
        "            full_prompt = f\"{prompt}\\n\\nDocument text:\\n{document_text}\"\n",
        "            \n",
        "            # Call LLM to generate the property value\n",
        "            response = self.llm.invoke(full_prompt)\n",
        "            property_value = response.content.strip()\n",
        "            \n",
        "            return {\n",
        "                'node_id': node_id,\n",
        "                'property_name': property_name,\n",
        "                'property_value': property_value,\n",
        "                'success': True\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': f\"Error processing {property_name} for {source}: {e}\",\n",
        "                'success': False\n",
        "            }\n",
        "    \n",
        "    # Create all combinations of documents and properties\n",
        "    tasks = []\n",
        "    for doc_node in document_nodes:\n",
        "        for prop_config in params.document_multi_vector_properties:\n",
        "            tasks.append((doc_node, prop_config))\n",
        "    \n",
        "    # Process in parallel with optimal number of workers\n",
        "    max_workers = min(8, len(tasks), os.cpu_count() or 4)\n",
        "    results = []\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_task = {\n",
        "            executor.submit(process_single_property, doc_node, prop_config): (doc_node, prop_config)\n",
        "            for doc_node, prop_config in tasks\n",
        "        }\n",
        "        \n",
        "        # Collect results as they complete\n",
        "        for future in as_completed(future_to_task):\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "    \n",
        "    # Process results and update Neo4j in batches\n",
        "    return self._batch_update_document_properties(results)\n",
        "```\n",
        "\n",
        "The multi-vector property enhancement system demonstrates advanced AI integration:\n",
        "\n",
        "**Dynamic Property Generation**: The system uses LLMs to generate custom properties for document nodes based on configurable prompts, creating multiple semantic representations of the same content.\n",
        "\n",
        "**Parallel Processing**: Document property generation is performed in parallel, dramatically improving processing speed for large document collections.\n",
        "\n",
        "**Comprehensive Embeddings**: Generated properties are automatically converted to embeddings and indexed, enabling multi-dimensional search capabilities.\n",
        "\n",
        "#### 11. Main Execution Pipeline\n",
        "\n",
        "```python\n",
        "def run_ingestion(self) -> None:\n",
        "    \"\"\"Execute the complete ingestion process.\"\"\"\n",
        "    print(\"üöÄ Starting Knowledge Graph Ingestion Process\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Reset session tracking for this run\n",
        "        self._reset_session_tracking()\n",
        "        \n",
        "        # Setup phase\n",
        "        self.setup_environment()\n",
        "        self.initialize_neo4j_connection()\n",
        "        self.setup_llm_and_transformer()\n",
        "        \n",
        "        # Data processing phase\n",
        "        self.load_documents()\n",
        "        graph_docs = self.extract_graph_documents()\n",
        "        \n",
        "        # Ingestion phase\n",
        "        self.ingest_to_neo4j(graph_docs)\n",
        "\n",
        "        # Vector embeddings phase (optional) - only for current session\n",
        "        self.create_vector_embeddings()\n",
        "        \n",
        "        # Add custom properties to Document nodes - only for current session\n",
        "        self.add_multivectors_to_document_nodes()\n",
        "        \n",
        "        print(\"=\" * 50)\n",
        "        print(\"üéâ Knowledge Graph Ingestion Process Completed Successfully!\")\n",
        "        print(f\"üìä Processed {len(self.current_session_sources)} source files\")\n",
        "        print(f\"üìä Created embeddings for {len(self.current_session_nodes)} nodes and {len(self.current_session_relationships)} relationships\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ingestion process failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "        \n",
        "def main():\n",
        "    \"\"\"Main function to run the ingestion process.\"\"\"\n",
        "    ingestion = KnowledgeGraphIngestion()\n",
        "    ingestion.run_ingestion()\n",
        "```\n",
        "\n",
        "The main execution pipeline demonstrates comprehensive orchestration:\n",
        "\n",
        "**Phase-Based Processing**: The ingestion process is organized into clear phases (setup, processing, ingestion, enhancement), each with specific responsibilities and error handling.\n",
        "\n",
        "**Session Management**: Each ingestion run starts with a clean session state, enabling consistent processing and accurate tracking.\n",
        "\n",
        "**Comprehensive Reporting**: The system provides detailed progress reporting and final statistics, enabling easy monitoring and troubleshooting.\n",
        "\n",
        "**Robust Error Handling**: Comprehensive error handling ensures that failures are well-documented and don't leave the system in an inconsistent state.\n",
        "\n",
        "### System Architecture Summary\n",
        "\n",
        "The `KnowledgeGraphIngestion` system represents a sophisticated synthesis of multiple AI technologies:\n",
        "\n",
        "1. **Semantic Document Processing**: Intelligent chunking and text analysis\n",
        "2. **LLM-Powered Graph Extraction**: Entity and relationship identification\n",
        "3. **Parallel Vector Processing**: Efficient embedding generation and indexing\n",
        "4. **Multi-Vector Enhancement**: Dynamic property generation for enhanced search\n",
        "5. **Session-Based Tracking**: Efficient incremental processing capabilities\n",
        "\n",
        "The system's modular design enables easy extension and customization while maintaining robust performance and comprehensive error handling. The parallel processing architecture ensures efficient resource utilization, while the session tracking mechanism enables incremental updates without reprocessing entire datasets.\n",
        "\n",
        "This ingestion system creates the rich, multi-dimensional knowledge graph that serves as the foundation for the advanced retrieval and reasoning capabilities we'll explore in the following sections. The combination of semantic chunking, LLM-powered extraction, and multi-vector embeddings creates a knowledge representation that supports both precise similarity search and comprehensive graph traversal, enabling the hybrid RAG capabilities that make this system so powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HybridRAGQuery Architecture: The Query Processing Engine\n",
        "\n",
        "### Introduction to HybridRAGQuery\n",
        "\n",
        "The `HybridRAGQuery` class represents the core query processing engine of our hybrid RAG system, serving as the orchestrator that seamlessly integrates vector similarity search with advanced graph traversal techniques. This sophisticated architecture implements a two-phase retrieval strategy: first leveraging vector embeddings to identify semantically relevant document chunks, then employing intelligent graph traversal algorithms to discover additional contextual information through the knowledge graph's relationship structure.\n",
        "\n",
        "The system's revolutionary approach lies in its ability to combine the semantic understanding of vector embeddings with the structural intelligence of graph relationships. While traditional RAG systems rely solely on vector similarity, often missing crucial contextual information that may be semantically distant but structurally related, our hybrid approach ensures comprehensive information retrieval by exploring both semantic and structural dimensions of the knowledge graph.\n",
        "\n",
        "### Core Architecture Principles\n",
        "\n",
        "The HybridRAGQuery architecture is built on several fundamental principles that ensure both performance and flexibility:\n",
        "\n",
        "1. **Singleton Pattern**: Ensures efficient resource utilization by maintaining a single instance of the query processor throughout the application lifecycle\n",
        "2. **Lazy Initialization**: Services are initialized only when needed, reducing startup time and resource consumption\n",
        "3. **Multi-Vector Search**: Supports multiple embedding properties and node types for comprehensive document retrieval\n",
        "4. **Pluggable Traversal Algorithms**: Modular design allows for easy integration of different graph traversal strategies\n",
        "5. **Intelligent Deduplication**: Sophisticated merging of results from different search phases prevents redundant information\n",
        "6. **Comprehensive Error Handling**: Robust error management ensures system stability even when individual components fail\n",
        "\n",
        "### Detailed Code Analysis\n",
        "\n",
        "#### 1. Import Dependencies and Configuration\n",
        "\n",
        "```python\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from params import (\n",
        "    model, \n",
        "    VECTOR_SEARCH_CONFIGURATIONS, \n",
        "    TOP_K_INITIAL, \n",
        "    TOP_K_TRAVERSAL, \n",
        "    ACTIVATE_INITIAL_VECTOR_SEARCH,\n",
        "    GRAPH_TRAVERSAL_METHOD\n",
        ")\n",
        "import json\n",
        "import logging\n",
        "from prompts import FINAL_ANSWER_SYSTEM_PROMPT\n",
        "```\n",
        "\n",
        "This opening section establishes the foundational dependencies and configuration parameters that drive the entire hybrid RAG system. The import structure reveals the multi-layered architecture:\n",
        "\n",
        "- **Environment Management**: `os` and `dotenv` handle secure credential management and environment configuration\n",
        "- **Type Safety**: `typing` provides comprehensive type annotations for better code reliability and IDE support\n",
        "- **LangChain Integration**: Multiple LangChain components (`OpenAIEmbeddings`, `Neo4jGraph`, `ChatOpenAI`) create a unified interface for different AI services\n",
        "- **Message Abstractions**: `HumanMessage` and `SystemMessage` provide structured interfaces for LLM communication\n",
        "- **Configuration Management**: The `params` module centralizes all configurable parameters, enabling easy tuning of system behavior\n",
        "- **Logging Infrastructure**: Proper logging setup ensures comprehensive system monitoring and debugging capabilities\n",
        "\n",
        "The configuration parameters imported from `params` control critical system behavior:\n",
        "- `VECTOR_SEARCH_CONFIGURATIONS`: Defines which embedding properties and node types to search across\n",
        "- `TOP_K_INITIAL` and `TOP_K_TRAVERSAL`: Control the number of results returned in each phase\n",
        "- `ACTIVATE_INITIAL_VECTOR_SEARCH`: Allows toggling of the initial vector search phase\n",
        "- `GRAPH_TRAVERSAL_METHOD`: Specifies which graph traversal algorithm to employ\n",
        "\n",
        "#### 2. Graph Traversal Algorithm Imports\n",
        "\n",
        "```python\n",
        "# Import the traversal classes\n",
        "from traversal.context_to_cypher import ContextToCypher\n",
        "from traversal.khop_limited_bfs import KhopLimitedBFS\n",
        "from traversal.khop_limited_bfs_pred_llm import KhopLimitedBFSWithLLM\n",
        "from traversal.depth_limited_dfs import DepthLimitedDFS\n",
        "from traversal.depth_limited_dfs_pred_llm import DepthLimitedDFSWithLLM\n",
        "from traversal.uniform_cost_search_ucs import UniformCostSearchUCS\n",
        "from traversal.uniform_cost_search_ucs_pred_llm import UniformCostSearchUCSWithLLM\n",
        "from traversal.astar_search_heuristic import AStarSearchHeuristic\n",
        "from traversal.astar_search_heuristic_pred_llm import AStarSearchHeuristicWithLLM\n",
        "from traversal.beam_search_over_the_graph import BeamSearchOverGraph\n",
        "from traversal.beam_search_over_the_graph_pred_llm import BeamSearchOverGraphWithLLM\n",
        "```\n",
        "\n",
        "This comprehensive import section reveals the modular architecture of the graph traversal system. Each algorithm is implemented as a separate class, following the strategy pattern that allows for easy algorithm selection and extension. The naming convention reveals the sophisticated algorithm taxonomy:\n",
        "\n",
        "- **Base Algorithms**: Core traversal strategies like BFS, DFS, UCS, A*, and Beam Search\n",
        "- **LLM-Enhanced Variants**: Versions with `_pred_llm` suffix that incorporate large language model intelligence for relationship filtering\n",
        "- **Specialized Algorithms**: `ContextToCypher` represents the revolutionary approach of using LLMs to generate custom Cypher queries\n",
        "\n",
        "This modular design enables the system to select the most appropriate traversal strategy based on query characteristics, graph structure, and performance requirements. The availability of both basic and LLM-enhanced variants provides flexibility in balancing performance with intelligent exploration.\n",
        "\n",
        "#### 3. Class Declaration and Singleton Pattern\n",
        "\n",
        "```python\n",
        "class HybridRAGQuery:\n",
        "    \"\"\"\n",
        "    A singleton class for performing vector similarity search on Document nodes in Neo4j\n",
        "    to retrieve relevant context based on user queries.\n",
        "    Initialized only once when the first instance is created.\n",
        "    \"\"\"\n",
        "    \n",
        "    _instance = None\n",
        "    _initialized = False\n",
        "    \n",
        "    def __new__(cls):\n",
        "        \"\"\"Ensure only one instance of HybridRAGQuery exists.\"\"\"\n",
        "        if cls._instance is None:\n",
        "            cls._instance = super(HybridRAGQuery, cls).__new__(cls)\n",
        "        return cls._instance\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the context retrieval system - only runs once.\"\"\"\n",
        "        if not HybridRAGQuery._initialized:\n",
        "            print(\"üîß Initializing HybridRAGQuery singleton...\")\n",
        "            \n",
        "            # Initialize all services immediately\n",
        "            self._setup_environment()\n",
        "            self._initialize_embedding_service()\n",
        "            self._initialize_neo4j_connection()\n",
        "            \n",
        "            HybridRAGQuery._initialized = True\n",
        "            print(\"‚úÖ HybridRAGQuery singleton initialized successfully\")\n",
        "```\n",
        "\n",
        "The singleton pattern implementation demonstrates sophisticated resource management and initialization control. This design pattern is particularly valuable in the context of a hybrid RAG system for several reasons:\n",
        "\n",
        "**Resource Efficiency**: Database connections and embedding services are expensive to initialize. The singleton pattern ensures these resources are created only once and reused throughout the application lifecycle, significantly reducing startup time and memory consumption.\n",
        "\n",
        "**State Consistency**: The singleton maintains consistent state across different parts of the application, ensuring that configuration settings, database connections, and embedding services remain stable and accessible.\n",
        "\n",
        "**Initialization Control**: The `_initialized` flag prevents multiple initialization attempts, which could lead to resource conflicts or inconsistent state. The immediate initialization of all services ensures that the system is ready to handle queries as soon as the singleton is created.\n",
        "\n",
        "**Thread Safety Considerations**: While this implementation is suitable for single-threaded applications, production systems might require additional thread-safety mechanisms using locks or thread-local storage.\n",
        "\n",
        "#### 4. Environment Setup and Credential Management\n",
        "\n",
        "```python\n",
        "def _setup_environment(self) -> None:\n",
        "    \"\"\"Load environment variables and validate required credentials.\"\"\"\n",
        "    print(\"üîß Setting up environment...\")\n",
        "    load_dotenv()\n",
        "    \n",
        "    # OpenAI API Key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n",
        "    \n",
        "    # Neo4j Credentials validation\n",
        "    self._validate_neo4j_credentials()\n",
        "    print(\"‚úÖ Environment setup complete\")\n",
        "\n",
        "def _validate_neo4j_credentials(self) -> None:\n",
        "    \"\"\"Validate Neo4j connection credentials.\"\"\"\n",
        "    required_vars = [\"NEO4J_URI\", \"NEO4J_USERNAME\", \"NEO4J_PASSWORD\"]\n",
        "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "    \n",
        "    if missing_vars:\n",
        "        raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
        "```\n",
        "\n",
        "The environment setup system implements comprehensive credential validation and secure configuration management. This approach ensures that the system fails fast if required credentials are missing, preventing runtime errors during query processing.\n",
        "\n",
        "**Security Best Practices**: The system uses environment variables for sensitive credentials, following security best practices that prevent hardcoded secrets in the codebase. The `.env` file support enables convenient development workflows while maintaining security.\n",
        "\n",
        "**Comprehensive Validation**: The credential validation checks for all required Neo4j connection parameters, providing clear error messages that specify exactly which credentials are missing. This explicit validation prevents cryptic connection errors later in the initialization process.\n",
        "\n",
        "**Fail-Fast Philosophy**: By validating credentials during initialization rather than at first use, the system ensures that configuration issues are identified immediately, making debugging and deployment much more reliable.\n",
        "\n",
        "#### 5. Service Initialization Methods\n",
        "\n",
        "```python\n",
        "def _initialize_embedding_service(self) -> None:\n",
        "    \"\"\"Initialize embedding service for query processing.\"\"\"\n",
        "    print(\"ü§ñ Initializing embedding service...\")\n",
        "    try:\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        print(\"‚úÖ Embedding service initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to initialize embedding service: {e}\")\n",
        "        raise\n",
        "\n",
        "def _initialize_neo4j_connection(self) -> None:\n",
        "    \"\"\"Initialize Neo4j graph connection.\"\"\"\n",
        "    print(\"üîó Initializing Neo4j connection...\")\n",
        "    \n",
        "    self.graph = Neo4jGraph(\n",
        "        url=os.getenv(\"NEO4J_URI\"),\n",
        "        username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "        password=os.getenv(\"NEO4J_PASSWORD\"),\n",
        "        refresh_schema=False,\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Neo4j connection established\")\n",
        "```\n",
        "\n",
        "The service initialization methods demonstrate robust error handling and performance optimization. Each service is initialized with specific configurations that optimize for the hybrid RAG use case:\n",
        "\n",
        "**Embedding Service Configuration**: The `OpenAIEmbeddings` service is configured with default parameters that work well for most document retrieval scenarios. The error handling ensures that initialization failures are caught and reported clearly.\n",
        "\n",
        "**Neo4j Connection Optimization**: The `refresh_schema=False` parameter prevents automatic schema refreshing, which can be expensive in large databases. This optimization improves connection time while still maintaining full database functionality.\n",
        "\n",
        "**Error Propagation**: Both methods implement comprehensive error handling that catches initialization failures and propagates them with clear error messages. This approach ensures that system administrators can quickly identify and resolve configuration issues.\n",
        "\n",
        "#### 6. Vector Index Management and Discovery\n",
        "\n",
        "```python\n",
        "def list_all_vector_indexes(self) -> None:\n",
        "    \"\"\"List all available vector indexes in the Neo4j database.\"\"\"\n",
        "    print(\"\\nüìã Available Vector Indexes:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Query to get all vector indexes\n",
        "        result = self.graph.query(\"\"\"\n",
        "            SHOW INDEXES\n",
        "            YIELD name, type, labelsOrTypes, properties, state\n",
        "            WHERE type = 'VECTOR'\n",
        "            RETURN name, labelsOrTypes, properties, state\n",
        "            ORDER BY name\n",
        "        \"\"\")\n",
        "        \n",
        "        if not result:\n",
        "            print(\"‚ö†Ô∏è  No vector indexes found in the database\")\n",
        "            return\n",
        "        \n",
        "        for idx, index_info in enumerate(result, 1):\n",
        "            name = index_info.get('name', 'Unknown')\n",
        "            labels = index_info.get('labelsOrTypes', [])\n",
        "            properties = index_info.get('properties', [])\n",
        "            state = index_info.get('state', 'Unknown')\n",
        "            \n",
        "            print(f\"{idx}. Index: {name}\")\n",
        "            print(f\"   Labels: {labels}\")\n",
        "            print(f\"   Properties: {properties}\")\n",
        "            print(f\"   State: {state}\")\n",
        "            print()\n",
        "        \n",
        "        print(f\"üìä Total vector indexes found: {len(result)}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error listing vector indexes: {e}\")\n",
        "```\n",
        "\n",
        "The vector index management system provides comprehensive discovery and validation capabilities for the multi-vector search architecture. This introspection capability is crucial for understanding and debugging the search system:\n",
        "\n",
        "**Dynamic Index Discovery**: The system dynamically discovers available vector indexes rather than relying on hardcoded configurations. This approach ensures that the search system can adapt to changes in the database schema without requiring code modifications.\n",
        "\n",
        "**Comprehensive Index Information**: The method retrieves detailed information about each index, including the node labels, properties, and current state. This information is essential for understanding which embedding properties are available for search and whether indexes are properly configured.\n",
        "\n",
        "**Error Resilience**: The error handling ensures that index discovery failures don't prevent the system from functioning, allowing for graceful degradation in case of database schema issues.\n",
        "\n",
        "#### 7. Multi-Vector Search Implementation\n",
        "\n",
        "```python\n",
        "def _search_by_index(\n",
        "    self, \n",
        "    query_embedding: List[float], \n",
        "    embedding_property: str, \n",
        "    top_k: int,\n",
        "    target_label: str = \"Document\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search nodes using a specific embedding property and label.\"\"\"\n",
        "    try:\n",
        "        # Construct the proper index name using both property and label\n",
        "        index_name = f\"embedding_{embedding_property}_{target_label.lower()}_index\"\n",
        "        \n",
        "        # Check if vector index exists for this property and label combination\n",
        "        if not self._check_vector_index_exists_with_label(embedding_property, target_label):\n",
        "            print(f\"‚ö†Ô∏è  Vector index '{index_name}' does not exist, skipping\")\n",
        "            return []\n",
        "        \n",
        "        # Perform vector similarity search using Neo4j's vector index\n",
        "        cypher_query = f\"\"\"\n",
        "            CALL db.index.vector.queryNodes('{index_name}', $top_k, $query_embedding)\n",
        "            YIELD node, score\n",
        "            WHERE node:{target_label}\n",
        "            RETURN \n",
        "                elementId(node) as node_id,\n",
        "                node.text as text,\n",
        "                node.source as source,\n",
        "                node.chunk_index as chunk_index,\n",
        "                node.total_chunks as total_chunks,\n",
        "                score,\n",
        "                'embedding_{embedding_property}' as search_type,\n",
        "                labels(node) as labels,\n",
        "                keys(node) as properties\n",
        "            ORDER BY score DESC\n",
        "            LIMIT $top_k\n",
        "        \"\"\"\n",
        "        \n",
        "        result = self.graph.query(cypher_query, {\n",
        "            \"query_embedding\": query_embedding,\n",
        "            \"top_k\": top_k\n",
        "        })\n",
        "        \n",
        "        print(f\"üìä Found {len(result)} results using {index_name}\")\n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error searching by {index_name}: {e}\")\n",
        "        return []\n",
        "```\n",
        "\n",
        "The multi-vector search implementation represents the core innovation of the hybrid RAG system's retrieval phase. This sophisticated approach enables searching across multiple semantic representations of the same content:\n",
        "\n",
        "**Dynamic Index Construction**: The system dynamically constructs index names based on the embedding property and target label, enabling flexible search across different vector representations without hardcoded dependencies.\n",
        "\n",
        "**Comprehensive Result Metadata**: The Cypher query retrieves extensive metadata about each matching node, including chunk information, source details, and node properties. This metadata is crucial for subsequent graph traversal and result presentation.\n",
        "\n",
        "**Graceful Degradation**: The index existence check ensures that the system continues to function even if some vector indexes are missing, providing robustness in partially configured environments.\n",
        "\n",
        "**Performance Optimization**: The use of Neo4j's native vector index functions ensures optimal performance for similarity search operations, leveraging the database's optimized vector search capabilities.\n",
        "\n",
        "#### 8. Main Document Search Orchestration\n",
        "\n",
        "```python\n",
        "def search_similar_documents(self, user_query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search for Document nodes similar to the user query using vector similarity.\"\"\"\n",
        "    print(f\"üîç Searching for documents similar to: '{user_query}'\")\n",
        "    \n",
        "    try:\n",
        "        # Convert user query to embedding\n",
        "        query_embedding = self.embeddings.embed_query(user_query)\n",
        "        \n",
        "        # Store the query embedding for potential use in graph traversal\n",
        "        self.current_query_embedding = query_embedding\n",
        "        \n",
        "        # Search using all configured embedding properties and labels\n",
        "        similar_docs = []\n",
        "        \n",
        "        for embedding_property, target_label in VECTOR_SEARCH_CONFIGURATIONS:\n",
        "            print(f\"üîç Searching using embedding_{embedding_property} on {target_label} nodes...\")\n",
        "            \n",
        "            search_results = self._search_by_index(\n",
        "                query_embedding, embedding_property, top_k, target_label\n",
        "            )\n",
        "            \n",
        "            if search_results:\n",
        "                similar_docs.extend(search_results)\n",
        "                print(f\"   ‚úÖ Found {len(search_results)} results from embedding_{embedding_property}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  No results from embedding_{embedding_property} on {target_label}\")\n",
        "        \n",
        "        # Remove duplicates and sort by similarity score\n",
        "        unique_docs = self._deduplicate_and_sort_results(similar_docs, top_k)\n",
        "        \n",
        "        print(f\"‚úÖ Found {len(unique_docs)} similar document(s) total\")\n",
        "        return unique_docs\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error searching for similar documents: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "```\n",
        "\n",
        "The main document search orchestration demonstrates the sophisticated multi-vector search strategy that sets this system apart from traditional RAG approaches:\n",
        "\n",
        "**Multi-Vector Strategy**: The system searches across multiple embedding properties and node types as configured in `VECTOR_SEARCH_CONFIGURATIONS`, enabling comprehensive retrieval across different semantic representations of the same content.\n",
        "\n",
        "**Query Embedding Persistence**: The query embedding is stored in the instance for potential use by graph traversal algorithms, particularly A* search which uses it as a heuristic target.\n",
        "\n",
        "**Comprehensive Result Aggregation**: Results from all configured search properties are aggregated and deduplicated, ensuring comprehensive coverage while avoiding redundant information.\n",
        "\n",
        "**Robust Error Handling**: The extensive error handling with stack trace printing ensures that search failures are well-documented for debugging purposes.\n",
        "\n",
        "#### 9. Answer Generation and Context Formatting\n",
        "\n",
        "```python\n",
        "def generate_final_answer(self, context: List[Dict[str, Any]], user_query: str) -> str:\n",
        "    \"\"\"Generate a final answer to the user's query based on the provided context.\"\"\"\n",
        "    print(f\"ü§ñ Generating final answer based on {len(context)} context documents...\")\n",
        "    \n",
        "    try:\n",
        "        # Initialize ChatOpenAI\n",
        "        chat = ChatOpenAI(model=model)\n",
        "        \n",
        "        # Format the context with all properties\n",
        "        formatted_context = self._format_context_for_llm(context)\n",
        "        \n",
        "        # Create a comprehensive system prompt\n",
        "        system_prompt = FINAL_ANSWER_SYSTEM_PROMPT\n",
        "        \n",
        "        # Create the user prompt with context and query\n",
        "        user_prompt = f\"\"\"Context Information:\n",
        "{formatted_context}\n",
        "\n",
        "User Query: {user_query}\n",
        "\n",
        "Please provide a comprehensive answer based on the context provided above.\"\"\"\n",
        "        \n",
        "        # Generate response using LangChain\n",
        "        messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=user_prompt)\n",
        "        ]\n",
        "        \n",
        "        response = chat.invoke(messages)\n",
        "        \n",
        "        print(\"‚úÖ Final answer generated successfully\")\n",
        "        return response.content\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error generating final answer: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return \"I'm sorry, but I couldn't generate a response to your query due to a technical error.\"\n",
        "```\n",
        "\n",
        "The answer generation system demonstrates sophisticated context formatting and LLM integration:\n",
        "\n",
        "**Structured Context Formatting**: The system formats the retrieved context into a structured JSON format that clearly distinguishes between initial documents and additional nodes from graph traversal.\n",
        "\n",
        "**Template-Based Prompting**: The use of system and user prompts provides clear instructions to the LLM while maintaining separation of concerns between system behavior and user queries.\n",
        "\n",
        "**Robust Error Handling**: The comprehensive error handling ensures that LLM failures result in helpful error messages rather than system crashes.\n",
        "\n",
        "#### 10. Main Execution Flow\n",
        "\n",
        "```python\n",
        "def main():\n",
        "    \"\"\"Example usage of the HybridRAGQuery singleton class.\"\"\"\n",
        "    print(\"üöÄ Testing HybridRAGQuery Singleton\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize the singleton\n",
        "    context_retriever = HybridRAGQuery()\n",
        "    \n",
        "    query = \"Draw a three-generation family graph starting with Abraham Einstein...\"\n",
        "    \n",
        "    print(\"\\n\" + \"üîÑ PHASE 1: VECTOR SEARCH\" + \"\\n\" + \"=\" * 50)\n",
        "    if ACTIVATE_INITIAL_VECTOR_SEARCH:\n",
        "        print(\"üîç Performing initial vector search...\")\n",
        "        initial_nodes = context_retriever.get_document_context(query, top_k=TOP_K_INITIAL)\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è  Skipping initial vector search\")\n",
        "        initial_nodes = []\n",
        "    \n",
        "    print(\"\\n\" + \"üîÑ PHASE 2: GRAPH TRAVERSAL ANALYSIS\" + \"\\n\" + \"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Initialize graph traversal method based on configuration\n",
        "        if GRAPH_TRAVERSAL_METHOD == \"kop_limited_bfs\":\n",
        "            traversal_method = KhopLimitedBFS()\n",
        "        elif GRAPH_TRAVERSAL_METHOD == \"context_to_cypher\":\n",
        "            traversal_method = ContextToCypher()\n",
        "        # ... other algorithm selections ...\n",
        "        \n",
        "        # Get additional context through graph traversal\n",
        "        additional_context = traversal_method.traverse_graph(initial_nodes, query)\n",
        "        \n",
        "        # Combine initial and additional context with deduplication\n",
        "        all_context = _deduplicate_contexts(initial_nodes, additional_context, TOP_K_TRAVERSAL)\n",
        "        \n",
        "        print(f\"\\nüìä FINAL CONTEXT SUMMARY:\")\n",
        "        print(f\"Initial documents: {len(initial_nodes)}\")\n",
        "        print(f\"Additional documents from graph traversal: {len(additional_context)}\")\n",
        "        print(f\"Total context documents: {len(all_context)}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error during graph traversal: {e}\")\n",
        "        all_context = initial_nodes[:TOP_K_TRAVERSAL]\n",
        "    \n",
        "    print(\"\\n\" + \"üîÑ PHASE 3: ANSWER GENERATION\" + \"\\n\" + \"=\" * 50)\n",
        "    \n",
        "    if all_context:\n",
        "        final_answer = context_retriever.generate_final_answer(all_context, query)\n",
        "        print(\"\\n\" + \"üéØ FINAL ANSWER\" + \"\\n\" + \"=\" * 50)\n",
        "        print(final_answer)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No context available to generate an answer.\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üèÅ Query processing completed!\")\n",
        "```\n",
        "\n",
        "The main execution flow demonstrates the complete hybrid RAG pipeline in action:\n",
        "\n",
        "**Three-Phase Processing**: The system implements a clear three-phase approach: vector search, graph traversal, and answer generation. Each phase is clearly delineated with visual separators and progress indicators.\n",
        "\n",
        "**Configurable Algorithm Selection**: The extensive algorithm selection logic demonstrates the flexibility of the system, allowing for easy experimentation with different traversal strategies.\n",
        "\n",
        "**Robust Error Handling**: The comprehensive error handling in the graph traversal phase ensures that the system can fall back to vector search results if graph traversal fails.\n",
        "\n",
        "**Comprehensive Reporting**: The detailed progress reporting and context summaries provide clear visibility into the system's operation, making it easy to understand what information was retrieved and how.\n",
        "\n",
        "### System Architecture Summary\n",
        "\n",
        "The HybridRAGQuery architecture represents a sophisticated synthesis of multiple AI technologies:\n",
        "\n",
        "1. **Vector Similarity Search**: Provides semantic understanding and initial document retrieval\n",
        "2. **Graph Traversal Algorithms**: Discover additional contextual information through relationship exploration\n",
        "3. **Large Language Models**: Enable intelligent filtering, custom query generation, and final answer synthesis\n",
        "4. **Neo4j Graph Database**: Provides high-performance storage and querying for both vector and graph data\n",
        "\n",
        "The system's modular design enables easy extension and customization while maintaining robust performance and comprehensive error handling. The singleton pattern ensures efficient resource utilization, while the multi-vector search approach maximizes retrieval recall across different semantic dimensions.\n",
        "\n",
        "This architecture demonstrates how traditional RAG systems can be enhanced through the integration of graph-based reasoning, creating a more comprehensive and intelligent information retrieval system that can handle complex queries requiring both semantic understanding and structural reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2e1a3c",
      "metadata": {},
      "source": [
        "### Architecture Overview: Hybrid RAG Graph Traversal System\n",
        "\n",
        "![Hybrid RAG Graph Traversal System](graph_traversals.png)\n",
        "\n",
        "This comprehensive diagram illustrates the conceptual foundations and technical mechanics of each graph traversal algorithm employed in our hybrid RAG system. The visualization demonstrates how different traversal techniques navigate the knowledge graph structure, from the initial vector similarity search through various exploration strategies including BFS expansion, DFS depth exploration, A* heuristic guidance, and LLM-driven query generation. Each algorithm's unique approach to discovering and connecting relevant information is clearly depicted, showing how they complement each other to provide comprehensive context retrieval for complex question-answering scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical Foundations: Advanced Graph Traversal Algorithms\n",
        "\n",
        "### Introduction to Graph Traversal in Knowledge Graphs\n",
        "\n",
        "Graph traversal represents one of the most fundamental and powerful techniques in knowledge graph exploration, serving as the bridge between initial vector similarity search and comprehensive context discovery. Unlike traditional database queries that rely on exact matches or simple joins, graph traversal algorithms intelligently navigate the complex web of relationships between entities, concepts, and documents to uncover hidden connections and provide comprehensive context for question answering. \n",
        "\n",
        "In the context of hybrid RAG systems, graph traversal serves multiple critical functions. First, it expands the initial search results by following relationships to discover additional relevant information that may not have been captured by vector similarity alone. Second, it provides structural context by revealing how different pieces of information relate to each other, enabling more nuanced and comprehensive answers. Third, it enables the discovery of indirect relationships and multi-hop connections that are essential for complex reasoning tasks. \n",
        "\n",
        "The power of graph traversal lies in its ability to leverage the inherent structure of knowledge graphs. While vector similarity search excels at finding semantically similar content, it may miss important information that is structurally related but semantically distant. Graph traversal algorithms bridge this gap by following the explicit relationships encoded in the knowledge graph, ensuring that all relevant information is considered regardless of its semantic similarity to the original query. \n",
        "\n",
        "Modern graph traversal algorithms in hybrid RAG systems go beyond simple breadth-first or depth-first search, incorporating sophisticated techniques like cost-based optimization, heuristic guidance, beam search pruning, and LLM-powered predicate filtering. These advanced algorithms can adapt their exploration strategies based on the specific characteristics of the query, the structure of the knowledge graph, and the semantic relevance of discovered information. \n",
        "\n",
        "The algorithms we explore in this system represent a comprehensive toolkit for knowledge graph exploration, each optimized for different types of queries and graph structures. From the revolutionary Context-to-Cypher approach that uses LLMs to generate custom queries, to the systematic exploration of K-hop BFS, to the goal-directed search of A* algorithms, each technique provides unique capabilities for discovering and organizing relevant information. \n",
        "\n",
        "### 1. Context-to-Cypher: Revolutionary LLM-Driven Query Generation\n",
        "\n",
        "The Context-to-Cypher algorithm represents a paradigm shift in graph traversal by leveraging large language models to dynamically generate custom Cypher queries based on query context and initial results. This approach transcends the limitations of fixed algorithmic patterns by employing meta-reasoning to analyze information gaps and construct sophisticated database queries that precisely target missing information. \n",
        "\n",
        "The revolutionary aspect of Context-to-Cypher lies in its adaptive intelligence. Traditional graph traversal algorithms follow predetermined patterns regardless of the specific information needs of a query. In contrast, Context-to-Cypher acts as an intelligent database expert that examines the initial search results, identifies what information is missing or incomplete, and then crafts custom queries designed to fill those specific gaps. This approach enables the system to handle complex, multi-faceted queries that would be difficult or impossible to address with fixed algorithmic approaches. \n",
        "\n",
        "The system incorporates a comprehensive pattern library containing over 18 advanced search techniques, from basic vector similarity and multi-index searches to sophisticated approaches like community detection, temporal relationship analysis, and cross-domain bridging. The LLM selects and combines these patterns based on the specific context of each query, creating hybrid search strategies that leverage the most appropriate techniques for each situation. \n",
        "\n",
        "One of the key innovations is the system's ability to perform semantic gap analysis. The LLM examines the initial results and the user's query to identify what types of information are missing, what relationships have not been explored, and what alternative search strategies might reveal additional relevant content. This analysis then informs the generation of custom Cypher queries that specifically target these identified gaps. \n",
        "\n",
        "The Context-to-Cypher approach also demonstrates remarkable flexibility in handling different types of queries. For factual questions, it might generate queries focused on direct relationships and properties. For analytical questions, it might combine multiple search patterns to gather comprehensive context from different perspectives. For exploratory questions, it might employ broad discovery patterns that reveal unexpected connections and relationships. \n",
        "\n",
        "The system's query generation process incorporates sophisticated schema analysis, examining the available node types, relationship types, and property structures to ensure that generated queries are both syntactically correct and semantically meaningful. This deep understanding of the graph structure enables the generation of complex queries that leverage the full capabilities of the knowledge graph. \n",
        "\n",
        "### 2. K-hop Limited BFS: Systematic Breadth-First Exploration\n",
        "\n",
        "K-hop Limited BFS represents a systematic approach to graph exploration that balances comprehensive discovery with computational efficiency. This algorithm performs controlled breadth-first traversal, expanding the search frontier level by level while implementing intelligent limiting mechanisms to prevent exponential growth and maintain manageable result sets. \n",
        "\n",
        "The algorithm begins with the initial set of nodes discovered through vector similarity search and systematically explores their immediate neighbors (1-hop), then the neighbors of those neighbors (2-hop), and so on, up to a configurable maximum depth K. This level-by-level expansion ensures that all nearby relationships are discovered before moving to more distant connections, providing comprehensive coverage of the local graph neighborhood. \n",
        "\n",
        "The \"limited\" aspect of this algorithm is crucial for practical applications. Without limits, breadth-first search in highly connected graphs can quickly explode to millions of nodes, making the results unwieldy and the computation expensive. The K-hop limitation provides a natural stopping point, typically set to 2-3 hops, which captures the most relevant local context while maintaining computational feasibility. \n",
        "\n",
        "Additional limiting mechanisms include per-node neighbor limits, which cap the number of neighbors explored from each node to prevent highly connected nodes from dominating the search, and total result limits, which ensure that the algorithm returns a manageable number of nodes regardless of graph structure. These limits are carefully tuned to balance comprehensiveness with usability. \n",
        "\n",
        "The algorithm is particularly effective for queries that require broad contextual understanding, such as \"What do we know about artificial intelligence?\" or \"Tell me about Einstein's contributions to physics.\" In these cases, the systematic exploration of nearby concepts, related people, associated institutions, and connected ideas provides comprehensive background information that enhances the quality of generated answers. \n",
        "\n",
        "The BFS approach also ensures that the most directly relevant information is prioritized in the results. Since nodes are discovered in order of their distance from the initial seeds, the algorithm naturally ranks information by structural relevance, with closer connections being considered more important than distant ones. \n",
        "\n",
        "### 3. K-hop Limited BFS with LLM Predicate Filtering\n",
        "\n",
        "The K-hop Limited BFS with LLM Predicate Filtering algorithm enhances the systematic exploration of standard BFS by incorporating intelligent relationship filtering based on semantic relevance. This approach combines the comprehensive coverage of breadth-first search with the contextual understanding of large language models to focus exploration on the most semantically relevant paths. \n",
        "\n",
        "The key innovation lies in the predicate filtering mechanism. At each hop level, before expanding to the next set of neighbors, the algorithm uses an LLM to analyze the available relationship types and select only those that are most relevant to the current query context. This filtering process considers both the semantic meaning of the relationships and their potential contribution to answering the user's question. \n",
        "\n",
        "The LLM predicate filtering operates through a sophisticated prompt-based system that provides the language model with context about the current query, the types of relationships available for exploration, and the goal of the search. The LLM then makes intelligent decisions about which relationship types to follow, effectively pruning irrelevant paths before they can consume computational resources. \n",
        "\n",
        "This approach addresses one of the key limitations of standard BFS: the tendency to explore all relationships equally, regardless of their relevance to the query. By incorporating semantic filtering, the algorithm can focus its exploration on the most promising directions while still maintaining the systematic, level-by-level approach that ensures comprehensive coverage. \n",
        "\n",
        "The filtering mechanism is particularly valuable in knowledge graphs with diverse relationship types. For example, when exploring information about a scientific concept, the algorithm might choose to follow relationships like \"RELATED_TO,\" \"USED_IN,\" and \"DISCOVERED_BY\" while filtering out less relevant relationships like \"MENTIONED_IN_DOCUMENT\" or \"SIMILAR_SPELLING_TO.\" \n",
        "\n",
        "The algorithm also demonstrates adaptive behavior, where the filtering criteria can change based on the specific characteristics of the query. For biographical queries, it might prioritize relationships related to life events, achievements, and associations. For technical queries, it might focus on personal relationships and career development paths. For technical queries, it might focus on conceptual relationships and methodological connections. \n",
        "\n",
        "### 4. Depth-Limited DFS: Deep Relationship Chain Exploration\n",
        "\n",
        "Depth-Limited DFS represents a focused approach to graph traversal that excels at discovering deep, multi-step relationships and causal chains within knowledge graphs. Unlike breadth-first approaches that explore all nearby connections, DFS follows individual relationship paths to their natural conclusion, uncovering complex narratives and sequential dependencies that might be missed by broader search strategies. \n",
        "\n",
        "The algorithm begins with initial nodes and selects specific relationship paths to follow in depth, continuing along each path until reaching either a natural termination point or the configured depth limit. This approach is particularly valuable for queries that require understanding of causal relationships, temporal sequences, or hierarchical structures within the knowledge graph. \n",
        "\n",
        "The depth-limited aspect is crucial for preventing infinite traversal in cyclic graphs and ensuring that the algorithm terminates within reasonable bounds. The depth limit is typically set based on the complexity of relationships expected in the domain, with values ranging from 3-5 hops for most applications. This limitation ensures that the algorithm can discover meaningful chains of relationships without getting lost in overly complex or tangential paths. \n",
        "\n",
        "DFS excels at uncovering stories and narratives within the knowledge graph. For example, when exploring a query about the development of a scientific theory, DFS might follow a path from the theory to its discoverer, then to the discoverer's educational background, then to their influential mentors, revealing the intellectual lineage that contributed to the theory's development. \n",
        "\n",
        "The algorithm's path-following behavior makes it particularly effective for queries that involve understanding influence, causation, or evolution over time. It can trace the development of ideas through chains of researchers, the evolution of technologies through sequences of innovations, or the flow of influence through networks of related concepts. \n",
        "\n",
        "The memory efficiency of DFS is another significant advantage, as it only needs to maintain information about the current path being explored rather than all nodes at the current level. This makes it suitable for exploring large graphs where breadth-first approaches might consume excessive memory. \n",
        "\n",
        "### 5. Depth-Limited DFS with LLM Predicate Filtering\n",
        "\n",
        "The Depth-Limited DFS with LLM Predicate Filtering algorithm combines the deep exploration capabilities of depth-first search with intelligent path selection based on semantic relevance. This approach uses large language models to guide the traversal process, ensuring that the algorithm follows the most meaningful and relevant paths while maintaining the depth-focused exploration strategy. \n",
        "\n",
        "The predicate filtering mechanism operates at each node along the traversal path, analyzing the available outgoing relationships and selecting only those that are most likely to contribute to answering the user's query. This intelligent filtering prevents the algorithm from following irrelevant paths while ensuring that it can still discover deep, multi-step relationships when they are semantically meaningful. \n",
        "\n",
        "The LLM filtering process considers multiple factors when making path selection decisions. It evaluates the semantic relationship between the current path and the user's query, assesses the potential value of different relationship types for the specific query context, and considers the coherence of the emerging narrative or causal chain. This multi-factor analysis ensures that the selected paths are both relevant and meaningful. \n",
        "\n",
        "The algorithm demonstrates particular strength in handling complex analytical queries that require understanding of multi-step reasoning or causal relationships. For example, when exploring a query about the impact of early cybernetics research on modern machine learning, the algorithm might follow a path from cybernetics concepts to early computational models, then to neural network research, then to contemporary deep learning approaches, revealing the intellectual connections across decades of research. \n",
        "\n",
        "The filtering mechanism also enables the algorithm to adapt its exploration strategy based on the type of query being processed. For historical queries, it might prioritize temporal relationships and causal connections. For biographical queries, it might focus on personal relationships and career development paths. For technical queries, it might emphasize conceptual relationships and methodological connections. \n",
        "\n",
        "The combination of depth-first exploration with intelligent filtering creates a powerful approach for discovering meaningful narratives and causal chains within knowledge graphs. The algorithm can follow complex paths while ensuring that each step along the path contributes to the overall understanding of the query topic. \n",
        "\n",
        "### 6. Uniform Cost Search (UCS): Optimal Semantic Exploration\n",
        "\n",
        "Uniform Cost Search represents a sophisticated approach to graph traversal that applies optimal search principles to semantic exploration within knowledge graphs. This algorithm adapts Dijkstra's shortest path algorithm to operate over semantic embeddings, treating the cosine distance between vector embeddings as the cost metric for traversal decisions. \n",
        "\n",
        "The fundamental innovation of UCS in the knowledge graph context is its use of semantic similarity as the optimization criterion. Rather than optimizing for shortest path length or minimum hop count, UCS optimizes for semantic relevance, ensuring that the algorithm always explores the most semantically similar nodes before moving to less similar ones. This approach guarantees that nodes are discovered in order of increasing semantic distance from the initial query context. \n",
        "\n",
        "The algorithm maintains a priority queue of nodes to explore, with priorities based on the cumulative semantic cost from the initial seed nodes. At each step, the algorithm selects the node with the lowest cumulative cost, ensuring that exploration always proceeds along the most semantically promising paths. This systematic approach to semantic exploration provides optimal guarantees about the relevance ordering of discovered nodes. \n",
        "\n",
        "The cost calculation process is central to the algorithm's effectiveness. The semantic cost between two nodes is typically calculated as the cosine distance between their vector embeddings, providing a quantitative measure of semantic dissimilarity. The cumulative cost represents the total semantic distance traveled from the initial seed nodes, enabling the algorithm to find the most semantically direct paths to relevant information. \n",
        "\n",
        "UCS is particularly effective for queries that require finding information that is semantically related but may not be structurally close in the graph. Traditional graph traversal algorithms might miss relevant information if it is separated by several hops of less relevant relationships. UCS, by contrast, can discover semantically relevant information regardless of its structural distance, as long as the semantic path is optimal. \n",
        "\n",
        "The algorithm also demonstrates robust behavior in handling diverse graph structures. In densely connected graphs, UCS prevents the exploration from being overwhelmed by structural connectivity, instead focusing on semantic relevance. In sparsely connected graphs, it ensures that the most relevant available information is discovered first. \n",
        "\n",
        "### 7. Uniform Cost Search with LLM Predicate Filtering\n",
        "\n",
        "The Uniform Cost Search with LLM Predicate Filtering algorithm combines the optimal semantic exploration capabilities of UCS with intelligent relationship filtering based on contextual relevance. This approach uses large language models to select the most appropriate relationship types for exploration while maintaining the optimal cost-based ordering that ensures semantically relevant nodes are discovered first. \n",
        "\n",
        "The predicate filtering mechanism operates before the cost calculation phase, analyzing the available relationships from each node and selecting only those that are contextually appropriate for the current query. This filtering process considers not only the semantic relevance of the relationships but also their potential contribution to the overall search objectives, ensuring that the algorithm explores only the most promising paths. \n",
        "\n",
        "The integration of LLM filtering with UCS creates a powerful synergy where the semantic optimization of UCS is enhanced by the contextual understanding of the language model. The LLM can identify relationships that, while semantically distant, might be crucial for understanding the query context, while UCS ensures that the exploration of these relationships proceeds in optimal order. \n",
        "\n",
        "The algorithm demonstrates particular strength in handling complex queries that require both semantic understanding and contextual reasoning. For example, when exploring a query about the applications of quantum mechanics in modern technology, the algorithm might use LLM filtering to identify relevant relationship types like \"APPLIED_IN,\" \"ENABLES,\" and \"BASED_ON,\" while using UCS to ensure that the most semantically relevant applications are discovered first. \n",
        "\n",
        "The filtering mechanism also enables the algorithm to adapt its exploration strategy based on the evolving context of the search. As new information is discovered, the LLM can adjust its filtering criteria to focus on the most promising directions, while UCS continues to ensure optimal exploration within the selected relationship types. \n",
        "\n",
        "This combination creates an algorithm that is both semantically optimal and contextually intelligent, capable of discovering the most relevant information while following the most appropriate paths through the knowledge graph. The result is a search process that is both efficient and comprehensive, providing high-quality results for complex queries. \n",
        "\n",
        "### 8. A* Search Heuristic: Goal-Directed Semantic Exploration\n",
        "\n",
        "The A* Search Heuristic algorithm represents the pinnacle of goal-directed graph traversal, combining the optimal exploration guarantees of Uniform Cost Search with intelligent heuristic guidance toward the query objective. This approach uses the user's query embedding as a goal state, enabling the algorithm to balance the cumulative cost of exploration with the estimated remaining cost to reach semantically relevant information. \n",
        "\n",
        "The fundamental innovation of A* in the knowledge graph context is its use of heuristic functions to guide exploration toward semantically relevant information. The algorithm maintains two cost components for each node: the g-score, representing the cumulative semantic cost from the initial seed nodes, and the h-score, representing the estimated semantic cost to reach the query objective. The f-score, calculated as g + h, provides the overall priority for exploration. \n",
        "\n",
        "The heuristic function typically uses the cosine distance between a node's embedding and the query embedding as the h-score, providing an estimate of how much additional semantic distance must be traversed to reach information that directly addresses the query. This heuristic enables the algorithm to prioritize nodes that are both semantically reachable from the initial context and semantically relevant to the query objective. \n",
        "\n",
        "The A* algorithm demonstrates remarkable efficiency in large knowledge graphs by focusing exploration on the most promising regions of the graph space. Rather than exhaustively exploring all semantically similar nodes, A* uses its heuristic to identify nodes that are likely to be both reachable and relevant, significantly reducing the search space while maintaining optimality guarantees. \n",
        "\n",
        "The algorithm is particularly effective for queries that have a clear semantic objective, such as \"What are the health effects of air pollution?\" or \"How does climate change affect agriculture?\" In these cases, the query embedding provides a clear goal state, and the heuristic can effectively guide exploration toward information that directly addresses the question. \n",
        "\n",
        "The balance between g-score and h-score creates sophisticated exploration behavior. Early in the search, when the cumulative cost is low, the heuristic dominates and the algorithm focuses on finding semantically relevant information. As the search progresses and cumulative costs increase, the algorithm balances between continuing along established paths and exploring new, potentially more relevant directions. \n",
        "\n",
        "### 9. A* Search Heuristic with LLM Predicate Filtering\n",
        "\n",
        "The A* Search Heuristic with LLM Predicate Filtering algorithm combines the goal-directed efficiency of A* search with intelligent relationship filtering based on contextual understanding. This approach uses large language models to analyze and select the most relevant relationship types for exploration while maintaining the optimal cost-plus-heuristic guidance that makes A* search so effective. \n",
        "\n",
        "The integration of LLM filtering with A* search creates a sophisticated exploration strategy that considers multiple factors in path selection. The LLM filter analyzes the available relationships from each node, considering their semantic relevance to the query, their potential contribution to the search objectives, and their coherence with the emerging path narrative. Meanwhile, the A* algorithm ensures that exploration proceeds optimally toward the query objective. \n",
        "\n",
        "The predicate filtering mechanism operates before the cost and heuristic calculations, analyzing the relationship types available from each node and selecting only those that are contextually appropriate. This filtering process considers not only the direct semantic relevance of the relationships but also their potential for leading to information that addresses the query objective. \n",
        "\n",
        "The algorithm demonstrates exceptional performance on complex queries that require both semantic understanding and strategic path selection. For example, when exploring a query about the economic impacts of renewable energy adoption, the algorithm might use LLM filtering to focus on relationships like \"ECONOMIC_EFFECT,\" \"CAUSED_BY,\" and \"INFLUENCES,\" while using A* guidance to ensure that the most relevant economic information is discovered first. \n",
        "\n",
        "The heuristic guidance remains effective even with LLM filtering because the filtering process eliminates irrelevant paths rather than changing the fundamental cost structure. This means that the A* algorithm can still provide optimal exploration within the set of contextually relevant relationships, ensuring that the most promising paths are explored first. \n",
        "\n",
        "The combination creates an algorithm that is both strategically intelligent and optimally efficient, capable of navigating complex knowledge graphs while maintaining focus on the query objective. The result is a search process that discovers high-quality, relevant information while avoiding the exploration of irrelevant or tangential paths. \n",
        "\n",
        "### 10. Beam Search Over the Graph: Controlled Breadth Exploration\n",
        "\n",
        "Beam Search Over the Graph represents a sophisticated approach to graph traversal that combines the systematic exploration of breadth-first search with intelligent pruning to control memory usage and computational complexity. This algorithm performs level-by-level exploration like BFS but retains only the top W scoring nodes at each depth, where W is the beam width, effectively controlling the search space while maintaining quality results. \n",
        "\n",
        "The fundamental innovation of beam search in the graph context is its ability to balance exploration breadth with computational efficiency. Traditional BFS can suffer from exponential growth in the number of nodes at each level, quickly becoming computationally intractable in highly connected graphs. Beam search addresses this by maintaining a constant beam width, ensuring that computational and memory requirements remain manageable regardless of graph structure. \n",
        "\n",
        "The scoring mechanism is central to beam search effectiveness. Nodes are typically scored based on their semantic similarity to the query, their relevance to the current search context, or a combination of factors including structural importance and semantic relevance. This scoring enables the algorithm to retain the most promising nodes at each level while pruning less relevant alternatives. \n",
        "\n",
        "The algorithm demonstrates particular strength in handling queries that require broad contextual understanding but with controlled scope. For example, when exploring a query about \"machine learning applications in healthcare,\" beam search can systematically explore different application areas, specific techniques, and implementation examples while maintaining a manageable result set through intelligent pruning. \n",
        "\n",
        "The beam width parameter provides fine-grained control over the exploration-efficiency trade-off. Smaller beam widths result in more focused exploration with lower computational costs, while larger beam widths enable more comprehensive exploration at the cost of increased resource requirements. This flexibility allows the algorithm to be tuned for different query types and computational constraints. \n",
        "\n",
        "Beam search also exhibits interesting emergent behaviors in knowledge graphs. The pruning mechanism tends to favor nodes that are both semantically relevant and structurally important, leading to natural clustering of results around key concepts and entities. This clustering can reveal important themes and relationships that might not be apparent in the individual search results. \n",
        "\n",
        "### 11. Beam Search Over the Graph with LLM Predicate Filtering\n",
        "\n",
        "The Beam Search Over the Graph with LLM Predicate Filtering algorithm combines the controlled exploration capabilities of beam search with intelligent relationship filtering based on contextual understanding. This approach uses large language models to analyze and select the most relevant relationship types for exploration while maintaining the systematic pruning that makes beam search computationally efficient. \n",
        "\n",
        "The integration of LLM filtering with beam search creates a powerful exploration strategy that operates at two levels of intelligence. The LLM filter provides high-level contextual understanding, analyzing the semantic relevance of different relationship types and their potential contribution to the search objectives. The beam search mechanism provides systematic exploration with intelligent pruning, ensuring that the most promising paths are explored while maintaining computational efficiency. \n",
        "\n",
        "The predicate filtering operates before the scoring and pruning phases, analyzing the relationships available from each node and selecting only those that are contextually appropriate for the current query. This filtering process considers the semantic meaning of the relationships, their relevance to the query context, and their potential for leading to valuable information. \n",
        "\n",
        "The algorithm demonstrates exceptional performance on complex queries that require both broad exploration and contextual understanding. For example, when exploring a query about the societal impacts of artificial intelligence, the algorithm might use LLM filtering to focus on relationships like \"IMPACTS,\" \"AFFECTS,\" \"CHANGES,\" and \"INFLUENCES,\" while using beam search to systematically explore different societal domains while maintaining a manageable result set. \n",
        "\n",
        "The filtering mechanism also enables adaptive behavior where the filtering criteria can evolve based on the information discovered during exploration. As new context is uncovered, the LLM can adjust its filtering strategy to focus on the most promising directions, while beam search continues to provide systematic exploration with intelligent pruning. \n",
        "\n",
        "The combination of LLM filtering and beam search creates an algorithm that is both contextually intelligent and computationally efficient. The LLM filtering ensures that exploration focuses on the most relevant relationships, while beam search ensures that the exploration remains systematic and manageable. The result is a search process that can handle complex queries while maintaining high-quality results and reasonable computational requirements. \n",
        "\n",
        "This sophisticated combination of techniques represents the cutting edge of graph traversal algorithms, providing a framework that can adapt to different query types, graph structures, and computational constraints while maintaining both semantic relevance and exploration efficiency. The algorithm demonstrates how advanced AI techniques can be integrated with classical graph algorithms to create powerful tools for knowledge discovery and question answering. \n",
        "\n"
      ]
    }
    
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
