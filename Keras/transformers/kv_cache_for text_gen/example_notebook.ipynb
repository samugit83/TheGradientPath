{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6e23a1",
   "metadata": {},
   "source": [
    "# Comprehensive Transformer Implementation for Text Generation with KV Cache Optimization\n",
    "\n",
    "This notebook implements a complete Transformer-based language model from scratch using TensorFlow/Keras.\n",
    "The implementation focuses on text generation with an advanced Key-Value (KV) Cache optimization system\n",
    "that significantly speeds up autoregressive text generation.\n",
    "\n",
    "## What This Implementation Does:\n",
    "1. Implements a full Transformer architecture with multi-head self-attention\n",
    "2. Provides KV Cache optimization for faster text generation\n",
    "3. Trains the model on text data (Shakespeare or custom corpus)\n",
    "4. Generates text using the trained model with performance comparisons\n",
    "5. Includes comprehensive logging and visualization tools\n",
    "\n",
    "## Key Concepts Explained:\n",
    "- **Transformers**: Neural network architecture that uses attention mechanisms\n",
    "- **Self-Attention**: Allows the model to focus on different parts of the input sequence\n",
    "- **KV Cache**: Optimization technique that stores computed attention keys/values to avoid redundant calculations\n",
    "- **Autoregressive Generation**: Generating text one token at a time, using previous tokens as context\n",
    "- **Causal Masking**: Prevents the model from looking at future tokens during training and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e6fb2",
   "metadata": {},
   "source": [
    "## üì∫ Watch the Tutorial\n",
    "\n",
    "Prefer a video walkthrough? Check out the accompanying tutorial on YouTube:\n",
    "\n",
    "[Video Tutorial](https://youtu.be/N44mgyzxxKU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4c2a1",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Guide\n",
    "\n",
    "Ready to run the code? Follow these simple steps to set up your environment and execute the Transformer text generation model:\n",
    "\n",
    "### Step 1: Create a Python Virtual Environment\n",
    "```bash\n",
    "# Create a new virtual environment\n",
    "python -m venv /path/to/your/project/\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On macOS/Linux:\n",
    "source /path/to/your/project/bin/activate\n",
    "```\n",
    "\n",
    "### Step 2: Install Required Dependencies\n",
    "```bash\n",
    "# Install all required packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "The `requirements.txt` file includes:\n",
    "- `tensorflow>=2.0` - Deep learning framework\n",
    "- `numpy` - Numerical computing\n",
    "- `matplotlib` - Plotting and visualization\n",
    "- `visualkeras` - Model architecture visualization\n",
    "\n",
    "### Step 3: Prepare Your Text Corpus (Optional)\n",
    "The script can work with two data sources:\n",
    "- **Web dataset:** Automatically downloads Shakespeare's complete works\n",
    "- **Local corpus:** Place your own text file as `corpus.txt` in the project directory\n",
    "\n",
    "To use your own text data:\n",
    "```bash\n",
    "# Place your text file in the project directory\n",
    "cp /path/to/your/text/data.txt corpus.txt\n",
    "```\n",
    "\n",
    "### Step 4: Run the Main Script\n",
    "```bash\n",
    "# Execute the Transformer text generation model\n",
    "python main.py\n",
    "```\n",
    "\n",
    "### What Happens When You Run It?\n",
    "1. **Data Loading:** Downloads Shakespeare dataset or loads local corpus.txt\n",
    "2. **Text Preprocessing:** Tokenizes and vectorizes text using TextVectorization\n",
    "3. **Sequence Generation:** Creates input-target pairs for next-token prediction\n",
    "4. **Model Building:** Constructs the Transformer architecture with positional encoding\n",
    "5. **Training:** Trains the model with early stopping and TensorBoard logging\n",
    "6. **Visualization:** Generates model architecture diagrams and training plots\n",
    "7. **Text Generation:** Creates novel text sequences using the trained model\n",
    "\n",
    "### Monitoring Training Progress\n",
    "The script automatically sets up TensorBoard logging. After running, you can monitor training in real-time:\n",
    "```bash\n",
    "# Launch TensorBoard (the script will show you the exact command)\n",
    "tensorboard --logdir logs/[timestamp]\n",
    "```\n",
    "\n",
    "Then open your browser to `http://localhost:6006` to view:\n",
    "- Training loss curves\n",
    "- Model architecture visualization\n",
    "- Text generation samples\n",
    "- Model weights and gradients\n",
    "\n",
    "**Expected Runtime:** Approximately 10-20 minutes on a modern CPU, faster with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92d7e0", 
   "metadata": {},
   "source": [
    "## Import Libraries and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os  # Operating system interface for file/directory operations\n",
    "from datetime import datetime  # For timestamping logs and outputs\n",
    "import numpy as np  # Numerical computing library for array operations\n",
    "import tensorflow as tf  # Deep learning framework - our main ML library\n",
    "\n",
    "# TensorFlow/Keras specific imports for building neural network components\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer,              # Base class for all neural network layers\n",
    "    Dense,              # Fully connected (linear) layer - core building block\n",
    "    LayerNormalization, # Normalization technique for stable training\n",
    "    Dropout,            # Regularization technique to prevent overfitting\n",
    "    Embedding,          # Converts token IDs to dense vector representations\n",
    "    TextVectorization   # Preprocesses text data into numerical format\n",
    ")\n",
    "from tensorflow.keras.models import Model  # Base class for complex models\n",
    "from tensorflow.keras.utils import get_file  # Utility for downloading datasets\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Prevents overfitting during training\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualizations\n",
    "import visualkeras  # Library for visualizing neural network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_detailed_explanation",
   "metadata": {},
   "source": [
    "### üîç **Detailed Analysis: Import Libraries and Their Critical Roles**\n",
    "\n",
    "Each import in this transformer implementation serves a specific, crucial purpose in building a state-of-the-art language model. Let's dive deep into understanding why each library is essential:\n",
    "\n",
    "#### **Core System Libraries**\n",
    "\n",
    "**`import os`** - Operating System Interface\n",
    "- **Purpose**: Provides cross-platform file system operations\n",
    "- **Usage in our implementation**: \n",
    "  - Creating directory structures for model checkpoints and logs\n",
    "  - Managing file paths for saving/loading trained models\n",
    "  - Handling corpus file detection and validation\n",
    "- **Why it's critical**: Deep learning projects require extensive file management for datasets, model weights, logs, and outputs\n",
    "\n",
    "**`from datetime import datetime`** - Timestamp Management\n",
    "- **Purpose**: Provides date and time functionality for logging and organization\n",
    "- **Usage in our implementation**:\n",
    "  - Creating unique timestamps for TensorBoard log directories\n",
    "  - Organizing training runs by date/time for experiment tracking\n",
    "  - Generating unique filenames to prevent overwrites\n",
    "- **Why it's critical**: Proper experiment tracking is essential for reproducible research and model development\n",
    "\n",
    "#### **Numerical Computing Foundation**\n",
    "\n",
    "**`import numpy as np`** - Numerical Python\n",
    "- **Purpose**: Fundamental package for scientific computing with Python\n",
    "- **Usage in our implementation**:\n",
    "  - **Positional Encoding**: Creating sinusoidal position embeddings using trigonometric functions\n",
    "  - **Data Preprocessing**: Converting text sequences to numerical arrays\n",
    "  - **Mathematical Operations**: Angle calculations, array manipulations, and tensor operations\n",
    "  - **Initialization**: Setting up mathematical constants and arrays\n",
    "- **Why it's critical**: NumPy provides the mathematical foundation that TensorFlow builds upon, especially for custom mathematical operations\n",
    "\n",
    "#### **Deep Learning Framework**\n",
    "\n",
    "**`import tensorflow as tf`** - TensorFlow Core\n",
    "- **Purpose**: Google's open-source machine learning framework\n",
    "- **Usage in our implementation**:\n",
    "  - **Tensor Operations**: All mathematical computations (matrix multiplication, activation functions)\n",
    "  - **Automatic Differentiation**: Gradient computation for backpropagation\n",
    "  - **GPU Acceleration**: Utilizing CUDA cores for parallel processing\n",
    "  - **Graph Execution**: Optimized computation graphs for efficient training\n",
    "- **Why it's critical**: TensorFlow provides the computational engine that makes training large transformer models feasible\n",
    "\n",
    "#### **Neural Network Building Blocks**\n",
    "\n",
    "**`Layer`** - Base Layer Class\n",
    "- **Purpose**: Abstract base class for all neural network layers\n",
    "- **Usage in our implementation**:\n",
    "  - **Custom Layers**: MultiHeadSelfAttention and TransformerBlock inherit from this\n",
    "  - **State Management**: Handles layer weights, training states, and forward/backward passes\n",
    "  - **Composability**: Enables stacking layers to build complex architectures\n",
    "- **Deep Principle**: Object-oriented design allows modular, reusable components that can be combined to create sophisticated models\n",
    "\n",
    "**`Dense`** - Fully Connected Layer\n",
    "- **Purpose**: Implements linear transformation: output = input √ó weight + bias\n",
    "- **Usage in our implementation**:\n",
    "  - **Attention Projections**: Query, Key, Value transformations (Q = XW_q, K = XW_k, V = XW_v)\n",
    "  - **Feed-Forward Networks**: Two-layer MLPs within transformer blocks\n",
    "  - **Output Projection**: Final layer mapping hidden states to vocabulary probabilities\n",
    "- **Mathematical Foundation**: Dense layers perform affine transformations that are fundamental to neural network expressiveness\n",
    "\n",
    "**`LayerNormalization`** - Normalization Technique\n",
    "- **Purpose**: Normalizes inputs across features to stabilize training\n",
    "- **Mathematical Formula**: LN(x) = Œ≥ √ó (x - Œº) / œÉ + Œ≤\n",
    "- **Usage in our implementation**:\n",
    "  - **Pre-Norm Architecture**: Applied before attention and feed-forward operations\n",
    "  - **Gradient Stabilization**: Prevents vanishing/exploding gradients in deep networks\n",
    "  - **Training Acceleration**: Enables higher learning rates and faster convergence\n",
    "- **Why it's superior to BatchNorm**: Works better with variable-length sequences and doesn't depend on batch statistics\n",
    "\n",
    "**`Dropout`** - Regularization Technique\n",
    "- **Purpose**: Randomly sets input units to 0 during training to prevent overfitting\n",
    "- **Mathematical Principle**: During training, each neuron has probability p of being \"dropped out\"\n",
    "- **Usage in our implementation**:\n",
    "  - **Attention Dropout**: Applied after attention weights computation\n",
    "  - **Feed-Forward Dropout**: Applied in the MLP layers\n",
    "  - **Generalization**: Forces the model to not rely on specific neurons\n",
    "- **Critical Insight**: Dropout acts as an ensemble method, training multiple sub-networks simultaneously\n",
    "\n",
    "**`Embedding`** - Token-to-Vector Conversion\n",
    "- **Purpose**: Converts discrete token IDs to dense vector representations\n",
    "- **Mathematical Operation**: lookup_table[token_id] ‚Üí dense_vector\n",
    "- **Usage in our implementation**:\n",
    "  - **Token Embeddings**: Converting word/subword tokens to learnable vectors\n",
    "  - **Semantic Representation**: Capturing semantic relationships between tokens\n",
    "  - **Dimensionality**: Maps vocabulary_size ‚Üí embedding_dimension\n",
    "- **Deep Learning Principle**: Embeddings learn distributed representations where similar tokens have similar vectors\n",
    "\n",
    "**`TextVectorization`** - Text Preprocessing Pipeline\n",
    "- **Purpose**: Converts raw text to numerical sequences suitable for neural networks\n",
    "- **Operations Performed**:\n",
    "  - **Tokenization**: Splitting text into individual tokens\n",
    "  - **Vocabulary Building**: Creating token-to-ID mappings\n",
    "  - **Sequence Conversion**: Converting text to integer sequences\n",
    "  - **Padding/Truncation**: Ensuring uniform sequence lengths\n",
    "- **Usage in our implementation**: Preprocessing training data and handling text generation inputs\n",
    "\n",
    "#### **Model Architecture and Training**\n",
    "\n",
    "**`Model`** - Keras Model Base Class\n",
    "- **Purpose**: High-level API for building and training complex neural networks\n",
    "- **Features Provided**:\n",
    "  - **Training Loop**: Built-in fit() method with optimization\n",
    "  - **Serialization**: Save/load model weights and architecture\n",
    "  - **Metrics**: Built-in loss computation and evaluation\n",
    "  - **Callbacks**: Extensible training process with custom behaviors\n",
    "- **Usage**: Our TransformerModel inherits from Model to get all these capabilities\n",
    "\n",
    "**`get_file`** - Dataset Utility\n",
    "- **Purpose**: Downloads and caches datasets from URLs\n",
    "- **Usage in our implementation**: Downloading Shakespeare dataset for training\n",
    "- **Features**: Automatic caching, integrity checking, and progress bars\n",
    "\n",
    "**`EarlyStopping`** - Training Callback\n",
    "- **Purpose**: Prevents overfitting by stopping training when validation metrics stop improving\n",
    "- **Algorithm**: Monitors specified metric and stops training after patience epochs without improvement\n",
    "- **Usage**: Automatically stops training when loss plateaus, saving computational resources\n",
    "\n",
    "#### **Visualization and Analysis**\n",
    "\n",
    "**`matplotlib.pyplot`** - Plotting Library\n",
    "- **Purpose**: Creating publication-quality plots and visualizations\n",
    "- **Usage in our implementation**:\n",
    "  - **Training Curves**: Plotting loss over epochs\n",
    "  - **Performance Analysis**: Visualizing generation speed comparisons\n",
    "  - **Model Insights**: Creating attention heatmaps and analysis plots\n",
    "\n",
    "**`visualkeras`** - Neural Network Visualization\n",
    "- **Purpose**: Creates visual representations of neural network architectures\n",
    "- **Usage**: Generating diagrams of our transformer model structure\n",
    "- **Benefit**: Helps understand model complexity and architecture design\n",
    "\n",
    "### **üéØ Integration and Synergy**\n",
    "\n",
    "These libraries work together to create a complete ecosystem:\n",
    "\n",
    "1. **Data Flow**: `os` and `get_file` handle data acquisition ‚Üí `TextVectorization` preprocesses ‚Üí `numpy` provides mathematical operations\n",
    "2. **Model Building**: `Layer`, `Dense`, `Embedding` create components ‚Üí `Model` orchestrates training\n",
    "3. **Training Process**: `tensorflow` provides the engine ‚Üí `LayerNormalization` and `Dropout` ensure stability ‚Üí `EarlyStopping` prevents overfitting\n",
    "4. **Analysis**: `matplotlib` and `visualkeras` provide insights into model behavior and performance\n",
    "\n",
    "This carefully curated set of imports provides everything needed for a production-ready transformer implementation with proper data handling, model architecture, training procedures, and analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU CONFIGURATION - Optimizing hardware utilization\n",
    "# =============================================================================\n",
    "\n",
    "# Detect all available GPU devices on the system\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:  # If GPUs are detected\n",
    "    try:\n",
    "        # Configure each GPU for memory growth to prevent allocation issues\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Get logical GPU devices\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs configured.\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting up GPU memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu_detailed_explanation",
   "metadata": {},
   "source": [
    "### üöÄ **Deep Dive: GPU Configuration and Memory Management**\n",
    "\n",
    "The GPU configuration section is critical for optimal performance when training and running transformer models. Let's explore the sophisticated memory management strategies employed:\n",
    "\n",
    "#### **üîß GPU Detection and Enumeration**\n",
    "\n",
    "**`tf.config.list_physical_devices('GPU')`**\n",
    "- **Purpose**: Discovers all available GPU hardware on the system\n",
    "- **Technical Details**: \n",
    "  - Queries the CUDA runtime to enumerate physical GPU devices\n",
    "  - Returns a list of PhysicalDevice objects representing each GPU\n",
    "  - Works with NVIDIA GPUs (CUDA) and potentially other accelerators\n",
    "- **Why This Matters**: \n",
    "  - Enables dynamic hardware detection across different systems\n",
    "  - Allows code to adapt to available hardware resources\n",
    "  - Prevents crashes when GPU drivers or hardware aren't available\n",
    "\n",
    "#### **üß† Memory Growth Strategy: The Key to Stable Training**\n",
    "\n",
    "**`tf.config.experimental.set_memory_growth(gpu, True)`**\n",
    "\n",
    "This is one of the most critical optimizations for deep learning workflows. Here's why:\n",
    "\n",
    "**Traditional GPU Memory Allocation (Default Behavior):**\n",
    "- TensorFlow typically allocates ALL available GPU memory at startup\n",
    "- Example: If you have 8GB GPU, TensorFlow claims all 8GB immediately\n",
    "- **Problems with this approach**:\n",
    "  - Prevents running multiple models simultaneously\n",
    "  - Causes out-of-memory errors in multi-process environments\n",
    "  - Wastes memory when model doesn't need full capacity\n",
    "  - Makes development and experimentation difficult\n",
    "\n",
    "**Memory Growth Strategy (Our Implementation):**\n",
    "- **Dynamic Allocation**: Starts with minimal memory usage\n",
    "- **On-Demand Growth**: Allocates more memory only when needed\n",
    "- **Gradual Expansion**: Memory usage grows incrementally as model requires it\n",
    "\n",
    "**Technical Implementation Details:**\n",
    "```python\n",
    "# Without memory growth:\n",
    "# GPU Memory: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 8GB allocated immediately\n",
    "# Available:  [                                ] 0GB for other processes\n",
    "\n",
    "# With memory growth:\n",
    "# Initial:    [‚ñà‚ñà                              ] 512MB allocated\n",
    "# As needed:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        ] 2GB allocated\n",
    "# Maximum:    [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 8GB only if required\n",
    "```\n",
    "\n",
    "**Benefits for Transformer Training:**\n",
    "1. **Multi-Model Development**: Can run multiple experiments simultaneously\n",
    "2. **Resource Sharing**: Other applications can use GPU memory\n",
    "3. **Debugging Friendly**: Easier to debug without memory conflicts\n",
    "4. **Production Deployment**: Better resource utilization in production environments\n",
    "\n",
    "#### **üéØ Logical vs Physical GPU Devices**\n",
    "\n",
    "**Physical GPUs**: The actual hardware devices (e.g., RTX 3080, A100)\n",
    "**Logical GPUs**: Software abstractions that can be created from physical GPUs\n",
    "\n",
    "**Why This Distinction Matters:**\n",
    "- **Memory Partitioning**: One physical GPU can be split into multiple logical GPUs\n",
    "- **Resource Isolation**: Different models can use different logical GPUs\n",
    "- **Multi-Tenancy**: Multiple users can share the same physical hardware\n",
    "\n",
    "**Example Scenarios:**\n",
    "```python\n",
    "# Scenario 1: Single GPU System\n",
    "# Physical GPUs: 1 (RTX 3080 with 10GB)\n",
    "# Logical GPUs: 1 (Full access to 10GB)\n",
    "\n",
    "# Scenario 2: Multi-GPU Workstation\n",
    "# Physical GPUs: 4 (Each with 24GB)\n",
    "# Logical GPUs: 4 (Each model can use different GPU)\n",
    "\n",
    "# Scenario 3: Memory-Limited Partitioning\n",
    "# Physical GPUs: 1 (A100 with 40GB)\n",
    "# Logical GPUs: 4 (Each limited to 10GB for different experiments)\n",
    "```\n",
    "\n",
    "#### **‚ö†Ô∏è Error Handling and Robustness**\n",
    "\n",
    "**`try-except` Block Analysis:**\n",
    "```python\n",
    "try:\n",
    "    # GPU configuration code\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error setting up GPU memory growth: {e}\")\n",
    "```\n",
    "\n",
    "**Common RuntimeError Scenarios:**\n",
    "1. **Already Initialized**: TensorFlow context already created with different settings\n",
    "2. **Driver Issues**: CUDA drivers incompatible or not installed\n",
    "3. **Hardware Problems**: GPU hardware malfunction or overheating\n",
    "4. **Permission Issues**: Insufficient permissions to access GPU resources\n",
    "\n",
    "**Graceful Degradation Strategy:**\n",
    "- **Primary**: Attempt GPU configuration with memory growth\n",
    "- **Fallback**: Continue with default GPU settings if memory growth fails\n",
    "- **Ultimate Fallback**: Use CPU if no GPU available\n",
    "\n",
    "#### **üîç Performance Implications for Transformer Models**\n",
    "\n",
    "**Why GPU Configuration is Critical for Transformers:**\n",
    "\n",
    "1. **Attention Computation Complexity**: O(n¬≤) memory usage for sequence length n\n",
    "2. **Large Parameter Count**: Modern transformers have millions to billions of parameters\n",
    "3. **Batch Processing**: Training requires processing multiple sequences simultaneously\n",
    "4. **KV Cache Storage**: Our implementation stores attention keys/values for fast generation\n",
    "\n",
    "**Memory Usage Patterns:**\n",
    "```python\n",
    "# Training Phase:\n",
    "# - Model weights: ~100MB to 10GB depending on size\n",
    "# - Gradients: Same size as model weights\n",
    "# - Activations: Varies with batch size and sequence length\n",
    "# - Optimizer states: 2-3x model weight size (Adam optimizer)\n",
    "\n",
    "# Generation Phase:\n",
    "# - Model weights: Same as training\n",
    "# - KV Cache: Grows with sequence length\n",
    "# - Attention matrices: O(sequence_length¬≤)\n",
    "```\n",
    "\n",
    "#### **üéõÔ∏è Advanced Configuration Options**\n",
    "\n",
    "**Additional GPU optimizations you could implement:**\n",
    "\n",
    "```python\n",
    "# Memory limit setting (alternative to memory growth)\n",
    "tf.config.experimental.set_memory_limit(gpu, 4096)  # Limit to 4GB\n",
    "\n",
    "# Mixed precision training (faster training, less memory)\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision': True})\n",
    "\n",
    "# XLA compilation (faster execution)\n",
    "tf.config.optimizer.set_jit(True)\n",
    "```\n",
    "\n",
    "#### **üèóÔ∏è Production Considerations**\n",
    "\n",
    "**For Production Deployment:**\n",
    "1. **Resource Monitoring**: Track GPU memory usage and temperature\n",
    "2. **Batch Size Optimization**: Balance throughput vs memory usage\n",
    "3. **Model Sharding**: Split large models across multiple GPUs\n",
    "4. **Inference Optimization**: Use TensorRT or similar optimizations\n",
    "\n",
    "**Container Deployment:**\n",
    "```dockerfile\n",
    "# Docker considerations for GPU access\n",
    "# Requires nvidia-docker runtime\n",
    "# Must expose GPU devices to container\n",
    "# Memory growth becomes even more critical in containerized environments\n",
    "```\n",
    "\n",
    "This GPU configuration section ensures our transformer implementation can:\n",
    "- **Scale efficiently** across different hardware configurations\n",
    "- **Share resources** in multi-user or multi-model environments\n",
    "- **Degrade gracefully** when optimal hardware isn't available\n",
    "- **Optimize memory usage** for both training and inference workloads\n",
    "\n",
    "The memory growth strategy is particularly crucial for transformer models due to their large memory footprint and the variable memory requirements during different phases of training and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention_section",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention with KV Cache\n",
    "\n",
    "The core of the Transformer architecture with advanced KV caching for efficient text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    MULTI-HEAD SELF-ATTENTION MECHANISM - THE HEART OF TRANSFORMERS\n",
    "    \n",
    "    This class implements the core attention mechanism that allows tokens to\n",
    "    communicate with each other. It includes advanced KV cache optimization\n",
    "    for efficient autoregressive text generation.\n",
    "    \n",
    "    Key Features:\n",
    "    - Multiple parallel attention heads for different relationship types\n",
    "    - KV cache support for fast text generation\n",
    "    - Causal masking for autoregressive modeling\n",
    "    - Scaled dot-product attention with proper normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear transformation layers for Q, K, V\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        \n",
    "        # Final layer to combine outputs from all attention heads\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Core attention computation with causal masking support\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        \n",
    "        # Scale by sqrt(d_k) to prevent vanishing gradients\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        \n",
    "        # Apply causal mask if provided\n",
    "        if mask is not None:\n",
    "            scaled_score += (mask * -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = tf.matmul(weights, value)\n",
    "        \n",
    "        return output, weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split embedding dimension into multiple heads for parallel processing\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, kv_cache=None, use_cache=False, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass with KV cache optimization for efficient text generation\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Compute Q, K, V transformations\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # KV Cache optimization\n",
    "        if use_cache and kv_cache is not None:\n",
    "            cached_key = kv_cache.get('key')\n",
    "            cached_value = kv_cache.get('value')\n",
    "            \n",
    "            if cached_key is not None and cached_value is not None:\n",
    "                key = tf.concat([cached_key, key], axis=2)\n",
    "                value = tf.concat([cached_value, value], axis=2)\n",
    "        \n",
    "        # Create causal mask for autoregressive generation\n",
    "        total_seq_len = tf.shape(key)[2]\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, total_seq_len)), -1, 0)\n",
    "        mask = tf.where(mask == 0, 1.0, 0.0)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_output, attention_weights = self.attention(query, key, value, mask)\n",
    "        \n",
    "        # Combine attention heads\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        \n",
    "        # Update cache for next iteration\n",
    "        new_cache = {'key': key, 'value': value} if use_cache else None\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention_detailed_explanation",
   "metadata": {},
   "source": [
    "### üß† **Deep Dive: Multi-Head Self-Attention - The Revolutionary Mechanism**\n",
    "\n",
    "The Multi-Head Self-Attention mechanism is the cornerstone innovation that makes Transformers so powerful. Let's dissect every component and understand the profound mathematical and computational principles at work.\n",
    "\n",
    "#### **üéØ The Attention Revolution: Why It Changed Everything**\n",
    "\n",
    "**Before Attention (RNNs/LSTMs):**\n",
    "- Sequential processing: word‚ÇÅ ‚Üí word‚ÇÇ ‚Üí word‚ÇÉ ‚Üí ... ‚Üí word‚Çô\n",
    "- Information bottleneck: distant words lose context\n",
    "- No parallelization: must process sequentially\n",
    "- Vanishing gradients: long-range dependencies are difficult to learn\n",
    "\n",
    "**With Self-Attention (Transformers):**\n",
    "- Parallel processing: all words attend to all other words simultaneously\n",
    "- Direct connections: word‚ÇÅ can directly influence word‚Çô\n",
    "- Dynamic relationships: attention weights adapt based on context\n",
    "- Scalable: O(n¬≤) complexity but highly parallelizable\n",
    "\n",
    "#### **üî¨ Mathematical Foundation: The Attention Equation**\n",
    "\n",
    "**The Core Attention Formula:**\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V\n",
    "```\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "**Step 1: Query-Key Similarity**\n",
    "```python\n",
    "scores = tf.matmul(query, key, transpose_b=True)  # QK^T\n",
    "# Shape: [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "```\n",
    "- **Purpose**: Compute similarity between each query and every key\n",
    "- **Intuition**: \"How much should each word pay attention to every other word?\"\n",
    "- **Mathematical Meaning**: Dot product measures vector similarity/alignment\n",
    "\n",
    "**Step 2: Scaled Attention (Critical for Stability)**\n",
    "```python\n",
    "scaled_scores = scores / tf.math.sqrt(dim_key)  # QK^T / ‚àöd_k\n",
    "```\n",
    "- **Why Scaling is Essential**: \n",
    "  - Without scaling: dot products grow with dimension size\n",
    "  - Large values ‚Üí softmax saturation ‚Üí vanishing gradients\n",
    "  - ‚àöd_k scaling keeps variance approximately constant\n",
    "- **Mathematical Insight**: If Q and K have unit variance, QK^T has variance d_k\n",
    "\n",
    "**Step 3: Attention Weights via Softmax**\n",
    "```python\n",
    "attention_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "```\n",
    "- **Purpose**: Convert raw scores to probability distribution\n",
    "- **Properties**: \n",
    "  - All weights sum to 1.0 for each query\n",
    "  - Differentiable (enables gradient-based learning)\n",
    "  - Emphasizes highest-scoring keys while maintaining some attention to others\n",
    "\n",
    "**Step 4: Weighted Value Aggregation**\n",
    "```python\n",
    "output = tf.matmul(attention_weights, value)  # softmax(...)V\n",
    "```\n",
    "- **Purpose**: Combine value vectors based on attention weights\n",
    "- **Result**: Each output position is a weighted combination of all input values\n",
    "\n",
    "#### **üé≠ Multi-Head Architecture: Parallel Attention Specialists**\n",
    "\n",
    "**Why Multiple Heads?**\n",
    "Single attention head limitations:\n",
    "- Can only capture one type of relationship at a time\n",
    "- May focus on dominant patterns, missing subtle interactions\n",
    "- Limited representational capacity\n",
    "\n",
    "**Multi-Head Solution:**\n",
    "```python\n",
    "# Instead of one 512-dimensional attention:\n",
    "# Use 8 heads √ó 64 dimensions each = 512 total\n",
    "self.num_heads = 8\n",
    "self.projection_dim = embed_dim // num_heads  # 512 // 8 = 64\n",
    "```\n",
    "\n",
    "**Each Head Specializes in Different Relationships:**\n",
    "- **Head 1**: Syntactic relationships (subject-verb agreement)\n",
    "- **Head 2**: Semantic similarity (synonyms, related concepts)\n",
    "- **Head 3**: Positional relationships (adjacent words)\n",
    "- **Head 4**: Long-range dependencies (pronouns to antecedents)\n",
    "- **Head 5-8**: Other learned patterns specific to the data\n",
    "\n",
    "#### **üîÑ Head Splitting and Recombination Process**\n",
    "\n",
    "**`split_heads()` Function Deep Analysis:**\n",
    "```python\n",
    "def split_heads(self, x, batch_size):\n",
    "    # Input shape: [batch_size, seq_len, embed_dim]\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "    # New shape: [batch_size, seq_len, num_heads, projection_dim]\n",
    "    \n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    # Final shape: [batch_size, num_heads, seq_len, projection_dim]\n",
    "```\n",
    "\n",
    "**Transformation Visualization:**\n",
    "```\n",
    "Original: [Batch, Sequence, 512]\n",
    "\n",
    "Reshape: [Batch, Sequence, 8_heads, 64_per_head]\n",
    "\n",
    "Transpose: [Batch, 8_heads, Sequence, 64_per_head]\n",
    "```\n",
    "\n",
    "**Why This Transformation?**\n",
    "- Enables parallel processing of all attention heads\n",
    "- Each head operates on its own 64-dimensional subspace\n",
    "- Maintains batch and sequence dimensions for efficient computation\n",
    "\n",
    "#### **‚ö° KV Cache: The Performance Game-Changer**\n",
    "\n",
    "**The Autoregressive Generation Problem:**\n",
    "```python\n",
    "# Without KV Cache (Inefficient):\n",
    "# Step 1: Process \"The cat\"\n",
    "# Step 2: Process \"The cat sat\" (recomputes \"The cat\" keys/values)\n",
    "# Step 3: Process \"The cat sat on\" (recomputes everything again)\n",
    "# Step N: Exponentially growing computation!\n",
    "```\n",
    "\n",
    "**KV Cache Solution:**\n",
    "```python\n",
    "# With KV Cache (Efficient):\n",
    "# Step 1: Process \"The cat\", cache K‚ÇÅ,V‚ÇÅ\n",
    "# Step 2: Process \"sat\", use cached K‚ÇÅ,V‚ÇÅ + new K‚ÇÇ,V‚ÇÇ\n",
    "# Step 3: Process \"on\", use cached K‚ÇÅ,V‚ÇÅ,K‚ÇÇ,V‚ÇÇ + new K‚ÇÉ,V‚ÇÉ\n",
    "# Step N: Linear computation growth!\n",
    "```\n",
    "\n",
    "**Implementation Details:**\n",
    "```python\n",
    "if use_cache and kv_cache is not None:\n",
    "    cached_key = kv_cache.get('key')      # Previously computed keys\n",
    "    cached_value = kv_cache.get('value')  # Previously computed values\n",
    "    \n",
    "    if cached_key is not None and cached_value is not None:\n",
    "        # Concatenate old and new keys/values\n",
    "        key = tf.concat([cached_key, key], axis=2)    # Along sequence dimension\n",
    "        value = tf.concat([cached_value, value], axis=2)\n",
    "```\n",
    "\n",
    "**Performance Impact:**\n",
    "- **Without Cache**: O(n¬≤) computation for each new token\n",
    "- **With Cache**: O(n) computation for each new token\n",
    "- **Speed Improvement**: 10x to 100x faster generation for long sequences\n",
    "\n",
    "#### **üé≠ Causal Masking: Preventing Future Information Leakage**\n",
    "\n",
    "**The Causality Requirement:**\n",
    "In autoregressive generation, token at position i should only attend to positions ‚â§ i\n",
    "\n",
    "**Mask Creation:**\n",
    "```python\n",
    "# Create lower triangular matrix\n",
    "mask = tf.linalg.band_part(tf.ones((seq_len, total_seq_len)), -1, 0)\n",
    "# Convert 0s to large negative values, 1s to 0s\n",
    "mask = tf.where(mask == 0, 1.0, 0.0)\n",
    "```\n",
    "\n",
    "**Mask Visualization:**\n",
    "```\n",
    "Sequence: [\"The\", \"cat\", \"sat\", \"on\"]\n",
    "\n",
    "Attention Matrix (before masking):\n",
    "     The  cat  sat  on\n",
    "The  [1.0  0.8  0.2  0.1]\n",
    "cat  [0.3  1.0  0.9  0.4]\n",
    "sat  [0.1  0.6  1.0  0.8]\n",
    "on   [0.2  0.3  0.7  1.0]\n",
    "\n",
    "Causal Mask:\n",
    "     The  cat  sat  on\n",
    "The  [0    -‚àû   -‚àû   -‚àû ]\n",
    "cat  [0    0    -‚àû   -‚àû ]\n",
    "sat  [0    0    0    -‚àû ]\n",
    "on   [0    0    0    0  ]\n",
    "\n",
    "After Softmax (masked positions become 0):\n",
    "     The  cat  sat  on\n",
    "The  [1.0  0.0  0.0  0.0]\n",
    "cat  [0.4  0.6  0.0  0.0]\n",
    "sat  [0.1  0.3  0.6  0.0]\n",
    "on   [0.1  0.2  0.3  0.4]\n",
    "```\n",
    "\n",
    "#### **üîß Layer Architecture and Weight Initialization**\n",
    "\n",
    "**Query, Key, Value Projections:**\n",
    "```python\n",
    "self.query_dense = Dense(embed_dim)  # X ‚Üí Q transformation\n",
    "self.key_dense = Dense(embed_dim)    # X ‚Üí K transformation  \n",
    "self.value_dense = Dense(embed_dim)  # X ‚Üí V transformation\n",
    "```\n",
    "\n",
    "**Why Separate Projections?**\n",
    "- **Learned Specialization**: Each projection learns different aspects\n",
    "- **Query**: \"What am I looking for?\"\n",
    "- **Key**: \"What do I represent for others to find?\"\n",
    "- **Value**: \"What information do I contribute?\"\n",
    "\n",
    "**Output Combination:**\n",
    "```python\n",
    "self.combine_heads = Dense(embed_dim)\n",
    "```\n",
    "- **Purpose**: Integrate information from all attention heads\n",
    "- **Learnable Mixing**: Model learns optimal way to combine head outputs\n",
    "- **Dimensionality**: Maps concatenated heads back to original embedding size\n",
    "\n",
    "#### **üé™ Attention Patterns in Practice**\n",
    "\n",
    "**Common Attention Patterns Discovered:**\n",
    "1. **Local Attention**: Focus on nearby words (n-gram patterns)\n",
    "2. **Syntactic Attention**: Connect grammatically related words\n",
    "3. **Semantic Attention**: Link semantically similar concepts\n",
    "4. **Positional Attention**: Attend to specific relative positions\n",
    "5. **Global Attention**: Some heads attend broadly across the sequence\n",
    "\n",
    "**Example Attention Visualization:**\n",
    "```\n",
    "Input: \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "Head 1 (Syntactic): \"fox\" ‚Üí \"jumps\" (subject-verb)\n",
    "Head 2 (Semantic): \"quick\" ‚Üí \"fast\", \"lazy\" ‚Üí \"slow\"\n",
    "Head 3 (Positional): Each word ‚Üí previous word\n",
    "Head 4 (Determiners): \"the\" ‚Üí following nouns\n",
    "```\n",
    "\n",
    "This Multi-Head Self-Attention mechanism is what enables Transformers to:\n",
    "- **Capture Complex Relationships**: Multiple types of dependencies simultaneously\n",
    "- **Process in Parallel**: Unlike sequential RNNs\n",
    "- **Scale Efficiently**: With KV cache optimization for generation\n",
    "- **Learn Interpretable Patterns**: Different heads specialize in different linguistic phenomena\n",
    "\n",
    "The combination of mathematical elegance, computational efficiency, and representational power makes this attention mechanism the foundation of modern NLP breakthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer_block_section",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "A complete transformer layer combining attention, feed-forward networks, and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer_block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    \"\"\"\n",
    "    TRANSFORMER BLOCK - A complete transformer layer\n",
    "    \n",
    "    Combines:\n",
    "    1. Multi-Head Self-Attention\n",
    "    2. Feed-Forward Network\n",
    "    3. Residual Connections\n",
    "    4. Layer Normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, kv_cache=None, use_cache=False, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass with residual connections and layer normalization\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention with residual connection\n",
    "        attn_output, new_cache = self.att(\n",
    "            inputs, \n",
    "            kv_cache=kv_cache, \n",
    "            use_cache=use_cache, \n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        output = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer_block_detailed_explanation",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Deep Dive: Transformer Block - The Complete Processing Unit**\n",
    "\n",
    "The Transformer Block is where the magic happens - it's a sophisticated processing unit that combines multiple architectural innovations to create a powerful, trainable, and stable deep learning component. Let's dissect every aspect of this engineering marvel.\n",
    "\n",
    "#### **üéØ The Transformer Block Architecture: A Symphony of Components**\n",
    "\n",
    "**The Complete Processing Pipeline:**\n",
    "```\n",
    "Input ‚Üí Layer Norm ‚Üí Multi-Head Attention ‚Üí Dropout ‚Üí Residual Add\n",
    "  ‚Üì\n",
    "Layer Norm ‚Üí Feed-Forward Network ‚Üí Dropout ‚Üí Residual Add ‚Üí Output\n",
    "```\n",
    "\n",
    "This architecture represents years of research refinements, with each component serving a critical purpose in the overall system.\n",
    "\n",
    "#### **üîó Residual Connections: The Highway to Deep Learning**\n",
    "\n",
    "**The Vanishing Gradient Problem:**\n",
    "In deep networks without residual connections:\n",
    "- Gradients diminish exponentially as they backpropagate\n",
    "- Deep layers receive virtually no learning signal\n",
    "- Training becomes ineffective beyond ~6-10 layers\n",
    "- Model performance degrades with increased depth\n",
    "\n",
    "**Residual Connection Solution:**\n",
    "```python\n",
    "# Instead of: output = F(input)\n",
    "# We use: output = F(input) + input\n",
    "\n",
    "attn_output = self.dropout1(attn_output, training=training)\n",
    "out1 = self.layernorm1(inputs + attn_output)  # Residual connection!\n",
    "\n",
    "ffn_output = self.dropout2(ffn_output, training=training)\n",
    "output = self.layernorm2(out1 + ffn_output)   # Another residual connection!\n",
    "```\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "Traditional: y = F(x)\n",
    "Residual: y = F(x) + x\n",
    "\n",
    "Gradient flow:\n",
    "‚àÇy/‚àÇx = ‚àÇF(x)/‚àÇx + 1\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- **Gradient Highway**: The \"+1\" term ensures gradients always have a direct path\n",
    "- **Identity Mapping**: If F(x) = 0, the layer becomes an identity function\n",
    "- **Easier Optimization**: Model can learn when to use vs skip transformations\n",
    "- **Stable Training**: Prevents gradient vanishing even in very deep networks\n",
    "\n",
    "**Practical Benefits:**\n",
    "- Enables training of 100+ layer networks\n",
    "- Faster convergence during training\n",
    "- Better gradient flow to early layers\n",
    "- More stable optimization dynamics\n",
    "\n",
    "#### **üß™ Layer Normalization: The Stability Engine**\n",
    "\n",
    "**The Normalization Formula:**\n",
    "```\n",
    "LayerNorm(x) = Œ≥ √ó (x - Œº) / œÉ + Œ≤\n",
    "\n",
    "Where:\n",
    "Œº = mean(x)     # Mean across features\n",
    "œÉ = std(x)      # Standard deviation across features\n",
    "Œ≥, Œ≤ = learnable parameters\n",
    "```\n",
    "\n",
    "**Implementation Details:**\n",
    "```python\n",
    "self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "```\n",
    "\n",
    "**Why epsilon=1e-6?**\n",
    "- Prevents division by zero when variance is very small\n",
    "- Ensures numerical stability in edge cases\n",
    "- Small enough to not affect normal operation\n",
    "\n",
    "**Layer Norm vs Batch Norm:**\n",
    "\n",
    "| Aspect | Batch Norm | Layer Norm |\n",
    "|--------|------------|------------|\n",
    "| **Normalization Axis** | Across batch dimension | Across feature dimension |\n",
    "| **Sequence Handling** | Poor (variable lengths) | Excellent |\n",
    "| **Training/Inference** | Different behavior | Consistent behavior |\n",
    "| **Batch Size Dependency** | Yes (needs large batches) | No (works with batch=1) |\n",
    "| **NLP Suitability** | Poor | Excellent |\n",
    "\n",
    "**Why Layer Norm is Perfect for Transformers:**\n",
    "1. **Variable Sequence Lengths**: Each sequence normalized independently\n",
    "2. **Consistent Behavior**: Same computation during training and inference\n",
    "3. **Batch Independence**: Works with any batch size, including 1\n",
    "4. **Feature Stability**: Normalizes across embedding dimensions\n",
    "\n",
    "#### **üèóÔ∏è Pre-Norm vs Post-Norm Architecture**\n",
    "\n",
    "**Our Implementation (Pre-Norm):**\n",
    "```python\n",
    "# Pre-normalization: Norm ‚Üí Function ‚Üí Residual\n",
    "normalized_input = self.layernorm1(inputs)\n",
    "attn_output, new_cache = self.att(normalized_input, ...)\n",
    "out1 = inputs + attn_output  # Residual connection\n",
    "```\n",
    "\n",
    "**Alternative (Post-Norm):**\n",
    "```python\n",
    "# Post-normalization: Function ‚Üí Residual ‚Üí Norm\n",
    "attn_output, new_cache = self.att(inputs, ...)\n",
    "out1 = self.layernorm1(inputs + attn_output)\n",
    "```\n",
    "\n",
    "**Pre-Norm Advantages (Why We Use It):**\n",
    "- **Better Gradient Flow**: Direct path for gradients through residual connections\n",
    "- **Training Stability**: Less prone to training instabilities\n",
    "- **Faster Convergence**: Often converges faster than post-norm\n",
    "- **Deeper Networks**: Enables training of very deep transformer models\n",
    "\n",
    "#### **üß† Feed-Forward Network: The Computational Powerhouse**\n",
    "\n",
    "**Architecture:**\n",
    "```python\n",
    "self.ffn = tf.keras.Sequential([\n",
    "    Dense(ff_dim, activation=\"relu\"),  # Expansion layer\n",
    "    Dense(embed_dim),                  # Projection layer\n",
    "])\n",
    "```\n",
    "\n",
    "**The Two-Layer MLP Design:**\n",
    "\n",
    "**Layer 1: Expansion (embed_dim ‚Üí ff_dim)**\n",
    "- **Purpose**: Expand representation to higher-dimensional space\n",
    "- **Typical Ratio**: ff_dim = 4 √ó embed_dim (e.g., 256 ‚Üí 1024)\n",
    "- **Activation**: ReLU for non-linearity and computational efficiency\n",
    "- **Mathematical Operation**: ReLU(xW‚ÇÅ + b‚ÇÅ)\n",
    "\n",
    "**Layer 2: Projection (ff_dim ‚Üí embed_dim)**\n",
    "- **Purpose**: Project back to original embedding dimension\n",
    "- **No Activation**: Linear transformation for maximum expressiveness\n",
    "- **Mathematical Operation**: (ReLU_output)W‚ÇÇ + b‚ÇÇ\n",
    "\n",
    "**Why This Architecture?**\n",
    "\n",
    "**The 4x Expansion Principle:**\n",
    "```\n",
    "Input: [batch, seq_len, 256]     # Original embedding space\n",
    "  ‚Üì\n",
    "Expand: [batch, seq_len, 1024]   # 4x larger intermediate space\n",
    "  ‚Üì\n",
    "Project: [batch, seq_len, 256]   # Back to embedding space\n",
    "```\n",
    "\n",
    "**Benefits of Expansion:**\n",
    "1. **Increased Capacity**: More parameters for complex transformations\n",
    "2. **Non-Linear Mixing**: ReLU enables complex feature interactions\n",
    "3. **Representation Learning**: Learns rich intermediate representations\n",
    "4. **Computational Efficiency**: Balanced between capacity and speed\n",
    "\n",
    "**ReLU Activation Choice:**\n",
    "```python\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "**Why ReLU over other activations?**\n",
    "- **Computational Efficiency**: Simple max(0, x) operation\n",
    "- **Gradient Properties**: No vanishing gradient for positive values\n",
    "- **Sparsity**: Creates sparse representations (many zeros)\n",
    "- **Empirical Success**: Proven effective in transformer architectures\n",
    "\n",
    "#### **üé™ Dropout: The Regularization Maestro**\n",
    "\n",
    "**Dropout Implementation:**\n",
    "```python\n",
    "self.dropout1 = Dropout(rate)  # After attention\n",
    "self.dropout2 = Dropout(rate)  # After feed-forward\n",
    "```\n",
    "\n",
    "**Dropout Mechanism:**\n",
    "- **Training**: Randomly set neurons to 0 with probability `rate`\n",
    "- **Inference**: Scale outputs by (1 - rate) to maintain expected values\n",
    "- **Purpose**: Prevent overfitting and improve generalization\n",
    "\n",
    "**Strategic Placement:**\n",
    "```python\n",
    "# After attention computation\n",
    "attn_output = self.dropout1(attn_output, training=training)\n",
    "\n",
    "# After feed-forward computation\n",
    "ffn_output = self.dropout2(ffn_output, training=training)\n",
    "```\n",
    "\n",
    "**Why These Specific Locations?**\n",
    "1. **After Complex Computations**: Applied after the most complex operations\n",
    "2. **Before Residual Addition**: Prevents dropout from affecting residual paths\n",
    "3. **Balanced Regularization**: Regularizes both attention and feed-forward paths\n",
    "\n",
    "#### **üîÑ The Complete Forward Pass: Step-by-Step Analysis**\n",
    "\n",
    "**Step 1: Attention Sub-Layer**\n",
    "```python\n",
    "# 1. Apply layer normalization to input\n",
    "normalized_input = self.layernorm1(inputs)\n",
    "\n",
    "# 2. Multi-head self-attention\n",
    "attn_output, new_cache = self.att(\n",
    "    normalized_input, \n",
    "    kv_cache=kv_cache, \n",
    "    use_cache=use_cache, \n",
    "    training=training\n",
    ")\n",
    "\n",
    "# 3. Apply dropout for regularization\n",
    "attn_output = self.dropout1(attn_output, training=training)\n",
    "\n",
    "# 4. Residual connection\n",
    "out1 = inputs + attn_output\n",
    "```\n",
    "\n",
    "**Step 2: Feed-Forward Sub-Layer**\n",
    "```python\n",
    "# 1. Apply layer normalization\n",
    "normalized_out1 = self.layernorm2(out1)\n",
    "\n",
    "# 2. Feed-forward network\n",
    "ffn_output = self.ffn(normalized_out1)\n",
    "\n",
    "# 3. Apply dropout\n",
    "ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "# 4. Final residual connection\n",
    "output = out1 + ffn_output\n",
    "```\n",
    "\n",
    "#### **üìä Information Flow and Representation Evolution**\n",
    "\n",
    "**Representation Evolution Through the Block:**\n",
    "```\n",
    "Input Representation:\n",
    "- Raw token embeddings + positional encoding\n",
    "- Contains basic semantic and positional information\n",
    "\n",
    "After Attention:\n",
    "- Context-aware representations\n",
    "- Each token knows about relevant other tokens\n",
    "- Relationship information encoded\n",
    "\n",
    "After Feed-Forward:\n",
    "- Non-linearly transformed representations\n",
    "- Complex feature combinations\n",
    "- Task-specific patterns learned\n",
    "\n",
    "Output Representation:\n",
    "- Rich, context-aware, non-linearly processed features\n",
    "- Ready for next transformer block or final prediction\n",
    "```\n",
    "\n",
    "#### **üéØ Design Principles and Trade-offs**\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "1. **Pre-Norm Architecture**: Better training stability vs slightly lower final performance\n",
    "2. **4x FF Expansion**: Balance between capacity and computational cost\n",
    "3. **ReLU Activation**: Simplicity and efficiency vs potentially richer activations\n",
    "4. **Dropout Placement**: Effective regularization vs potential information loss\n",
    "\n",
    "**Computational Complexity:**\n",
    "```\n",
    "Attention: O(n¬≤ √ó d) where n = sequence length, d = embedding dimension\n",
    "Feed-Forward: O(n √ó d √ó ff_dim) = O(n √ó d¬≤) typically\n",
    "Total per block: O(n¬≤ √ó d + n √ó d¬≤)\n",
    "```\n",
    "\n",
    "**Scaling Considerations:**\n",
    "- **Short sequences**: Feed-forward dominates computation\n",
    "- **Long sequences**: Attention dominates computation\n",
    "- **Large models**: Both components scale significantly\n",
    "\n",
    "This Transformer Block design represents the culmination of deep learning research, combining:\n",
    "- **Attention mechanisms** for relationship modeling\n",
    "- **Residual connections** for gradient flow\n",
    "- **Layer normalization** for training stability\n",
    "- **Feed-forward networks** for non-linear transformation\n",
    "- **Dropout regularization** for generalization\n",
    "\n",
    "The result is a powerful, trainable, and scalable building block that can be stacked to create models of arbitrary depth and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## Complete Transformer Model\n",
    "\n",
    "The full transformer model with embedding, positional encoding, and multiple transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(Model):\n",
    "    \"\"\"\n",
    "    COMPLETE TRANSFORMER MODEL FOR TEXT GENERATION\n",
    "    \n",
    "    This implements a decoder-only transformer suitable for autoregressive text generation.\n",
    "    Features include KV cache optimization and proper causal masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) \n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output projection layer\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def positional_encoding(self, seq_length, embed_dim):\n",
    "        \"\"\"\n",
    "        Generate sinusoidal positional encoding\n",
    "        \"\"\"\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(seq_length)[:, np.newaxis],\n",
    "            np.arange(embed_dim)[np.newaxis, :],\n",
    "            embed_dim\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, embed_dim):\n",
    "        \"\"\"\n",
    "        Calculate angles for positional encoding\n",
    "        \"\"\"\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, inputs, kv_cache=None, use_cache=False, start_pos=0, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass with KV cache support for efficient generation\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Handle positional encoding based on generation phase\n",
    "        if start_pos > 0 and start_pos < self.seq_length:\n",
    "            # Decode phase: specific positions\n",
    "            pos_encoding = self.pos_encoding[:, start_pos:start_pos + seq_len, :]\n",
    "        else:\n",
    "            # Training or prefill phase\n",
    "            if start_pos >= self.seq_length:\n",
    "                # Handle out-of-bounds positions\n",
    "                pos_encoding = self.pos_encoding[:, -1:, :]\n",
    "                pos_encoding = tf.tile(pos_encoding, [1, seq_len, 1])\n",
    "            else:\n",
    "                pos_encoding = self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Convert tokens to embeddings and add positional encoding\n",
    "        x = self.embedding(inputs)\n",
    "        x += pos_encoding\n",
    "        \n",
    "        # Initialize KV cache if needed\n",
    "        if use_cache and kv_cache is None:\n",
    "            kv_cache = [None] * self.num_layers\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        new_caches = []\n",
    "        for i, transformer_block in enumerate(self.transformer_blocks):\n",
    "            layer_cache = kv_cache[i] if kv_cache else None\n",
    "            \n",
    "            x, new_cache = transformer_block(\n",
    "                x, \n",
    "                kv_cache=layer_cache, \n",
    "                use_cache=use_cache, \n",
    "                training=training\n",
    "            )\n",
    "            \n",
    "            new_caches.append(new_cache)\n",
    "        \n",
    "        # Project to vocabulary probabilities\n",
    "        output = self.dense(x)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, new_caches\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep_section",
   "metadata": {},
   "source": [
    "## Data Preparation and Utility Functions\n",
    "\n",
    "Functions for creating training sequences and managing data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(text, seq_length):\n",
    "    \"\"\"\n",
    "    Create training sequences for language modeling\n",
    "    \n",
    "    Creates input-target pairs where target is input shifted by one position\n",
    "    for next-token prediction training.\n",
    "    \"\"\"\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    # Create overlapping sequences\n",
    "    for i in range(len(text) - seq_length):\n",
    "        input_seq = text[i:i + seq_length]\n",
    "        target_seq = text[i + 1:i + seq_length + 1]\n",
    "        \n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "    \n",
    "    return np.array(input_seqs), np.array(target_seqs)\n",
    "\n",
    "\n",
    "def load_corpus(corpus_source):\n",
    "    \"\"\"\n",
    "    Load text corpus from web or local file\n",
    "    \"\"\"\n",
    "    if corpus_source == \"web\":\n",
    "        print(\"Loading Shakespeare dataset from web...\")\n",
    "        path_to_file = get_file(\n",
    "            'shakespeare.txt',\n",
    "            'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "        )\n",
    "        text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "        print(f\"Web dataset loaded. Text length: {len(text)} characters\")\n",
    "        \n",
    "    elif corpus_source == \"local\":\n",
    "        print(\"Loading corpus from local file 'corpus.txt'...\")\n",
    "        try:\n",
    "            with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            print(f\"Local corpus loaded. Text length: {len(text)} characters\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                \"corpus.txt not found. Please make sure the file exists in the current directory.\"\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Failed to decode with UTF-8, trying with latin-1...\")\n",
    "            with open('corpus.txt', 'r', encoding='latin-1') as f:\n",
    "                text = f.read()\n",
    "            print(f\"Local corpus loaded with latin-1 encoding. Text length: {len(text)} characters\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid corpus source: {corpus_source}. Must be 'web' or 'local'.\"\n",
    "        )\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation_section",
   "metadata": {},
   "source": [
    "## Text Generation with KV Cache Optimization\n",
    "\n",
    "Advanced text generation function with KV cache support for dramatic speed improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_kv_cache(model, vectorizer, start_string, seq_length, \n",
    "                                num_generate=100, temperature=1.0, use_kv_cache=True):\n",
    "    \"\"\"\n",
    "    OPTIMIZED TEXT GENERATION WITH KV CACHE SUPPORT\n",
    "    \n",
    "    This function demonstrates the significant performance improvement possible with\n",
    "    KV caching during autoregressive generation.\n",
    "    \n",
    "    Key Features:\n",
    "    - Prefill phase: Process initial prompt, build attention cache\n",
    "    - Decode phase: Generate tokens one by one, reusing cached computations\n",
    "    - Performance comparison between cached and standard generation\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained transformer model\n",
    "    - vectorizer: Text preprocessing pipeline\n",
    "    - start_string: Initial text to begin generation\n",
    "    - seq_length: Model's expected sequence length\n",
    "    - num_generate: Number of tokens to generate\n",
    "    - temperature: Sampling temperature for creativity control\n",
    "    - use_kv_cache: Whether to use KV cache optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess input text\n",
    "    input_eval = vectorizer([start_string]).numpy()\n",
    "    \n",
    "    # Handle sequence length mismatches\n",
    "    if input_eval.shape[1] < seq_length:\n",
    "        # Pad with zeros at the beginning\n",
    "        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n",
    "        input_eval = np.concatenate((padding, input_eval), axis=1)\n",
    "    elif input_eval.shape[1] > seq_length:\n",
    "        # Truncate to keep only the last tokens\n",
    "        input_eval = input_eval[:, -seq_length:]\n",
    "\n",
    "    input_eval = tf.convert_to_tensor(input_eval)\n",
    "    text_generated = []\n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "    \n",
    "    if use_kv_cache:\n",
    "        print(\"Using KV Cache for generation...\")\n",
    "        \n",
    "        # PREFILL PHASE: Process prompt and build cache\n",
    "        predictions, kv_cache = model(\n",
    "            input_eval, \n",
    "            use_cache=True,\n",
    "            start_pos=0,\n",
    "            training=False\n",
    "        )\n",
    "        \n",
    "        # Sample first token\n",
    "        last_predictions = predictions[0, -1, :]\n",
    "        last_predictions = last_predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "            tf.expand_dims(last_predictions, 0), \n",
    "            num_samples=1\n",
    "        )[0, 0].numpy()\n",
    "        \n",
    "        if predicted_id < len(vocab):\n",
    "            text_generated.append(vocab[predicted_id])\n",
    "        \n",
    "        current_pos = input_eval.shape[1]\n",
    "        \n",
    "        # DECODE PHASE: Generate tokens using cache\n",
    "        for i in range(num_generate - 1):\n",
    "            next_token = tf.convert_to_tensor([[predicted_id]])\n",
    "            \n",
    "            # Generate next token using cached keys/values\n",
    "            predictions, kv_cache = model(\n",
    "                next_token,\n",
    "                kv_cache=kv_cache,\n",
    "                use_cache=True,\n",
    "                start_pos=current_pos,\n",
    "                training=False\n",
    "            )\n",
    "            \n",
    "            last_predictions = predictions[0, -1, :]\n",
    "            last_predictions = last_predictions / temperature\n",
    "            predicted_id = tf.random.categorical(\n",
    "                tf.expand_dims(last_predictions, 0), \n",
    "                num_samples=1\n",
    "            )[0, 0].numpy()\n",
    "            \n",
    "            if predicted_id < len(vocab):\n",
    "                text_generated.append(vocab[predicted_id])\n",
    "            \n",
    "            current_pos += 1\n",
    "            \n",
    "    else:\n",
    "        print(\"Using standard generation (no KV cache)...\")\n",
    "        \n",
    "        # STANDARD GENERATION: No cache optimization\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval, use_cache=False, training=False)\n",
    "            predictions = predictions[0, -1, :]\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(\n",
    "                tf.expand_dims(predictions, 0), \n",
    "                num_samples=1\n",
    "            )[0, 0].numpy()\n",
    "\n",
    "            # Update input sequence\n",
    "            input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)\n",
    "            input_eval = input_eval[:, -seq_length:]\n",
    "            input_eval = tf.convert_to_tensor(input_eval)\n",
    "\n",
    "            if predicted_id < len(vocab):\n",
    "                text_generated.append(vocab[predicted_id])\n",
    "\n",
    "    return start_string + ' ' + ' '.join(text_generated)\n",
    "\n",
    "\n",
    "def generate_text(model, vectorizer, start_string, seq_length, num_generate=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Legacy text generation function for backward compatibility\n",
    "    \"\"\"\n",
    "    return generate_text_with_kv_cache(\n",
    "        model, vectorizer, start_string, seq_length, \n",
    "        num_generate, temperature, use_kv_cache=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "Complete training pipeline with monitoring, visualization, and model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    COMPREHENSIVE MODEL TRAINING PIPELINE\n",
    "    \n",
    "    Implements complete training workflow:\n",
    "    1. Data loading and preprocessing\n",
    "    2. Model architecture setup\n",
    "    3. Training with callbacks and logging\n",
    "    4. Visualization and monitoring\n",
    "    5. Model persistence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    corpus_source = \"local\"  # Options: \"web\" for Shakespeare, \"local\" for corpus.txt\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    text = load_corpus(corpus_source)\n",
    "    print(\"Preview of the dataset:\")\n",
    "    print(text[:500])\n",
    "\n",
    "    # Text preprocessing configuration\n",
    "    vocab_size = 10000\n",
    "    seq_length = 100\n",
    "    \n",
    "    # Create text vectorizer\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int'\n",
    "    )\n",
    "    \n",
    "    # Adapt vectorizer to text data\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1)\n",
    "    vectorizer.adapt(text_ds)\n",
    "\n",
    "    # Convert text to numerical format\n",
    "    vectorized_text = vectorizer([text])[0]\n",
    "    print(f\"Vectorized text shape: {vectorized_text.shape}\")\n",
    "    print(f\"First 10 vectorized tokens: {vectorized_text.numpy()[:10]}\")\n",
    "\n",
    "    # Generate training sequences\n",
    "    X, Y = create_sequences(vectorized_text.numpy(), seq_length)\n",
    "    print(f\"Number of sequences generated: {len(X)}\")\n",
    "    \n",
    "    assert X.size > 0, \"Input data X is empty\"\n",
    "    assert Y.size > 0, \"Target data Y is empty\"\n",
    "    \n",
    "    X = tf.convert_to_tensor(X)\n",
    "    Y = tf.convert_to_tensor(Y)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of Y: {Y.shape}\")\n",
    "\n",
    "    # Model architecture configuration\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    num_layers = 4\n",
    "\n",
    "    # Create the transformer model\n",
    "    model = TransformerModel(\n",
    "        vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length\n",
    "    )\n",
    "\n",
    "    # Build the model\n",
    "    _ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
    "\n",
    "    # Configure training\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy'\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # Setup logging\n",
    "    logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "    \n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=logdir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "    \n",
    "    print(f\"TensorBoard logs in: {os.path.abspath(logdir)}\")\n",
    "    print(f\"Run: tensorboard --logdir {logdir}\")\n",
    "\n",
    "    # Training callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Execute training\n",
    "    history = model.fit(\n",
    "        X, Y,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # Save model weights\n",
    "    weights_save_path = \"transformer_model.weights.h5\"\n",
    "    model.save_weights(weights_save_path)\n",
    "    print(f\"Model weights saved to: {weights_save_path}\")\n",
    "\n",
    "    # Save vectorizer and model configuration\n",
    "    import pickle\n",
    "    vectorizer_path = \"text_vectorizer.pkl\"\n",
    "    \n",
    "    model_metadata = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'vocab_size': vocab_size,\n",
    "        'seq_length': seq_length,\n",
    "        'embed_dim': embed_dim,\n",
    "        'num_heads': num_heads,\n",
    "        'ff_dim': ff_dim,\n",
    "        'num_layers': num_layers\n",
    "    }\n",
    "    \n",
    "    with open(vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(model_metadata, f)\n",
    "    print(f\"Vectorizer and model parameters saved to: {vectorizer_path}\")\n",
    "\n",
    "    # Create training visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training Loss Over Time', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = os.path.join(logdir, 'training_loss.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Training loss plot saved to: {plot_path}\")\n",
    "\n",
    "    return model, vectorizer, vocab_size, seq_length, logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_function_section",
   "metadata": {},
   "source": [
    "## Main Function and Execution Modes\n",
    "\n",
    "The main function provides different execution modes to run the transformer model. This allows you to easily switch between training, generation, and performance comparison modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    MAIN EXECUTION FUNCTION - CONTROLS PROGRAM FLOW\n",
    "    \n",
    "    This function serves as the entry point for the transformer model implementation.\n",
    "    It provides four different execution modes to accommodate various use cases:\n",
    "    \n",
    "    1. Training Mode: Train a new model from scratch\n",
    "    2. Generation Mode: Generate text using a pre-trained model\n",
    "    3. Performance Comparison Mode: Compare KV cache vs standard generation\n",
    "    4. Both Mode: Train a model and then run performance comparison\n",
    "    \n",
    "    Configuration Variables:\n",
    "    - mode: Determines which operation to perform\n",
    "    - use_kv_cache: Enables/disables KV cache optimization\n",
    "    - weights_path: Path to saved model weights\n",
    "    - vectorizer_path: Path to saved text vectorizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration variables - Change these as needed\n",
    "    mode = 'generate_compare'  # Options: 'train', 'generate', 'generate_compare', 'both'\n",
    "    use_kv_cache = True  # Set to False to disable KV cache optimization\n",
    "    weights_path = 'transformer_model.weights.h5'\n",
    "    vectorizer_path = 'text_vectorizer.pkl'\n",
    "    \n",
    "    if mode == 'train':\n",
    "        print(\"=\"*60)\n",
    "        print(\"TRAINING MODE\")\n",
    "        print(\"=\"*60)\n",
    "        train()\n",
    "        \n",
    "    elif mode == 'generate':\n",
    "        print(\"=\"*60)\n",
    "        print(\"GENERATION MODE\")\n",
    "        print(\"=\"*60)\n",
    "        generate(use_kv_cache=use_kv_cache, \n",
    "                weights_path=weights_path, \n",
    "                vectorizer_path=vectorizer_path)\n",
    "        \n",
    "    elif mode == 'generate_compare':\n",
    "        print(\"=\"*60)\n",
    "        print(\"PERFORMANCE COMPARISON MODE\")\n",
    "        print(\"=\"*60)\n",
    "        generate_compare(weights_path=weights_path, \n",
    "                        vectorizer_path=vectorizer_path)\n",
    "        \n",
    "    else:  # both\n",
    "        print(\"=\"*60)\n",
    "        print(\"TRAINING MODE\")\n",
    "        print(\"=\"*60)\n",
    "        model, vectorizer, vocab_size, seq_length, logdir = train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PERFORMANCE COMPARISON MODE\")\n",
    "        print(\"=\"*60)\n",
    "        generate_compare(weights_path=weights_path, \n",
    "                        vectorizer_path=vectorizer_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution_modes_section",
   "metadata": {},
   "source": [
    "## Understanding the Execution Modes\n",
    "\n",
    "The main function provides four distinct execution modes, each designed for different use cases:\n",
    "\n",
    "### 1. **Training Mode** (`mode = 'train'`)\n",
    "- **Purpose**: Train a new transformer model from scratch\n",
    "- **What it does**:\n",
    "  - Loads text corpus (either from web or local file)\n",
    "  - Preprocesses text data and creates training sequences\n",
    "  - Builds and trains the transformer model\n",
    "  - Saves model weights and vectorizer for later use\n",
    "  - Creates training visualizations and logs\n",
    "- **Use when**: You want to train a new model or retrain with different data\n",
    "- **Requirements**: Text corpus (either `corpus.txt` file or web download)\n",
    "\n",
    "### 2. **Generation Mode** (`mode = 'generate'`)\n",
    "- **Purpose**: Generate text using a pre-trained model\n",
    "- **What it does**:\n",
    "  - Loads pre-trained model weights and vectorizer\n",
    "  - Generates text with or without KV cache optimization\n",
    "  - Allows you to experiment with different prompts and parameters\n",
    "- **Use when**: You have a trained model and want to generate text\n",
    "- **Requirements**: Pre-trained model weights and vectorizer files\n",
    "- **Configuration**: Set `use_kv_cache=True` for faster generation\n",
    "\n",
    "### 3. **Performance Comparison Mode** (`mode = 'generate_compare'`)\n",
    "- **Purpose**: Compare KV cache optimization vs standard generation\n",
    "- **What it does**:\n",
    "  - Loads pre-trained model\n",
    "  - Generates identical text using both methods\n",
    "  - Measures and compares generation times\n",
    "  - Demonstrates the performance benefits of KV caching\n",
    "- **Use when**: You want to see the performance improvement from KV cache\n",
    "- **Requirements**: Pre-trained model weights and vectorizer files\n",
    "- **Output**: Side-by-side comparison with timing metrics\n",
    "\n",
    "### 4. **Both Mode** (`mode = 'both'`)\n",
    "- **Purpose**: Complete workflow from training to performance demonstration\n",
    "- **What it does**:\n",
    "  - First runs training mode (trains new model)\n",
    "  - Then automatically runs performance comparison\n",
    "  - Provides end-to-end demonstration\n",
    "- **Use when**: You want to see the complete pipeline in action\n",
    "- **Requirements**: Text corpus for training\n",
    "- **Duration**: Longest execution time (includes full training)\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Key Parameters to Modify:\n",
    "\n",
    "```python\n",
    "mode = 'generate_compare'  # Change this to select execution mode\n",
    "use_kv_cache = True        # Enable/disable KV cache in generation mode\n",
    "weights_path = 'transformer_model.weights.h5'  # Path to model weights\n",
    "vectorizer_path = 'text_vectorizer.pkl'        # Path to text vectorizer\n",
    "```\n",
    "\n",
    "### File Requirements by Mode:\n",
    "\n",
    "| Mode | Required Files | Generated Files |\n",
    "|------|----------------|----------------|\n",
    "| `train` | `corpus.txt` (if using local corpus) | `transformer_model.weights.h5`, `text_vectorizer.pkl`, logs |\n",
    "| `generate` | `transformer_model.weights.h5`, `text_vectorizer.pkl` | Generated text output |\n",
    "| `generate_compare` | `transformer_model.weights.h5`, `text_vectorizer.pkl` | Performance comparison results |\n",
    "| `both` | `corpus.txt` (if using local corpus) | All training files + comparison results |\n",
    "\n",
    "## Quick Start Guide\n",
    "\n",
    "1. **First Time Usage**: Set `mode = 'train'` to train a new model\n",
    "2. **Text Generation**: Set `mode = 'generate'` to create text with your trained model\n",
    "3. **Performance Testing**: Set `mode = 'generate_compare'` to see KV cache benefits\n",
    "4. **Full Demo**: Set `mode = 'both'` for complete training and testing workflow\n",
    "\n",
    "Simply change the `mode` variable in the main function and run the script to switch between different operations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
} 