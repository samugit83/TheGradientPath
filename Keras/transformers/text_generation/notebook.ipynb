{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6e23a1",
   "metadata": {},
   "source": [
    "# Transformer-Based Text Generation: Building an LLM from Scratch with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92d7e0",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Welcome! This notebook provides a comprehensive guide to implementing cutting-edge Transformer architectures for text generation and language modeling. Unlike traditional RNNs that process text sequentially, Transformers leverage self-attention mechanisms to capture long-range dependencies and complex linguistic patterns in text data.\n",
    "\n",
    "The script (`main.py`) performs several key operations:\n",
    "1. Loads text corpus data (Shakespeare dataset from web or local corpus file).\n",
    "2. Implements a complete Transformer architecture from scratch using Keras with TensorFlow.\n",
    "3. Trains the model for next-token prediction using multi-head self-attention and positional encoding.\n",
    "4. Visualizes the training process and model architecture using TensorBoard and Visualkeras.\n",
    "5. Generates coherent text sequences using the trained language model.\n",
    "\n",
    "This implementation provides a solid foundation for real-world natural language processing applications, from creative writing assistance to chatbots and content generation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e6fb2",
   "metadata": {},
   "source": [
    "## ðŸ“º Watch the Tutorial\n",
    "\n",
    "Prefer a video walkthrough? Check out the accompanying tutorial on YouTube:\n",
    "\n",
    "[Transformer Text Generation Tutorial](https://youtu.be/oOpe8lvGIrI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4c2a1",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Start Guide\n",
    "\n",
    "Ready to run the code? Follow these simple steps to set up your environment and execute the Transformer text generation model:\n",
    "\n",
    "### Step 1: Create a Python Virtual Environment\n",
    "```bash\n",
    "# Create a new virtual environment\n",
    "python -m venv /path/to/your/project/\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On macOS/Linux:\n",
    "source /path/to/your/project/bin/activate\n",
    "```\n",
    "\n",
    "### Step 2: Install Required Dependencies\n",
    "```bash\n",
    "# Install all required packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "The `requirements.txt` file includes:\n",
    "- `tensorflow>=2.0` - Deep learning framework\n",
    "- `numpy` - Numerical computing\n",
    "- `matplotlib` - Plotting and visualization\n",
    "- `visualkeras` - Model architecture visualization\n",
    "\n",
    "### Step 3: Prepare Your Text Corpus (Optional)\n",
    "The script can work with two data sources:\n",
    "- **Web dataset:** Automatically downloads Shakespeare's complete works\n",
    "- **Local corpus:** Place your own text file as `corpus.txt` in the project directory\n",
    "\n",
    "To use your own text data:\n",
    "```bash\n",
    "# Place your text file in the project directory\n",
    "cp /path/to/your/text/data.txt corpus.txt\n",
    "```\n",
    "\n",
    "### Step 4: Run the Main Script\n",
    "```bash\n",
    "# Execute the Transformer text generation model\n",
    "python main.py\n",
    "```\n",
    "\n",
    "### What Happens When You Run It?\n",
    "1. **Data Loading:** Downloads Shakespeare dataset or loads local corpus.txt\n",
    "2. **Text Preprocessing:** Tokenizes and vectorizes text using TextVectorization\n",
    "3. **Sequence Generation:** Creates input-target pairs for next-token prediction\n",
    "4. **Model Building:** Constructs the Transformer architecture with positional encoding\n",
    "5. **Training:** Trains the model with early stopping and TensorBoard logging\n",
    "6. **Visualization:** Generates model architecture diagrams and training plots\n",
    "7. **Text Generation:** Creates novel text sequences using the trained model\n",
    "\n",
    "### Monitoring Training Progress\n",
    "The script automatically sets up TensorBoard logging. After running, you can monitor training in real-time:\n",
    "```bash\n",
    "# Launch TensorBoard (the script will show you the exact command)\n",
    "tensorboard --logdir logs/[timestamp]\n",
    "```\n",
    "\n",
    "Then open your browser to `http://localhost:6006` to view:\n",
    "- Training loss curves\n",
    "- Model architecture visualization\n",
    "- Text generation samples\n",
    "- Model weights and gradients\n",
    "\n",
    "**Expected Runtime:** Approximately 10-20 minutes on a modern CPU, faster with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1a3c",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "The complete workflow of our Transformer-based text generation system:\n",
    "\n",
    "![Transformer Text Generation Architecture](text_gen_transformers.png)\n",
    "\n",
    "This diagram illustrates the end-to-end pipeline from raw text data through the Transformer architecture to generated text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4aab3",
   "metadata": {},
   "source": [
    "## 2. Core Concepts: Why Transformers for Text Generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3db18",
   "metadata": {},
   "source": [
    "### Traditional Approaches vs. Transformers\n",
    "Traditionally, text generation relied on:\n",
    "1. **Statistical methods** like n-gram models - work well for short sequences but struggle with long-term coherence.\n",
    "2. **Recurrent Neural Networks** like LSTMs or GRUs - process text sequentially, building context word by word.\n",
    "\n",
    "**The fundamental limitation:** RNNs process information sequentially, meaning they can only look at previous words when generating the next token, and they often struggle with very long-term dependencies due to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d688bc",
   "metadata": {},
   "source": [
    "### The Transformer Revolution\n",
    "Transformers completely revolutionize text generation:\n",
    "- **Parallel Processing:** Instead of processing text word by word, Transformers can attend to all positions simultaneously through self-attention.\n",
    "- **Long-Range Dependencies:** They can capture relationships between words that are very far apart - crucial for maintaining coherence in long texts.\n",
    "- **Multiple Perspectives:** Multi-head attention allows the model to focus on different types of linguistic patterns simultaneously (syntax, semantics, style).\n",
    "\n",
    "Think of it like having multiple editors simultaneously reviewing a text, where each editor can instantly reference any part of the document to understand context and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b556b67",
   "metadata": {},
   "source": [
    "### Self-Attention Mechanism: The Core Innovation\n",
    "Self-attention works by creating three different representations of input text:\n",
    "- **Queries (Q):** \"What information am I looking for?\"\n",
    "- **Keys (K):** \"What information is available at each position?\"\n",
    "- **Values (V):** \"What information should I extract and combine?\"\n",
    "\n",
    "The attention mechanism computes similarity scores between queries and keys, then uses these scores to create weighted combinations of values. This allows each word to directly attend to any other word in the sequence, regardless of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add3148",
   "metadata": {},
   "source": [
    "### Positional Encoding: Understanding Word Order\n",
    "Since Transformers process all positions simultaneously, they need a way to understand word order:\n",
    "1. **Sinusoidal Encoding:** Uses sine and cosine functions with different frequencies\n",
    "2. **Position-Dependent Patterns:** Each position gets a unique encoding pattern\n",
    "3. **Relative Position:** Model learns relationships between positions\n",
    "4. **Embedding Addition:** Positional encodings are added to word embeddings\n",
    "\n",
    "This allows the model to understand that \"The cat sat on the mat\" is different from \"The mat sat on the cat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f04ab",
   "metadata": {},
   "source": [
    "### Next-Token Prediction: The Training Objective\n",
    "Language models are trained using a simple but powerful objective:\n",
    "- **Input:** \"The quick brown fox jumps over the\"\n",
    "- **Target:** \"lazy\" (next word)\n",
    "- **Learning:** Model learns to predict the probability distribution over all possible next words\n",
    "- **Generation:** At inference, sample from this distribution to generate coherent text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561c10b",
   "metadata": {},
   "source": [
    "## 3. Code Deep Dive: Complete Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a4faa",
   "metadata": {},
   "source": [
    "### File Structure Overview\n",
    "The entire implementation is contained in `main.py`, featuring:\n",
    "- Custom TransformerBlock layers built from scratch\n",
    "- Multi-head self-attention with positional encoding\n",
    "- Complete training pipeline with text generation\n",
    "- Data preprocessing and evaluation utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1496a5a",
   "metadata": {},
   "source": [
    "### Environment Setup and GPU Configuration\n",
    "```python\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer, Dense, LayerNormalization, Dropout, Embedding, \n",
    "    MultiHeadAttention, TextVectorization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_file\n",
    "import matplotlib.pyplot as plt\n",
    "import visualkeras\n",
    "\n",
    "# Configure TensorFlow to use GPU if available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"{len(gpus)} Physical GPUs configured.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting up GPU memory growth: {e}\")\n",
    "```\n",
    "**Key Points:**\n",
    "- GPU memory growth prevents TensorFlow from allocating all GPU memory at startup\n",
    "- Essential for training large language models efficiently\n",
    "- TextVectorization handles tokenization and vocabulary management automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb1484",
   "metadata": {},
   "source": [
    "### TransformerBlock: The Building Block of Language Models\n",
    "```python\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "```\n",
    "**Architecture Components:**\n",
    "- **Multi-Head Attention:** Captures relationships between all word positions\n",
    "- **Feed-Forward Network:** Position-wise transformations for each word\n",
    "- **Residual Connections:** Prevents vanishing gradients in deep networks\n",
    "- **Layer Normalization:** Stabilizes training and speeds convergence\n",
    "- **Dropout:** Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef231c49",
   "metadata": {},
   "source": [
    "### Complete TransformerModel: From Embeddings to Text Generation\n",
    "```python\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) \n",
    "                                 for _ in range(num_layers)]\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        x = self.embedding(inputs)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "```\n",
    "**Model Pipeline:**\n",
    "1. **Token Embedding:** Converts word indices to dense vectors\n",
    "2. **Positional Encoding:** Adds position information to embeddings\n",
    "3. **Transformer Layers:** Stack of self-attention and feed-forward blocks\n",
    "4. **Output Projection:** Maps to vocabulary size for next-token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fed880",
   "metadata": {},
   "source": [
    "### Positional Encoding Implementation\n",
    "```python\n",
    "def positional_encoding(self, seq_length, embed_dim):\n",
    "    angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], \n",
    "                               np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd indices\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(self, pos, i, embed_dim):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "    return pos * angle_rates\n",
    "```\n",
    "**Mathematical Foundation:**\n",
    "- Uses sinusoidal functions with different frequencies for each dimension\n",
    "- Even dimensions use sine, odd dimensions use cosine\n",
    "- Creates unique, learnable patterns for each position\n",
    "- Allows model to understand relative and absolute positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0916f",
   "metadata": {},
   "source": [
    "### Data Preprocessing: From Text to Training Sequences\n",
    "```python\n",
    "def create_sequences(text, seq_length):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    for i in range(len(text) - seq_length):\n",
    "        input_seq = text[i:i + seq_length]\n",
    "        target_seq = text[i + 1:i + seq_length + 1]\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "    return np.array(input_seqs), np.array(target_seqs)\n",
    "```\n",
    "**Sequence Generation Strategy:**\n",
    "- Creates sliding windows of text for training\n",
    "- Example: \"Hello world how are\" â†’ input: \"Hello world how\", target: \"world how are\"\n",
    "- Each position learns to predict the next token\n",
    "- Maximizes training data from single text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d87317",
   "metadata": {},
   "source": [
    "### Text Vectorization and Vocabulary Management\n",
    "```python\n",
    "# Setup text vectorization\n",
    "vocab_size = 10000\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert text to token sequences\n",
    "vectorized_text = vectorizer([text])[0]\n",
    "```\n",
    "**Preprocessing Pipeline:**\n",
    "- **TextVectorization:** Handles tokenization, lowercasing, punctuation\n",
    "- **Vocabulary Building:** Creates word-to-index mapping from corpus\n",
    "- **Sequence Conversion:** Transforms text into integer sequences\n",
    "- **Automatic Handling:** Manages unknown words and special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97610fb6",
   "metadata": {},
   "source": [
    "### Text Generation: From Model to Creative Writing\n",
    "```python\n",
    "def generate_text(model, vectorizer, start_string, seq_length, \n",
    "                 num_generate=100, temperature=1.0):\n",
    "    # Convert start string to tokens\n",
    "    input_eval = vectorizer([start_string]).numpy()\n",
    "    \n",
    "    # Ensure correct sequence length\n",
    "    if input_eval.shape[1] < seq_length:\n",
    "        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n",
    "        input_eval = np.concatenate((padding, input_eval), axis=1)\n",
    "    \n",
    "    text_generated = []\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = predictions[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample next token\n",
    "        predicted_id = tf.random.categorical(tf.expand_dims(predictions, 0), \n",
    "                                           num_samples=1)[0, 0].numpy()\n",
    "        \n",
    "        # Update input sequence\n",
    "        input_eval = np.append(input_eval, [[predicted_id]], axis=1)\n",
    "        input_eval = input_eval[:, -seq_length:]\n",
    "        \n",
    "        # Convert token back to word\n",
    "        vocab = vectorizer.get_vocabulary()\n",
    "        if predicted_id < len(vocab):\n",
    "            text_generated.append(vocab[predicted_id])\n",
    "    \n",
    "    return start_string + ' ' + ' '.join(text_generated)\n",
    "```\n",
    "**Generation Process:**\n",
    "- **Seed Text:** Start with user-provided prompt\n",
    "- **Prediction:** Model outputs probability distribution over vocabulary\n",
    "- **Temperature Sampling:** Controls randomness (low = conservative, high = creative)\n",
    "- **Autoregressive:** Each generated token becomes input for next prediction\n",
    "- **Sliding Window:** Maintains fixed context length during generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0361121",
   "metadata": {},
   "source": [
    "### Training Pipeline with Corpus Loading\n",
    "```python\n",
    "def load_corpus(corpus_source):\n",
    "    if corpus_source == \"web\":\n",
    "        print(\"Loading Shakespeare dataset from web...\")\n",
    "        path_to_file = get_file('shakespeare.txt', \n",
    "                               'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "        text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "    elif corpus_source == \"local\":\n",
    "        print(\"Loading corpus from local file 'corpus.txt'...\")\n",
    "        with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    corpus_source = \"local\"  # \"web\" for Shakespeare, \"local\" for corpus.txt\n",
    "    text = load_corpus(corpus_source)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    num_layers = 4\n",
    "    seq_length = 100\n",
    "```\n",
    "**Flexible Data Sources:**\n",
    "- **Web Dataset:** Shakespeare's complete works (classic benchmark)\n",
    "- **Local Corpus:** Your own text data for domain-specific generation\n",
    "- **Preprocessing:** Automatic encoding detection and error handling\n",
    "- **Scalability:** Works with any size text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c022af0",
   "metadata": {},
   "source": [
    "### Training and Monitoring Setup\n",
    "```python\n",
    "    # Model compilation\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    # Setup logging and callbacks\n",
    "    logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "    \n",
    "    # Architecture visualization\n",
    "    visualkeras.layered_view(model, to_file='transformer_text_model_architecture.png')\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(X, Y, epochs=20, batch_size=32, \n",
    "                        callbacks=[early_stopping, tensorboard_cb])\n",
    "```\n",
    "**Training Features:**\n",
    "- **Sparse Categorical Crossentropy:** Efficient loss for large vocabularies\n",
    "- **Early Stopping:** Prevents overfitting with patience mechanism\n",
    "- **TensorBoard Integration:** Real-time training monitoring\n",
    "- **Model Visualization:** Beautiful architecture diagrams\n",
    "- **Checkpointing:** Saves best model weights automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1b1e3",
   "metadata": {},
   "source": [
    "### Text Generation and Evaluation\n",
    "```python\n",
    "    # Generate text samples\n",
    "    start_string = \"the object of our\"\n",
    "    generated_text = generate_text(model, vectorizer, start_string, seq_length, \n",
    "                                 num_generate=100, temperature=0.7)\n",
    "    \n",
    "    # Longer generation with different temperature\n",
    "    longer_text = generate_text(model, vectorizer, start_string, seq_length, \n",
    "                              num_generate=200, temperature=0.8)\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(logdir, 'generated_text.txt'), 'w') as f:\n",
    "        f.write(f\"Generated text (100 tokens):\\n{generated_text}\\n\\n\")\n",
    "        f.write(f\"Generated text (200 tokens):\\n{longer_text}\\n\")\n",
    "```\n",
    "**Generation Strategies:**\n",
    "- **Temperature Control:** Balance between coherence and creativity\n",
    "- **Multiple Samples:** Generate various lengths and styles\n",
    "- **Prompt Engineering:** Different starting phrases for diverse outputs\n",
    "- **Quality Assessment:** Manual evaluation of coherence and style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be61e6",
   "metadata": {},
   "source": [
    "## 4. Setup and Running the Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f1f23",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Python 3.8+ (recommended)\n",
    "- CUDA-compatible GPU (optional but recommended for performance)\n",
    "- `pip` package manager\n",
    "- Text corpus file (optional - defaults to Shakespeare dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a2d1c",
   "metadata": {},
   "source": [
    "### Installation Steps\n",
    "1. **Clone the repository:**\n",
    "   ```bash\n",
    "   git clone <your-repo-url>\n",
    "   cd transformers/text_generation\n",
    "   ```\n",
    "\n",
    "2. **Create virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv transformer_env\n",
    "   ```\n",
    "\n",
    "3. **Activate virtual environment:**\n",
    "   - Linux/macOS:\n",
    "     ```bash\n",
    "     source transformer_env/bin/activate\n",
    "     ```\n",
    "   - Windows:\n",
    "     ```bash\n",
    "     transformer_env\\Scripts\\activate\n",
    "     ```\n",
    "\n",
    "4. **Install required packages:**\n",
    "   ```bash\n",
    "   pip install tensorflow numpy matplotlib visualkeras\n",
    "   ```\n",
    "\n",
    "5. **For GPU support (optional):**\n",
    "   ```bash\n",
    "   pip install tensorflow-gpu\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b8f7a",
   "metadata": {},
   "source": [
    "### Required Dependencies (requirements.txt)\n",
    "```txt\n",
    "tensorflow>=2.8.0\n",
    "numpy>=1.21.0\n",
    "matplotlib>=3.5.0\n",
    "visualkeras>=0.0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b5e6f",
   "metadata": {},
   "source": [
    "### Running the Project\n",
    "Execute the main script:\n",
    "```bash\n",
    "python main.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Load text corpus (Shakespeare or local corpus.txt)\n",
    "- Build the Transformer model from scratch\n",
    "- Train for up to 20 epochs with early stopping\n",
    "- Create visualizations and save to timestamped log directory\n",
    "- Generate text samples with different parameters\n",
    "- Save model and generated text outputs\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Loading corpus from local file 'corpus.txt'...\n",
    "Local corpus loaded. Text length: 125000 characters\n",
    "Vectorized text shape: (23456,)\n",
    "Number of sequences generated: 23356\n",
    "Model: \"transformer_model\"\n",
    "...\n",
    "Total params: 8,467,200\n",
    "Trainable params: 8,467,200\n",
    "TensorBoard logs in: /path/to/logs/20231201-143022\n",
    "Starting training...\n",
    "Epoch 1/20\n",
    "730/730 [==============================] - 45s 62ms/step - loss: 6.2341\n",
    "...\n",
    "Generated text:\n",
    "the object of our study is to understand the nature of language and meaning...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c9d2a",
   "metadata": {},
   "source": [
    "### Monitoring with TensorBoard\n",
    "Start TensorBoard to monitor training:\n",
    "```bash\n",
    "tensorboard --logdir logs\n",
    "```\n",
    "\n",
    "Navigate to `http://localhost:6006` to view:\n",
    "- **Scalars:** Training loss progression over epochs\n",
    "- **Graphs:** Complete model computational graph\n",
    "- **Histograms:** Weight and bias distributions\n",
    "- **Images:** Model architecture diagrams\n",
    "- **Text:** Generated text samples during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e2c3b",
   "metadata": {},
   "source": [
    "### Customization Options\n",
    "Modify hyperparameters in the main() function:\n",
    "```python\n",
    "# Data source\n",
    "corpus_source = \"local\"  # \"web\" for Shakespeare, \"local\" for corpus.txt\n",
    "\n",
    "# Model architecture\n",
    "embed_dim = 256          # Embedding dimension\n",
    "num_heads = 4            # Number of attention heads\n",
    "ff_dim = 512            # Feed-forward dimension\n",
    "num_layers = 4          # Number of Transformer blocks\n",
    "seq_length = 100        # Context window size\n",
    "vocab_size = 10000      # Vocabulary size\n",
    "\n",
    "# Generation parameters\n",
    "temperature = 0.7       # Creativity vs. coherence (0.1-2.0)\n",
    "num_generate = 100      # Number of tokens to generate\n",
    "```\n",
    "\n",
    "**Hyperparameter Guidelines:**\n",
    "- Increase `embed_dim` for richer word representations\n",
    "- More `num_heads` captures diverse linguistic patterns\n",
    "- Deeper `num_layers` for complex language understanding\n",
    "- Larger `seq_length` for longer context (but more memory)\n",
    "- Higher `temperature` for more creative but less coherent text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d6e8e",
   "metadata": {},
   "source": [
    "## 5. Advanced Topics and Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d9c2f",
   "metadata": {},
   "source": [
    "### Causal (Masked) Attention for Language Modeling\n",
    "For truly autoregressive generation, implement causal masking:\n",
    "```python\n",
    "def create_causal_mask(seq_len):\n",
    "    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    return mask  # Lower triangular matrix\n",
    "\n",
    "# In MultiHeadAttention:\n",
    "causal_mask = create_causal_mask(seq_length)\n",
    "attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "```\n",
    "This prevents the model from \"cheating\" by looking at future tokens during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b9c8d",
   "metadata": {},
   "source": [
    "### Advanced Sampling Strategies\n",
    "Improve text quality with sophisticated sampling:\n",
    "```python\n",
    "def top_k_sampling(logits, k=40):\n",
    "    top_k_logits, top_k_indices = tf.nn.top_k(logits, k=k)\n",
    "    probabilities = tf.nn.softmax(top_k_logits)\n",
    "    return tf.random.categorical(tf.expand_dims(probabilities, 0), 1)\n",
    "\n",
    "def nucleus_sampling(logits, p=0.9):\n",
    "    sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "    sorted_logits = tf.gather(logits, sorted_indices)\n",
    "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits))\n",
    "    nucleus = tf.cast(cumulative_probs < p, tf.float32)\n",
    "    return nucleus * sorted_logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e3c7f",
   "metadata": {},
   "source": [
    "### Fine-tuning for Specific Domains\n",
    "Adapt pre-trained models to specific writing styles:\n",
    "```python\n",
    "# Load pre-trained model\n",
    "base_model = tf.keras.models.load_model('transformer_text_generation_model.h5')\n",
    "\n",
    "# Freeze early layers, fine-tune later layers\n",
    "for layer in base_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Train on domain-specific data with lower learning rate\n",
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), \n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e4c1e",
   "metadata": {},
   "source": [
    "### Real-World Text Data Integration\n",
    "Process various text formats for training:\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load from JSON dataset\n",
    "with open('dialogue_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    text = ' '.join([item['text'] for item in data])\n",
    "\n",
    "# Load from CSV\n",
    "df = pd.read_csv('text_dataset.csv')\n",
    "text = ' '.join(df['content'].astype(str))\n",
    "\n",
    "# Clean and preprocess\n",
    "text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "text = ' '.join(text.split())  # Remove extra whitespace\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c1d2",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5e9df",
   "metadata": {},
   "source": [
    "### Memory Management for Large Models\n",
    "- **Gradient Checkpointing:** Trade computation for memory in deep models\n",
    "- **Mixed Precision Training:** Use float16 for forward pass, float32 for gradients\n",
    "- **Batch Size Optimization:** Find optimal batch size for your GPU memory\n",
    "- **Sequence Length:** Balance context length with memory constraints\n",
    "\n",
    "### Training Acceleration\n",
    "- **Learning Rate Scheduling:** Warm-up and cosine decay strategies\n",
    "- **Multi-GPU Training:** Distribute training across multiple devices\n",
    "- **Data Pipeline Optimization:** Use tf.data for efficient data loading\n",
    "- **Compiled Training:** Use @tf.function decorators for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4a5e8",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8f3c6",
   "metadata": {},
   "source": [
    "### Training Issues\n",
    "- **Loss not decreasing:** Check learning rate, data quality, model initialization\n",
    "- **Exploding gradients:** Use gradient clipping, reduce learning rate\n",
    "- **Overfitting:** Increase dropout, reduce model size, add regularization\n",
    "- **GPU out of memory:** Reduce batch size, sequence length, or model size\n",
    "- **Poor text quality:** Increase model capacity, train longer, improve data quality\n",
    "\n",
    "### Generation Issues\n",
    "- **Repetitive text:** Adjust temperature, use nucleus/top-k sampling\n",
    "- **Incoherent output:** Lower temperature, increase context length\n",
    "- **Slow generation:** Optimize model architecture, use caching\n",
    "- **Memory errors:** Reduce sequence length during generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ac6e9",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "This notebook has provided a comprehensive walkthrough of implementing Transformer architecture for text generation and language modeling. We've covered:\n",
    "\n",
    "- **Theoretical Foundation:** Self-attention mechanism and positional encoding\n",
    "- **Complete Implementation:** From scratch Transformer blocks in Keras\n",
    "- **Training Pipeline:** Data preprocessing, model compilation, and monitoring\n",
    "- **Text Generation:** Autoregressive sampling and creativity control\n",
    "- **Practical Applications:** Domain-specific language model development\n",
    "\n",
    "**Key Advantages of Transformers for Text Generation:**\n",
    "- Parallel processing enables efficient training on long sequences\n",
    "- Self-attention captures long-range linguistic dependencies\n",
    "- Multiple attention heads learn diverse language patterns\n",
    "- Scalable architecture suitable for large language models\n",
    "- Flexible generation with temperature and sampling control\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different model architectures and hyperparameters\n",
    "- Try domain-specific datasets (poetry, code, dialogue)\n",
    "- Implement advanced sampling techniques (nucleus, top-k)\n",
    "- Add causal masking for proper autoregressive training\n",
    "- Scale up to larger models and datasets\n",
    "- Deploy for real-time text generation applications\n",
    "\n",
    "This implementation provides a solid foundation for building sophisticated language models and text generation systems. The Transformer's flexibility makes it adaptable to various NLP tasks including chatbots, creative writing assistants, code generation, and content creation tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
} 