# AI Agent Framework Benchmark

![Multi-Agent Workflow](workflow.png)

## ğŸ¯ Purpose

This repository provides a comprehensive, production-grade comparison of **7 major AI agent frameworks** by implementing the **exact same multi-agent system** across all of them. Rather than toy examples or superficial comparisons, this benchmark evaluates each framework through a real-world application: an intelligent conversational AI system with agent routing, tool integration, MCP server support, memory management, and state handling.

By keeping the functionality identical across implementations, we can objectively compare:
- **Code complexity and readability**
- **Developer experience and ease of setup**
- **Framework abstractions and flexibility**
- **Documentation quality**
- **Feature completeness (tools, memory, state, MCP integration)**

This benchmark is designed for AI engineers, MLOps practitioners, and developers who need to make informed decisions about which agent framework to use in production systems.

---

## ğŸ¬ Video Tutorial

This benchmark accompanies a full-length video tutorial where we:
- Walk through each framework implementation
- Explain architectural decisions and trade-offs
- Demonstrate live comparisons and debugging
- Provide production deployment insights

**ğŸ‘‰ [Watch the full tutorial on YouTube](https://youtu.be/ZIflDkdvOSA)**

---

## ğŸ—ï¸ System Architecture

Each framework implementation includes:

### Core Components
- **ğŸ¯ Routing/Orchestrator Agent**: Intelligently routes user queries to specialized agents
- **âš–ï¸ Legal Expert Agent**: Handles law-related questions and legal topics
- **ğŸ”§ Operational/General Agent**: Manages programming, tools, and general knowledge queries

### Advanced Features
- **ğŸ› ï¸ Tool Integration**: Multiple tools including weather lookup, calculator, web search
- **ğŸ”Œ MCP Server Integration**: Model Context Protocol server support for extended capabilities
- **ğŸ§  Memory Management**: Persistent conversation history and context retention
- **ğŸ“Š State Management**: Sophisticated state handling across agent interactions
- **ğŸ›¡ï¸ Content Safety**: Guardrails for safe and appropriate interactions
- **ğŸ“ˆ Usage Tracking**: Token consumption and cost monitoring

---

## ğŸ“Š Comprehensive Benchmark Results

We evaluated each framework across **6 critical dimensions**, scoring each on metrics like abstraction level, code readability, setup complexity, developer experience, documentation quality, and flexibility.

### ğŸ“ˆ Overall Rankings

| Rank | Framework | Total Score | Best For |
|------|-----------|-------------|----------|
| ğŸ¥‡ | **LangChain/LangGraph** | 284/360 | Maximum flexibility, complex workflows, perfect documentation |
| ğŸ¥ˆ | **OpenAI Agents** | 277/360 | Rapid development, minimal code, clean APIs |
| ğŸ¥‰ | **CrewAI** | 249/360 | Simple delegation patterns, rapid prototyping |
| 4ï¸âƒ£ | **LlamaIndex** | 227/360 | Balanced approach, workflow integration |
| 5ï¸âƒ£ | **AutoGen** | 195/360 | Enterprise async infrastructure, MCP integration |
| 6ï¸âƒ£ | **Semantic Kernel** | 178/360 | Microsoft ecosystem, plugin architecture |
| ğŸ“ | **Vanilla Python** | Baseline | Full control, maximum flexibility, zero framework overhead |

### ğŸ† Category Winners

| Category | Winner | Score | Key Strength |
|----------|--------|-------|--------------|
| **Agent Orchestration** | LangGraph | 48/60 | Perfect documentation & flexibility with state machine architecture |
| **Tool Integration** | CrewAI | 51/60 | Pydantic-powered automatic schema generation |
| **State Management** | LangGraph | 46/60 | Type-safe automatic state merging with maximum control |
| **Memory Management** | LangGraph | 50/60 | Seamless state-based memory with checkpointing |
| **MCP Integration** | OpenAI | 51/60 | Native first-class support with minimal configuration |
| **Other Features** | LangGraph | 48/60 | Best-in-class token tracking, Code generation and test & structured output utilities |

---

## ğŸ“‘ Detailed Benchmark Reports

Dive deep into each evaluation category:

- **[Agent Orchestration Benchmark](agent_orchestrate_score.md)** - Multi-agent coordination and workflow patterns
- **[Tool Integration Benchmark](tool_integration_score.md)** - Custom tool creation and integration
- **[State Management Benchmark](state_management_score.md)** - State handling and coordination
- **[Memory Management Benchmark](memory_management_score.md)** - Conversation history and context retention
- **[MCP Server Integration Benchmark](mcp_server_integration_score.md)** - Model Context Protocol server support
- **[Other Features Benchmark](other_small_feats_score.md)** - Token tracking, structured output, guardrails, code execution
- **[Overall Summary & Recommendations](overall_table_score_score.md)** - Complete comparison and final recommendations

Each report includes:
- âœ… Detailed scoring methodology (1-10 scale across 6 metrics)
- âœ… Framework-specific insights and trade-offs
- âœ… Practical recommendations for different use cases
- âœ… Code complexity comparisons

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- OpenAI API key (set as `OPENAI_API_KEY` environment variable)
- Basic understanding of AI agents and LLMs

### Quick Start for Any Framework

Each framework follows the same setup pattern:

```bash
# Navigate to the framework directory
cd <framework_name>

# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# On Linux/macOS:
source venv/bin/activate
# On Windows:
# venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set your OpenAI API key
export OPENAI_API_KEY='your-api-key-here'

# Run the application
python main.py
```

---

## ğŸ“¦ Framework-Specific Setup

### 1. **AutoGen** (`autogen/`)

```bash
cd autogen
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Async-first architecture with runtime introspection
- Enterprise-grade infrastructure
- Complex setup but high flexibility

---

### 2. **CrewAI** (`crewai/`)

```bash
cd crewai
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Highest abstraction level
- Declarative agent definition
- Pydantic-powered tool integration

---

### 3. **LangChain/LangGraph** (`langchain_langraph/`)

```bash
cd langchain_langraph
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- State machine architecture
- Perfect documentation
- Maximum customization potential
- Best overall framework (284/360)

---

### 4. **LlamaIndex** (`llamaindex/`)

```bash
cd llamaindex
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Workflow-based architecture
- Balanced abstraction level
- Good MCP integration

---

### 5. **OpenAI Agents** (`open_ai/`)

```bash
cd open_ai
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Minimal code, maximum productivity
- Native MCP support (51/60)
- Clean, intuitive APIs
- Second-best overall (277/360)

---

### 6. **Semantic Kernel** (`semantic_kernel/`)

```bash
cd semantic_kernel
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Microsoft ecosystem integration
- Plugin architecture
- Class-based patterns

---

### 7. **Vanilla Python** (`vanilla/`)

```bash
cd vanilla
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY='your-api-key-here'
python main.py
```

**Key Features:**
- Zero framework overhead
- Direct OpenAI API usage
- Complete control and transparency
- Baseline for complexity comparison

---

## ğŸ“ Educational Context

This benchmark is part of **TheGradientPath**, a comprehensive learning resource for modern AI and machine learning engineering. The repository covers everything from foundational ML concepts to production-grade systems.

### Related Projects in TheGradientPath:

- **[Real-World Cyber Attack Prediction](../../RealWorldProjects/CyberAttackPrediction/)** - Production ML system with AWS deployment
- **[RAG Systems](../../Rag/)** - Hybrid multi-vector knowledge graph RAG, vision RAG
- **[LLM Fine-Tuning](../../LLMFineTuning/)** - PEFT techniques, GRPO reasoning, SFT with tool choice
- **[MCP From Scratch](../../MCPFromScratch/)** - Build Model Context Protocol from scratch
- **[Transformers from Scratch](../../Keras/transformers/)** - KV cache, text generation, time series

---

## ğŸ’¡ Key Takeaways

### Choose **LangChain/LangGraph** if:
- âœ… You want the best overall framework (284/360)
- âœ… You need maximum flexibility and customization
- âœ… You're building complex, sophisticated workflows
- âœ… You want perfect documentation and community support
- âœ… Open-source ecosystem matters

### Choose **OpenAI Agents** if:
- âœ… You want maximum productivity with minimal code (277/360)
- âœ… You need the absolute best MCP integration
- âœ… You're comfortable with framework lock-in
- âœ… Rapid prototyping is your priority

### Choose **CrewAI** if:
- âœ… You need rapid prototyping capabilities
- âœ… Simple delegation patterns fit your use case
- âœ… You want minimal setup complexity

### Choose **Vanilla Python** if:
- âœ… You need complete transparency and control
- âœ… You want to avoid framework lock-in
- âœ… You're building custom abstractions
- âœ… You want to deeply understand agent mechanics

---

## ğŸ“ Repository Structure

```
AgentFrameworkBenchmark/
â”œâ”€â”€ workflow.png                          # System architecture diagram
â”œâ”€â”€ README.md                             # This file
â”‚
â”œâ”€â”€ Benchmark Reports/
â”‚   â”œâ”€â”€ agent_orchestrate_score.md        # Agent coordination evaluation
â”‚   â”œâ”€â”€ tool_integration_score.md         # Tool system comparison
â”‚   â”œâ”€â”€ state_management_score.md         # State handling analysis
â”‚   â”œâ”€â”€ memory_management_score.md        # Memory system comparison
â”‚   â”œâ”€â”€ mcp_server_integration_score.md   # MCP protocol integration
â”‚   â”œâ”€â”€ other_small_feats_score.md        # Utilities and extras
â”‚   â””â”€â”€ overall_table_score_score.md      # Complete summary
â”‚
â”œâ”€â”€ Framework Implementations/
â”‚   â”œâ”€â”€ autogen/                          # AutoGen implementation
â”‚   â”œâ”€â”€ crewai/                           # CrewAI implementation
â”‚   â”œâ”€â”€ langchain_langraph/               # LangChain/LangGraph implementation
â”‚   â”œâ”€â”€ llamaindex/                       # LlamaIndex implementation
â”‚   â”œâ”€â”€ open_ai/                          # OpenAI Agents implementation
â”‚   â”œâ”€â”€ semantic_kernel/                  # Semantic Kernel implementation
â”‚   â””â”€â”€ vanilla/                          # Vanilla Python implementation
â”‚
â””â”€â”€ Each framework folder contains:
    â”œâ”€â”€ main.py                           # Entry point
    â”œâ”€â”€ requirements.txt                  # Dependencies
    â”œâ”€â”€ code_generator_agents/ or         # Agent implementations
    â”‚   code_generator_multiagent/ or
    â”‚   handoff_agents.py
    â”œâ”€â”€ tools.py                          # Tool definitions
    â”œâ”€â”€ state.py                          # State management
    â”œâ”€â”€ prompts.py                        # Agent prompts
    â”œâ”€â”€ logging_config.py                 # Logging setup
    â””â”€â”€ logs/                             # Runtime logs
```

---

## ğŸ”¬ Methodology

### Scoring System (1-10 scale)

Each framework is evaluated on 6 metrics per category:

1. **Abstraction Grade** (Higher = More hidden complexity)
2. **Code Readability & Simplicity** (Higher = Better)
3. **Setup Complexity** (Higher = Easier setup)
4. **Developer Experience** (Higher = Better)
5. **Documentation & Clarity** (Higher = Better)
6. **Flexibility & Customization** (Higher = Better)

**Maximum Score:** 60 per category, 360 total across all 6 categories

### What Makes This Benchmark Unique

- âœ… **Identical Functionality**: Same features across all frameworks
- âœ… **Production-Grade**: Real-world complexity, not toy examples
- âœ… **Objective Metrics**: Quantifiable scoring across multiple dimensions
- âœ… **Hands-On**: Actual working code you can run and modify
- âœ… **Comprehensive**: Covers all critical aspects (agents, tools, memory, state, MCP)
- âœ… **Practical**: Clear recommendations for different use cases

---

## ğŸ§ª Use Cases Evaluated

The benchmark tests each framework through:

1. **Multi-Agent Orchestration**: Routing between specialized agents (Legal Expert, General Agent)
2. **Tool Execution**: Weather API, calculator, web search, custom tools
3. **MCP Server Integration**: External capabilities via Model Context Protocol
4. **Memory Persistence**: Conversation history across sessions
5. **State Management**: Complex state coordination between agents
6. **Content Safety**: Input guardrails and safety checks
7. **Usage Tracking**: Token consumption and cost monitoring
8. **Structured Output**: Type-safe responses with Pydantic models
9. **Error Handling**: Graceful failure and recovery
10. **Production Readiness**: Logging, monitoring, deployment considerations

---

## ğŸ› ï¸ Advanced Configuration

### Environment Variables

```bash
# Required
export OPENAI_API_KEY='your-api-key-here'

# Optional
export LOG_LEVEL='INFO'          # DEBUG, INFO, WARNING, ERROR, CRITICAL
export MODEL_NAME='gpt-4o'       # Model to use
export ENABLE_MCP='true'         # Enable MCP server integration
export PERSISTENT_MEMORY='true'  # Enable conversation persistence
```

### Logging

All frameworks use consistent logging:
- Logs are written to `logs/` directory in each framework folder
- Automatic log rotation (max 10MB per file)
- Configurable log levels
- Old logs cleaned up automatically (30-day retention)

---

## ğŸ¤ Contributing

Found a bug? Want to add a new framework? Contributions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Implement your changes
4. Follow the existing structure and patterns
5. Add tests and documentation
6. Submit a pull request

---

## ğŸ“§ Contact & Support

**Instructor:** Samuele Giampieri  
**Role:** AI Engineer specializing in Knowledge Graphs, NLP, and AI-Driven Systems

I'm passionate about bridging cutting-edge research with practical applications. My expertise spans knowledge graphs, multi-agent systems, RAG architectures, and production ML deployment.

### Connect With Me:
- ğŸ™ **GitHub:** [github.com/samugit83](https://github.com/samugit83)
- ğŸ’¼ **LinkedIn:** [Connect for AI/ML discussions](#)
- ğŸ¥ **YouTube:** Subscribe for weekly deep dives into AI, agents, and machine learning
- ğŸ“§ **Email:** [Your consulting/collaboration inquiries welcome](#)

### Support This Project:
- â­ **Star this repository** if you find it helpful
- ğŸ‘ **Like the video tutorial** on YouTube
- ğŸ”” **Subscribe** for more cutting-edge AI content
- ğŸ’¬ **Share your results** and feedback in the discussions
- ğŸ¤ **Contribute** improvements and new framework implementations

---

## ğŸ“œ License

This project is part of TheGradientPath educational initiative. Free to use for learning, research, and commercial applications.

---

## ğŸ™ Acknowledgments

Special thanks to:
- The open-source community for building these incredible frameworks
- OpenAI for pioneering agent architectures
- All contributors who helped refine this benchmark
- The AI/ML community for feedback and suggestions

---

## ğŸ”— Related Resources

- **[TheGradientPath Main Repository](https://github.com/samugit83/TheGradientPath)** - Complete AI/ML learning path
- **[Agent Framework Documentation](#)** - Links to official docs for each framework
- **[MCP Specification](https://modelcontextprotocol.io/)** - Model Context Protocol standards
- **[Production AI Systems Guide](#)** - Best practices for deploying AI in production

---

**Built with â¤ï¸ by Samuele Giampieri | Part of TheGradientPath Learning Initiative**

*Last Updated: October 2025*
