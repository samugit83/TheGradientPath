{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning a Small Language Model for Tool Selection using SFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "This document explains the Python script `main.py`, which demonstrates how to fine-tune a relatively small pre-trained language model (LLM) for a specific task: **tool selection**.\n",
        "\n",
        "**Supervised Fine-Tuning (SFT)** is a crucial technique for adapting large, general-purpose language models (LLMs) to perform well on specific downstream tasks. It leverages a pre-trained model, which has already learned vast amounts of information about language structure and world knowledge during its initial, often unsupervised, training phase. The \"supervised\" aspect means we provide the model with labeled examples, typically consisting of an input prompt and the desired output or completion. Unlike the broad pre-training stage, SFT uses these explicit input-output pairs to guide the model's learning towards a specific behavior or response format. During SFT, the model's internal parameters (weights) are further adjusted by training on this smaller, task-specific dataset. This process fine-tunes the model's capabilities, making it more adept at the target task without needing to retrain it from scratch, which would be computationally prohibitive. The goal is to minimize the difference between the model's generated output and the provided target completion for each example in the fine-tuning dataset. This allows us to steer the model's behavior, improve its accuracy for certain types of questions, or teach it to follow specific instructions or output formats. In this tutorial, for instance, we use SFT to teach the model a very specific skill: mapping a user's query to the name of the most appropriate helper function (like a calculator, weather API, or reminder tool), using examples where the prompt is the user query and the completion is the correct tool name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì∫ Watch the Tutorial\n",
        "\n",
        "Prefer a video walkthrough? Check out the accompanying tutorial on YouTube:\n",
        "\n",
        "[Fine-Tuning a Small LLM for Tool Selection (SFT)](https://youtu.be/Ain269vmeZg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concepts\n",
        "\n",
        "* **Language Model (LM)** ‚Äì learns statistical patterns of language and can generate text.\n",
        "* **Causal LM** ‚Äì predicts the next token given everything before it (e.g. GPT-style models).\n",
        "* **Fine-Tuning / SFT** ‚Äì continues training a pre-trained model on a smaller, task-specific corpus under *supervision* (we know the desired answer for every prompt).\n",
        "* **Tokenizer** ‚Äì maps text ‚Üî integer IDs; every LM has its own tokenizer.\n",
        "* **Hugging Face `transformers` / `trl`** ‚Äì high-level libraries that spare us from boiler-plate training code.\n",
        "* **Special Token** ‚Äì a sentinel string (here `<my_tool_selection>`) added to the vocabulary to mark where the model should output the tool name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Script Breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Environment Setup\n",
        "\n",
        "Run the two code blocks below **once** at the very start of your notebook:\n",
        "Install (or upgrade) all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚á© 1 | install / upgrade deps (comment-out after first run)\n",
        "!pip install -q torch datasets transformers trl accelerate --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1  Imports\n",
        "\n",
        "```python\n",
        "import random           # utilities for reproducible shuffling / sampling\n",
        "import torch            # underlying deep-learning framework (PyTorch)\n",
        "from datasets import Dataset  # HF library for fast, memory-mapped datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,   # generic loader for any causal LM\n",
        "    AutoTokenizer,          # matching tokenizer loader\n",
        "    pipeline,               # convenience wrapper for inference\n",
        ")\n",
        "from trl import SFTConfig, SFTTrainer  # higher-level SFT helpers\n",
        "from gen_dataset import generate_raw_examples  # custom data generator\n",
        "```\n",
        "No configurable parameters here, but remember that **matching model + tokenizer IDs** are mandatory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2  Data Generation & Preparation\n",
        "\n",
        "```python\n",
        "raw_examples = generate_raw_examples(10000)  # ‚ûú List[Tuple[str, str]]\n",
        "special = \"<my_tool_selection>\"             # sentinel delimiter\n",
        "```\n",
        "* **`generate_raw_examples(n)`** (custom): returns **n** `(query, tool)` pairs. In this tutorial the function produces *synthetic* queries.  \n",
        "  *Parameter* | *Type* | *Meaning*  \n",
        "  `n` | `int` | how many examples to create.\n",
        "\n",
        "```python\n",
        "dataset = Dataset.from_list([\n",
        "    {\n",
        "        \"prompt\":     f\"User: {q} {special}\\nAssistant:\",\n",
        "        \"completion\": f\" {tool}\"\n",
        "    }\n",
        "    for q, tool in raw_examples\n",
        "])\n",
        "```\n",
        "* **`Dataset.from_list(list_of_dicts)`** builds a HF Dataset.  Each dict must contain all the columns that the trainer will later reference (`prompt`, `completion`).\n",
        "\n",
        "```python\n",
        "splits = dataset.train_test_split(test_size=0.20, seed=42)\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "```\n",
        "* **`test_size`** ‚Äì fraction (or absolute count) reserved for validation.  \n",
        "* **`seed`** ‚Äì ensures deterministic shuffling so future reruns get the exact same split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3  Model & Tokenizer Loading\n",
        "\n",
        "```python\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"   # ‚Üê let HF dispatch layers across all GPUs / CPU\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # safe default\n",
        "```\n",
        "* **`device_map=\"auto\"`** ‚Äì Hugging Face will inspect available hardware (multiple GPUs / CPU RAM) and **split the weights** to fit in memory automatically. Good for big models on limited VRAM.\n",
        "* **`pad_token`** ‚Äì token used to right-pad sequences inside a batch so they all have the same length. Some older tokenizers miss it, so we reuse `eos_token`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4  Adding the Special Delimiter\n",
        "\n",
        "```python\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [special]})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "```\n",
        "* **`add_special_tokens`** ‚Äì extends the vocabulary and returns how many were added.  The new **ID** is accessible via `tokenizer.convert_tokens_to_ids(special)`.\n",
        "* **`resize_token_embeddings(new_size)`** ‚Äì resizes the model‚Äôs embedding / output matrices so the extra token gets its own learnable vector.  Must be called **after** adding tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5  `SFTConfig` ‚Äì every hyper-parameter explained\n",
        "\n",
        "```python\n",
        "sft_args = SFTConfig(\n",
        "    output_dir=\"./tool_choice_sft\",      # where checkpoints / logs go\n",
        "    num_train_epochs=2,                   # scan full dataset twice\n",
        "    per_device_train_batch_size=4,        # effective batch = 4 √ó gradient_accumulation_steps\n",
        "    gradient_accumulation_steps=1,        # accumulate ‚ß∏N steps before weight update\n",
        "    learning_rate=5e-5,                  # AdamW step size\n",
        "    warmup_ratio=0.10,                   # first 10 % of total steps = LR warm-up\n",
        "    logging_steps=5,                     # log train loss every 5 optimisation steps\n",
        "    eval_strategy=\"steps\",              # run evaluation every *N* steps (not epochs)\n",
        "    eval_steps=20,                       # evaluate on `eval_ds` every 20 steps\n",
        "    save_steps=50,                       # save checkpoint every 50 steps\n",
        "    report_to=[\"none\"],                 # disable WandB / TensorBoard\n",
        ")\n",
        "```\n",
        "**Additional notes**\n",
        "* `gradient_accumulation_steps` lets you simulate a larger batch without extra VRAM: gradients are accumulated locally and the optimiser runs only every *k* mini-batches.\n",
        "* `learning_rate` pairs with the *AdamW* optimiser under the hood (the default for `SFTTrainer`).\n",
        "* The **total number of optimiser steps** = `(train_examples / batch) √ó epochs / gradient_accumulation_steps` ‚Äì warm-up ratio is applied over that count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6  `SFTTrainer` initialisation parameters\n",
        "\n",
        "```python\n",
        "trainer = SFTTrainer(\n",
        "    model=model,               # the LM we just loaded / resized\n",
        "    args=sft_args,             # all hyper-params\n",
        "    train_dataset=train_ds,    # 80 % split\n",
        "    eval_dataset=eval_ds,      # 20 % split (never back-prop through)\n",
        "    tokenizer=tokenizer,       # for smart batching & padding\n",
        ")\n",
        "```\n",
        "* **`model`** ‚Äì *must* be a `PreTrainedModel` subclass that supports generation.\n",
        "* **`tokenizer`** ‚Äì ensures the trainer uses the right pad / special tokens.\n",
        "* **`train_dataset` / `eval_dataset`** ‚Äì any PyTorch-compatible dataset; the trainer wraps them in `DataLoader`s with automatic collation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7  Running the Training Loop\n",
        "\n",
        "```python\n",
        "trainer.train()  # heavy lifting happens here\n",
        "```\n",
        "Under the hood `SFTTrainer` performs:\n",
        "1. Epoch ‚Üí batch iteration, tokenisation & padding.\n",
        "2. Forward pass ‚Üí compute loss **only** on `completion` tokens (via an internal label-mask).\n",
        "3. Back-prop & AdamW update respecting `gradient_accumulation_steps`.\n",
        "4. Callbacks for logging, evaluation, checkpointing at the configured step intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8  Saving artefacts\n",
        "\n",
        "```python\n",
        "model.save_pretrained(\"./tool_choice_final\")\n",
        "tokenizer.save_pretrained(\"./tool_choice_final\")\n",
        "```\n",
        "* Saves both **weights** (`pytorch_model.bin`) and the **config** (`config.json`) so the model can be re-loaded with a single `from_pretrained()` call anywhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9  Inference `pipeline` parameters\n",
        "\n",
        "```python\n",
        "gen = pipeline(\n",
        "    \"text-generation\",      # task selector ‚Äì chooses correct pipeline class\n",
        "    model=\"./tool_choice_final\",  # path or HF-Hub ID\n",
        "    tokenizer=\"./tool_choice_final\",\n",
        "    max_new_tokens=5,         # safety cap ‚Äì tool names are short\n",
        "    eos_token_id=tokenizer.eos_token_id,  # stop when model emits EOS\n",
        "    do_sample=False,          # deterministic greedy decoding\n",
        ")\n",
        "```\n",
        "| Argument | Effect |\n",
        "| -------- | ------ |\n",
        "| `model` / `tokenizer` | Can be local path or remote repo ID; they *must match each other* or token IDs will diverge. |\n",
        "| `max_new_tokens` | Hard upper-bound on generated length; prevents runaway text. |\n",
        "| `eos_token_id` | Allows the pipeline to stop early if EOS appears before hitting the length cap. |\n",
        "| `do_sample` | `False` ‚ûú greedy decoding.  `True` would enable nucleus / temperature sampling for stochastic outputs (not desired for routers). |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Why *synthetic* data, and its trade-offs\n",
        "\n",
        "Synthetic data is quick, cheap, and lets us demonstrate technique without exposing real user logs. But be mindful of its limitations:\n",
        "\n",
        "* **Distribution shift.** If the phrasing of real queries differs from our synthetic ones, performance may drop in production.\n",
        "* **Bias reinforcement.** If our generator under-represents certain language styles, the fine-tuned model will likewise under-perform for those users.\n",
        "* **Evaluation realism.** Always test on *real* hold-out data before shipping.\n",
        "\n",
        "A common compromise is **mixed-source datasets**: seed the model with real anonymised queries (after consent & PII scrubbing) and pad with synthetic ones for coverage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Evaluation beyond token accuracy\n",
        "\n",
        "We logged `mean_token_accuracy`, but for a 6-class decision task we can use richer metrics:\n",
        "\n",
        "| Metric | Why it matters |\n",
        "| ------ | -------------- |\n",
        "| **Exact-match accuracy** | ‚ÄúDid we pick the right tool?‚Äù‚Äîsimple, interpretable. |\n",
        "| **Confusion matrix** | Reveals systematic mix-ups (e.g., `search_web` vs `translate_text`). |\n",
        "| **Macro-F1** | Balances precision & recall per class, helpful if class distribution is skewed. |\n",
        "| **Calibration error** | Tells us if the softmax probabilities are reliable enough to gate fallback rules (‚Äúonly call the tool if p > 0.8‚Äù). |\n",
        "\n",
        "After training, run a small script to compute and visualise these‚Äîmistakes jump out immediately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è Classification head *vs.* next-token generation\n",
        "\n",
        "Classification head is an alternative to next-token generation: attaching a **classification head** on top of the language model. Let‚Äôs do a quick side-by-side:\n",
        "\n",
        "| Aspect | Next-token generation | Classification head |\n",
        "| ------ | -------------------- | ------------------- |\n",
        "| **Architecture change** | None ‚Äì reuse the LM exactly as is. | Add a small feed-forward layer mapping hidden state ‚Üí 6 logits. |\n",
        "| **Training objective** | Cross-entropy over *tokens* (predict the tool name character-by-character). | Cross-entropy over *classes* (predict one of six tools). |\n",
        "| **Speed** | Slower at inference (needs multiple decoding steps). | Single forward pass ‚Äì faster. |\n",
        "| **Confidence scores** | Harder to interpret (need to read log-probs of whole string). | Softmax directly gives per-tool probability. |\n",
        "| **Flexibility** | Can generalise to unseen tools if vocabulary covers them. | Fixed to the predefined class set. |\n",
        "\n",
        "For small closed-set routers, the classification head is usually the pragmatic choice. We used generation here to stay closer to core SFT mechanics and keep the script model-agnostic‚Äîbut feel free to fork the repo and try the head-based variant!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
